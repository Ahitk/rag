{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG\n",
    "\n",
    "#### Problem: her sorguda farkli kategori retrieve edilince bellekte üstüne ekliyor, dolasiyla farkli kategorilerden data alabiliyor soru degistikce, vector database her seferinde hafizayi silmeli!\n",
    "###### Bütüm kodta tutarlilik kontrolü yap, mesela hersey cevabi \"answer\" olarak dönmeli, fronend icin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Kirmizi yepyeni bir ferrari almak istiyorum, hangi modelleri tavsiye edersin?\"\n",
    "#question = \"Magenta TV nedir? Bu ürünü satin alirsam ne gibi avantajlari olacaktir bana?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data directory\n",
    "###### data klasörünün altinda 8 ana kategörü olmali, tam olarak isimler uyusmali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the rag data\n",
    "data_directory = \"/Users/taha/Desktop/rag/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, FewShotChatMessagePromptTemplate, PromptTemplate\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from operator import itemgetter\n",
    "from typing import Literal, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model and template\n",
    "###### To-do: Kategorilerde uygun cevap bulunamazsa nasil bir tepki verecek bunu tanimla.\n",
    "###### Bu prompt daha efektif hale getirilebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API Keys from environment variables\n",
    "load_dotenv()  # Load environment variables from a .env file\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Initialize the chat model and embedding model\n",
    "# ChatOpenAI is used to interact with the OpenAI GPT model, and OpenAIEmbeddings is used for generating embeddings for documents\n",
    "model = ChatOpenAI(model=\"gpt-4o\", api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model max token limit\n",
    "MAX_TOKEN_LENGTH = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for generating an answer based on context and a question\n",
    "telekom_template = \"\"\"You are an assistant for question-answering tasks for telekom.de help, providing answers to Telekom customers or potential customers. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer or if the provided documents do not contain relevant information, simply say that unfortunately, you cannot assist with this question and please visit www.telekom.de/hilfe for further assistance. \n",
    "Use up to four sentences and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_telekom = ChatPromptTemplate.from_template(telekom_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summarizing chain with \"refine\" method to reduce token size\n",
    "summarize_chain = load_summarize_chain(model, chain_type=\"refine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizer - Token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_count(docs, question, prompt):\n",
    "    \"\"\"\n",
    "    If the total token count for the RAG chain exceeds the limit, summarize only the retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of documents to check for token limits and summarize if needed.\n",
    "        question (str): The original question to include in token count.\n",
    "        prompt (str): The prompt template to include in token count.\n",
    "        max_token_length (int): The maximum number of tokens allowed before summarization.\n",
    "\n",
    "    Returns:\n",
    "        list: Summarized documents or original documents based on token limit.\n",
    "    \"\"\"\n",
    "    # Calculate token counts for different components\n",
    "    prompt_tokens = num_tokens_from_string(prompt.format(context=\"dummy\", question=question), \"cl100k_base\")\n",
    "    question_tokens = num_tokens_from_string(question, \"cl100k_base\")\n",
    "    docs_tokens = sum([num_tokens_from_string(doc.page_content, \"cl100k_base\") for doc in docs])\n",
    "    \n",
    "    # Total token count including prompt, question, and documents\n",
    "    total_tokens = prompt_tokens + question_tokens + docs_tokens\n",
    "    #print(f\"Token count (prompt): {prompt_tokens}\")\n",
    "    #print(f\"Token count (question): {question_tokens}\")\n",
    "    #print(f\"Token count (retrieved documents): {docs_tokens}\")\n",
    "    #print(f\"Total token count (for RAG chain): {total_tokens}\")\n",
    "    \n",
    "   \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - vec1 (np.ndarray): The first vector.\n",
    "    - vec2 (np.ndarray): The second vector.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The cosine similarity between vec1 and vec2.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2) if (norm_vec1 and norm_vec2) else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Routing\n",
    "\n",
    "#### 8 Kategoriden birine atiyor, datacategory icerigine göre. Belki biraz daha genisletilebilir, daha uygun kategori atamasi icin.\n",
    "\n",
    "###### ESKI NOT DEGERLENDIR: Routing mantigi calismadi retriever'i sadece ilk seferde filtreliyor, her seferinde chroma ya gömüyü ve hepsini ariyor.\n",
    "###### chain invoke etmeden retriever cagrildigi yerde filtreleme olabilir. Bunu dene!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user question to the most relevant datacategory.\"\"\"\n",
    "\n",
    "    datacategory: Literal[\"vertrag_rechnung_ihre_daten_kundencenter_login-daten_rechnung_lieferstatus\", \n",
    "                          \"hilfe_stoerungen_stoerungen_selbst_beheben_melden_status_verfolgen\",\n",
    "                          \"mobilfunk_tarife_optionen_mobiles-internet_mailbox_esim_sim-karten\",\n",
    "                          \"internet_telefonie:_ausbau,_sicherheit,_einstellungen,_bauherren,_glasfaser_und_wlan\",\n",
    "                          \"tv_magentatv_streaming-dienste_magentatv_jugendschutz_pins\",\n",
    "                          \"magentains_kombi-pakete_mit_magentains_vorteil_und_treuebonus\",\n",
    "                          \"apps_dienste_e-mail_magenta_apps_voicemail_app_mobilityconnect\",\n",
    "                          \"geraete_zubehoer_anleitungen_fuer_smartphones_tablets_telefone_router_receiver\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datacategory would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "structured_model = model.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing user questions to the appropriate data category.\n",
    "\n",
    "Based on the help category the question is referring to, route it to the relevant data category. \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    # Kategorileri ve ilgili alt dizinleri bir sözlükte tanımlayın\n",
    "    category_map = {\n",
    "        \"vertrag_rechnung_ihre_daten_kundencenter_login-daten_rechnung_lieferstatus\": \"Vertrag & Rechnung\",\n",
    "        \"hilfe_stoerungen_stoerungen_selbst_beheben_melden_status_verfolgen\": \"Hilfe bei Störungen\",\n",
    "        \"mobilfunk_tarife_optionen_mobiles-internet_mailbox_esim_sim-karten\": \"Mobilfunk\",\n",
    "        \"internet_telefonie:_ausbau,_sicherheit,_einstellungen,_bauherren,_glasfaser_und_wlan\": \"Internet & Telefonie\",\n",
    "        \"tv_magentatv_streaming-dienste_magentatv_jugendschutz_pins\": \"TV\",\n",
    "        \"magentains_kombi-pakete_mit_magentains_vorteil_und_treuebonus\": \"MagentaEINS\",\n",
    "        \"apps_dienste_e-mail_magenta_apps_voicemail_app_mobilityconnect\": \"Apps & Dienste\",\n",
    "        \"geraete_zubehoer_anleitungen_fuer_smartphones_tablets_telefone_router_receiver\": \"Geräte & Zubehör\"\n",
    "    }\n",
    "    \n",
    "    # Datacategory'yi küçült ve sözlükte ara, yoksa \"Others\" döner\n",
    "    return category_map.get(result.datacategory.lower(), \"Others\")\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertrag & Rechnung\n",
      "/Users/taha/Desktop/rag/data/Vertrag & Rechnung\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"/Users/taha/Desktop/rag/data\"\n",
    "\n",
    "sub_directory = full_chain.invoke({\"question\": question})\n",
    "print(sub_directory)\n",
    "\n",
    "specific_directory = os.path.join(data_directory, sub_directory)\n",
    "print(specific_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseX retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings calculated: 121\n",
      "Total documents created: 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q9/c3sjffm125370mhwph6zm3ww0000gn/T/ipykernel_6202/1939026146.py:80: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  results = summary_retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, results found: 10\n",
      "Number of unique results retrieved: 10\n",
      "Summary vectorstore has been cleared.\n",
      "Successfully loaded document from: data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vorteile_andere_firmen.txt\n",
      "Successfully loaded document from: data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vertrag_kuendigung_mobilfunk_rufnummer_exportieren.txt\n",
      "Successfully loaded document from: data/Vertrag & Rechnung/youtube_Digitale Soforthilfe ohne Wartezeit - Kennst Du...Frag Magenta？ I Telekom.txt\n",
      "Successfully loaded document from: data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vertrag_bestellung_auftragsstatus.txt\n",
      "Successfully loaded document from: data/Vertrag & Rechnung/youtube_Online-Ident - Verträge von Zuhause mit der NECT-App bestätigen #shorts #telekom #telekomhilft.txt\n",
      "Successfully loaded document from: data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vorteile_for_friends_tarife.txt\n",
      "Successfully loaded document from: data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_rechnung_evn_mobilfunk.txt\n",
      "Successfully loaded document from: data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vertrag_meine_daten_mobilfunk_laufzeit_einsehen.txt\n",
      "Successfully loaded document from: data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vorteile_empfehlen_freundschaftswerbung.txt\n",
      "Successfully loaded document from: data/Vertrag & Rechnung/youtube_Frag Magenta - Kennst Du es bereits？ #shorts #telekom #magenta #fragmagenta #tech #service #help.txt\n"
     ]
    }
   ],
   "source": [
    "# Sabitler\n",
    "TOP_N = 10 # Ilgili specified directory'den kac tane en yakin dosyayi getirmek istedigim.\n",
    "SUMMARY_FILE_PATTERN = '**/_summary.txt'\n",
    "\n",
    "vectorstore = None\n",
    "retriever = None\n",
    "\n",
    "# Çöp toplama işlemi\n",
    "gc.collect()\n",
    "\n",
    "def load_summaries(data_directory):\n",
    "    \"\"\"\n",
    "    Summarize the content of _summary.txt files in the given directory.\n",
    "    \"\"\"\n",
    "    summary_files = glob.glob(os.path.join(data_directory, SUMMARY_FILE_PATTERN), recursive=True)\n",
    "    summaries = {}\n",
    "    \n",
    "    for file in summary_files:\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        chunks = content.split(\"=== Chunk ===\")\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            if \"File path:\" in chunk and \"File summary:\" in chunk:\n",
    "                try:\n",
    "                    lines = chunk.split('\\n')\n",
    "                    file_path_line = [line for line in lines if \"File path:\" in line]\n",
    "                    summary_line = [line for line in lines if \"File summary:\" in line]\n",
    "\n",
    "                    if file_path_line and summary_line:\n",
    "                        file_path = file_path_line[0].split(\"File path:\")[1].strip()\n",
    "                        summary_text = summary_line[0].split(\"File summary:\")[1].strip()\n",
    "                        summaries[file_path] = summary_text\n",
    "                except IndexError:\n",
    "                    print(f\"Warning: Skipping chunk due to formatting issues in file: {file}\")\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def create_chroma_vectorstore(summaries, embedding):\n",
    "    \"\"\"\n",
    "    Create a Chroma vectorstore from the provided summaries.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    summaries_text = list(summaries.values())\n",
    "    file_paths = list(summaries.keys())\n",
    "    \n",
    "    # Embed summaries in batch\n",
    "    summary_embeddings = embedding.embed_documents(summaries_text)\n",
    "    \n",
    "    # Debug: Print size of embeddings and a sample embedding\n",
    "    print(f\"Total embeddings calculated: {len(summary_embeddings)}\")\n",
    "\n",
    "    # Create Document objects\n",
    "    for i, summary in enumerate(summaries_text):\n",
    "        doc = Document(page_content=summary, metadata={'source': file_paths[i]})\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Debug: Print size of documents list\n",
    "    print(f\"Total documents created: {len(documents)}\")\n",
    "    \n",
    "    # Create Chroma vectorstore from documents\n",
    "    summary_vectorstore = Chroma.from_documents(documents=documents, embedding=embedding)\n",
    "    return summary_vectorstore\n",
    "\n",
    "\n",
    "def find_closest_summaries_with_chroma(question, summary_retriever, top_n=TOP_N):\n",
    "    \"\"\"\n",
    "    Finds the closest summary files based on the user's question using the Chroma retriever.\n",
    "    Ensures that only unique file paths are returned, with no duplicates. If there aren't\n",
    "    enough unique results, it keeps searching until it finds `top_n` unique results.\n",
    "    \"\"\"\n",
    "    unique_paths = []\n",
    "    seen_files = set()\n",
    "    retries = 0  # To prevent infinite loops in case something goes wrong\n",
    "\n",
    "    while len(unique_paths) < top_n and retries < 5:  # Limit retries to 5 to avoid infinite loops\n",
    "        # Get results from the retriever\n",
    "        results = summary_retriever.get_relevant_documents(question)\n",
    "        \n",
    "        # Debug: Print how many results were found in this iteration\n",
    "        print(f\"Iteration {retries + 1}, results found: {len(results)}\")\n",
    "\n",
    "        for result in results:\n",
    "            file_path = result.metadata['source']\n",
    "            \n",
    "            # Check if the file path has already been added\n",
    "            if file_path not in seen_files:\n",
    "                unique_paths.append(file_path)\n",
    "                seen_files.add(file_path)\n",
    "            \n",
    "            # Stop once we have the desired number of unique paths\n",
    "            if len(unique_paths) >= top_n:\n",
    "                break\n",
    "        \n",
    "        retries += 1  # Increment retry counter in case we need to search again\n",
    "\n",
    "    # Debug: Print how many unique results were retrieved in total\n",
    "    print(f\"Number of unique results retrieved: {len(unique_paths)}\")\n",
    "\n",
    "    # If after retries we still don't have enough results, warn the user\n",
    "    if len(unique_paths) < top_n:\n",
    "        print(f\"Warning: Only {len(unique_paths)} unique results were found after {retries} iterations.\")\n",
    "\n",
    "    return unique_paths\n",
    "\n",
    "\n",
    "def load_original_documents_from_summary_paths(summary_paths):\n",
    "    \"\"\"\n",
    "    Load the original documents based on the summary file paths.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for summary_path in summary_paths:\n",
    "        if not os.path.exists(summary_path):\n",
    "            print(f\"Original document not found for summary: {summary_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(summary_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            docs.append(Document(page_content=content, metadata={'source': summary_path}))\n",
    "            print(f\"Successfully loaded document from: {summary_path}\")  # Debug: Log successful load\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Original document not found for summary: {summary_path}\")  # Debug: Log missing file\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading document from {summary_path}: {e}\")  # Debug: Log any other error\n",
    "    \n",
    "    return docs\n",
    "\n",
    "\n",
    "# Özetleri yükleyin\n",
    "summaries = load_summaries(specific_directory)\n",
    "\n",
    "# Chroma vektör mağazasını oluşturun\n",
    "summary_vectorstore = create_chroma_vectorstore(summaries, embedding)\n",
    "\n",
    "# Chroma'dan bir retriever oluşturun\n",
    "summary_retriever = summary_vectorstore.as_retriever(search_kwargs={\"k\": TOP_N})\n",
    "\n",
    "# En yakın özetleri bulun\n",
    "closest_summary_files = find_closest_summaries_with_chroma(question, summary_retriever, top_n=TOP_N)\n",
    "\n",
    "# Clear Chroma vectorstore after use\n",
    "summary_vectorstore.delete_collection()  # This will delete all vectors in the collection\n",
    "print(\"Summary vectorstore has been cleared.\")\n",
    "\n",
    "# En yakın özetlerin işaret ettiği orijinal dosyaları yükleyin\n",
    "docs = load_original_documents_from_summary_paths(closest_summary_files)\n",
    "\n",
    "# Orijinal belgelerden bir vektör mağazası ve retriever oluşturun\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### orjinal filtreli retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# İlk olarak, eski vectorstore ve retriever nesnelerini temizleyelim\\nvectorstore = None\\nretriever = None\\n\\n# Çöp toplama işlemi\\ngc.collect()\\n\\n# Load documents from the specified directory\\nloader = DirectoryLoader(data_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\\ndocs = loader.load()  # Load all text documents matching the pattern\\nprint(f\"Loaded {len(docs)} documents.\")  # Debug print\\n\\n# Ensure all documents have metadata\\nfor doc in docs:\\n    if \\'full_path\\' not in doc.metadata:\\n        doc.metadata[\\'full_path\\'] = doc.metadata.get(\\'source\\', \\'unknown\\')\\n\\n# Manually filter documents based on metadata\\nfiltered_docs = [doc for doc in docs if doc.metadata.get(\\'full_path\\', \\'\\').startswith(specific_directory)]\\nprint(f\"Filtered {len(filtered_docs)} documents.\")  # Debug print\\n\\n# Create a Chroma vector store from the filtered documents and embeddings\\nif filtered_docs:\\n    vectorstore = Chroma.from_documents(documents=filtered_docs, embedding=embedding)\\n    print(\"Vectorstore created from filtered documents.\")\\n\\n    # Set up the retriever using the filtered vector store\\n    retriever = vectorstore.as_retriever()\\nelse:\\n    print(\"No documents found to create a vectorstore.\")\\n    retriever = None'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# İlk olarak, eski vectorstore ve retriever nesnelerini temizleyelim\n",
    "vectorstore = None\n",
    "retriever = None\n",
    "\n",
    "# Çöp toplama işlemi\n",
    "gc.collect()\n",
    "\n",
    "# Load documents from the specified directory\n",
    "loader = DirectoryLoader(data_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()  # Load all text documents matching the pattern\n",
    "print(f\"Loaded {len(docs)} documents.\")  # Debug print\n",
    "\n",
    "# Ensure all documents have metadata\n",
    "for doc in docs:\n",
    "    if 'full_path' not in doc.metadata:\n",
    "        doc.metadata['full_path'] = doc.metadata.get('source', 'unknown')\n",
    "\n",
    "# Manually filter documents based on metadata\n",
    "filtered_docs = [doc for doc in docs if doc.metadata.get('full_path', '').startswith(specific_directory)]\n",
    "print(f\"Filtered {len(filtered_docs)} documents.\")  # Debug print\n",
    "\n",
    "# Create a Chroma vector store from the filtered documents and embeddings\n",
    "if filtered_docs:\n",
    "    vectorstore = Chroma.from_documents(documents=filtered_docs, embedding=embedding)\n",
    "    print(\"Vectorstore created from filtered documents.\")\n",
    "\n",
    "    # Set up the retriever using the filtered vector store\n",
    "    retriever = vectorstore.as_retriever()\n",
    "else:\n",
    "    print(\"No documents found to create a vectorstore.\")\n",
    "    retriever = None'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever without filter\n",
    "##### Orjinal - yedek retriever, filtreleme yok. Bütün kategorilerden context ceker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def initialize_vectorstore(directory):\\n    \"\"\"\\n    Initializes a vector store from the documents found in the specified directory.\\n    This function performs the following steps:\\n    1. Loads text documents from the given directory using a DirectoryLoader.\\n    2. Creates embeddings for the loaded documents using a predefined embedding model.\\n    3. Initializes a Chroma vector store with these embeddings.\\n    \\n    Parameters:\\n        directory (str): The path to the directory containing text files to be processed.\\n    \\n        \\n    Returns:\\n        vectorstore (Chroma): A Chroma vector store object containing the embeddings of the documents.\\n        docs (List[Document]): A list of Document objects loaded from the specified directory.\\n        \\n    \"\"\"\\n    \\n    # Load documents from the specified directory using DirectoryLoader\\n    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\\n    docs = loader.load()  # Load all text documents matching the pattern\\n    \\n    # Create a Chroma vector store from the loaded documents and embeddings\\n    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\\n    \\n    return vectorstore, docs\\n\\n# Initialize the vector store and document list\\nvectorstore, docs = initialize_vectorstore(data_directory)\\n\\n# Set up the retriever using the vector store\\nretriever = vectorstore.as_retriever()'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def initialize_vectorstore(directory):\n",
    "    \"\"\"\n",
    "    Initializes a vector store from the documents found in the specified directory.\n",
    "    This function performs the following steps:\n",
    "    1. Loads text documents from the given directory using a DirectoryLoader.\n",
    "    2. Creates embeddings for the loaded documents using a predefined embedding model.\n",
    "    3. Initializes a Chroma vector store with these embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): The path to the directory containing text files to be processed.\n",
    "    \n",
    "        \n",
    "    Returns:\n",
    "        vectorstore (Chroma): A Chroma vector store object containing the embeddings of the documents.\n",
    "        docs (List[Document]): A list of Document objects loaded from the specified directory.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Load documents from the specified directory using DirectoryLoader\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs = loader.load()  # Load all text documents matching the pattern\n",
    "    \n",
    "    # Create a Chroma vector store from the loaded documents and embeddings\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "    \n",
    "    return vectorstore, docs\n",
    "\n",
    "# Initialize the vector store and document list\n",
    "vectorstore, docs = initialize_vectorstore(data_directory)\n",
    "\n",
    "# Set up the retriever using the vector store\n",
    "retriever = vectorstore.as_retriever()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n",
      "Unfortunately, I cannot assist with this question. Please visit www.telekom.de/hilfe for further assistance.\n"
     ]
    }
   ],
   "source": [
    "### Web search tool\n",
    "web_search_tool = TavilySearchResults(k=3) #k degerini degistirebilirsin bir bak tam olarak manasina\n",
    "\n",
    "\n",
    "### Retrieval Grader\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = model.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Binary prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Re_write prompt\n",
    "system = \"\"\"You are a question re-writer that converts an input question into a better version optimized for web search.\\n \n",
    "     Always provide the question in English. Look at the input  and try to reason about the underlying semantic intent or meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "grader_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "\n",
    "# Tüm dokümanların içeriklerini birleştir\n",
    "doc_txt = \" \".join([doc.page_content for doc in grader_docs])\n",
    "\n",
    "\n",
    "question_rewriter = re_write_prompt | model | StrOutputParser()\n",
    "\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n",
    "question_rewriter.invoke({\"question\": question})\n",
    "\n",
    "# Chain\n",
    "crag_chain = prompt_telekom | model | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = crag_chain.invoke({\"context\": grader_docs, \"question\": question})\n",
    "print(generation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRAG Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = crag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "workflow.add_node(\"web_search_node\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transform_query':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "\"Node 'web_search_node':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('Unfortunately, I cannot assist with this question. Please visit '\n",
      " 'www.telekom.de/hilfe for further assistance.')\n",
      "[Document(metadata={'source': 'data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vorteile_andere_firmen.txt'}, page_content='Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vorteile/andere-firmen\\nTelekom > Hilfe & Service > Vertrag & Rechnung > Telekom > Vorteile > Angebote > für > andere > Firmen\\n\\nQuestion: Welche Angebote bekomme ich als Mitarbeiter ausgewählter Unternehmen?\\nAnswer: Für Mitarbeitende bestimmter Unternehmen bietet die Telekom einen monatlichen Rabatt auf die Mobilfunk-Tarife und attraktive Preise für Smartphones & Tablets an. Für weitere Informationen registrieren Sie sich bitte mit Ihrer geschäftlichen E-Mail-Adresse auf unsererWebseite für die Vorteilsangebote.\\nHinweis: Wenn Sie wissen möchten, ob Ihre Firma dazu gehört, erkundigen Sie sich in Ihrem Unternehmen, in der Personalabteilung oder beim Betriebsrat.\\n\\n'), Document(metadata={'source': 'data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vertrag_kuendigung_mobilfunk_rufnummer_exportieren.txt'}, page_content='Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vertrag/kuendigung/mobilfunk-rufnummer-exportieren\\nTelekom > Hilfe & Service > Vertrag & Rechnung > Vertrag > Kündigung > Nummer bei Anbieterwechsel > mitnehmen\\n\\nQuestion: Wie kann ich meine Mobilfunk-Rufnummer zu einem anderen Anbieter mitnehmen?\\nAnswer: Sie können Ihre Telekom Mobilfunk-Rufnummer bereits während der Laufzeit, zum Vertragsende oder auch danach zu einem anderen Anbieter kostenfrei mitnehmen. Beachten Sie die dafür geltendenPortierungsfristenund gehen Sie wie folgt vor:\\nPortierung aus einem laufenden VertragFür den Export benötigen wir Ihre Zustimmung zur vorzeitigen Portierung (Mitnahme) der Rufnummer – das sogenannte Opt-In.Kontaktieren Sie\\xa0unsoder nutzen Sie unseren bequemenRückrufservice. Anschließend können Sie bei Ihrem neuen Anbieter die Rufnummernmitnahme beauftragen. Ihr Vertrag bei der Telekom läuft nach dem Export mit einer neuen Mobilfunk-Rufnummer weiter.\\nPortierung zum oder nach VertragsendeSobald Sie von uns die Kündigungsbestätigung erhalten haben, können Sie bei Ihrem neuen Anbieter die Rufnummernmitnahme beauftragen.\\nPortierung PrepaidSie können Ihre Telekom Mobilfunk-Rufnummer zum Vertragsende oder danach zu einem anderen Anbieter kostenfrei mitnehmen.\\nDamit die Portierung unterbrechungsfrei funktioniert, kündigen Sie idealerweise mit Wunschtermin und wählen ein Datum mindestens 14 Tage in der Zukunft aus. Sobald Sie von uns die Kündigungsbestätigung erhalten haben, können Sie bei Ihrem neuen Anbieter die Rufnummernmitnahme zum Vertragsende beauftragen. Ihre Telekom Rufnummer nutzen Sie bis dahin normal weiter.\\n\\n'), Document(metadata={'source': 'data/Vertrag & Rechnung/youtube_Digitale Soforthilfe ohne Wartezeit - Kennst Du...Frag Magenta？ I Telekom.txt'}, page_content='Question:\\nHallo Vanessa, wie kann ich fragmagenta auf meiner Mein Magenta-App verwenden?\\n\\nAnswer:\\nHallo, Vanessa stellt den Telekom-Service, fragmagenta, vor. Dieser digitale Assistent beantwortet Fragen zu Rechnungen, Vertragslaufzeiten oder dem Standort des nächsten Telekom-Shops - rund um die Uhr, ohne Wartezeit. Der Service deckt auch technischen Support, Vertragsfragen, Produktinformationen oder Telekom Services ab. Neue Features wie Avatamax, ein digitaler Ansprechpartner in der Mein Magenta-App, stehen ebenfalls zur Verfügung. Du findest fragmagenta in der Mein Magenta-App, auf der Telekom-Homepage oder in unseren Messaging-Kanälen.\\n\\n'), Document(metadata={'source': 'data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vertrag_bestellung_auftragsstatus.txt'}, page_content='Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vertrag/bestellung/auftragsstatus\\nTelekom > Hilfe & Service > Vertrag & Rechnung > Vertrag > Bestellung > Auftragsstatus\\n\\nQuestion: Wie kann ich meinen Auftragsstatus einsehen?\\nAnswer: So können Sie den aktuellen Stand Ihrer Bestellung oder Rücksendung über das Kundencenter und die MeinMagenta App prüfen:\\nReparaturauftrag für GeräteHier können Sie sich über denReparaturstatusinformieren, wenn Sie ein defektes Gerät (z. B. Smartphone oder Tablet) zur Reparatur an uns übergeben haben. Dazu benötigen Sie die IMEI bzw. Seriennummer Ihres Gerätes sowie Ihre PLZ oder Auftragsnummer.\\nBauherrenWenn Sie einen neuen Hausanschluss bestellt haben, können Sie hier denAuftrag verfolgen. Falls Sie noch keinen Telekom Login besitzen, klicken Sie auf \"Auftragsstatus einsehen\" und geben dort Ihre 7-stellige Auftragsnummer aus der Eingangsbestätigung ein.\\nGlasfaserHier zeigen wir Ihnen zwei Wege, wie Sie denStatus Ihrer Glasfaser-Bestellungeinsehen können.\\n• Kundencenter:Mein Profil > Weitere Services > Auftragsstatus\\n• MeinMagenta App:Service > Auftragsstatus einsehen\\n\\n'), Document(metadata={'source': 'data/Vertrag & Rechnung/youtube_Online-Ident - Verträge von Zuhause mit der NECT-App bestätigen #shorts #telekom #telekomhilft.txt'}, page_content='Question:\\nWarum benutzen Sie nicht einfach das Self-Eident mit der Nekt-App zur Handylegitimierung?\\n\\nAnswer:\\nIn dieser Unterhaltung geht es um eine Person, die kürzlich einen Handyvertrag abgeschlossen hat und nun eine Möglichkeit sucht, sich eigenständig zu legitimieren. Eine andere Person empfiehlt die Nutzung der Nekt-App in Verbindung mit dem sogenannten Self-Eident, um die Legitimierung zu starten. Diese Methode erspart der Person das Verlassen des Hauses und ermöglicht es ihr, den Vorgang jederzeit durchzuführen. Für den Vorgang ist ein Smartphone, die Nekt-App und ein gültiges Ausweisdokument erforderlich. Während der Person geraten wird, ihre neue Rufnummer zu nutzen, wird ihr angeboten, eine Maske zu tragen.\\n\\n'), Document(metadata={'source': 'data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vorteile_for_friends_tarife.txt'}, page_content='Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vorteile/for-friends-tarife\\nTelekom > Hilfe & Service > Vertrag & Rechnung > Telekom > Vorteile > Telekom > for > Friends > Tarife\\n\\nQuestion: Was sind for Friends Tarife?\\nAnswer: Mit Telekom for Friends erhalten Freunde von Mitarbeitern der Telekom einen Sparvorteil auf den Grundpreis für Mobilfunk- und Festnetz-Tarife.\\nWie Sie dieTelekom for Friends Tarife erhaltenkönnen, erfahren Sie auf unserer Hilfe-Seite.\\n\\n'), Document(metadata={'source': 'data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_rechnung_evn_mobilfunk.txt'}, page_content='Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/rechnung/evn-mobilfunk\\nTelekom > Hilfe & Service > Vertrag & Rechnung > Rechnung > Einzelverbindungsnachweis > für > Mobilfunk\\n\\nQuestion: Wie beauftrage ich den Einzelverbindungsnachweis für meinen Mobilfunk-Anschluss?\\nAnswer: Sie können Ihren Einzelverbindungsnachweis (EVN) für Ihren Mobilfunk-Anschluss im Kundencenter und in der MeinMagenta App beauftragen bzw. anpassen.\\nHinweis: Um nachträglich eine Verbindungsübersicht zu beantragen, nutzen Sie unserKontaktformular.\\n• KundencenterMein Profil > Rechnungseinstellungen > Einzelverbindungsnachweis\\n• MeinMagenta AppBenutzer-Symbol > Persönliche Daten > Rechnungseinstellungen > Einzelverbindungsnachweis\\n\\n'), Document(metadata={'source': 'data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vertrag_meine_daten_mobilfunk_laufzeit_einsehen.txt'}, page_content='Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vertrag/meine-daten/mobilfunk-laufzeit-einsehen\\nTelekom > Hilfe & Service > Vertrag & Rechnung > Vertrag > Meine > Daten > Mobilfunk-Laufzeit > einsehen\\n\\nQuestion: Wo kann ich die Laufzeit meines Mobilfunk-Vertrags sehen?\\nAnswer: So können Sie Ihre Mobilfunk-Vertragslaufzeit prüfen:\\n• Im KundencenterWählen Sie imKundencenterim Bereich \"Verträge\" den entsprechenden Mobilfunk-Vertrag aus.\\n• In der MeinMagenta AppWählen Sie in derMeinMagenta Appden entsprechenden Vertrag aus und klicken Sie dann auf \"Vertrag anpassen\".\\n• Auf Ihrer RechnungLoggen Sie sich im Kundencenter ein und gehen Sie im Bereich \"Rechnung & Zahlung\" auf \"Alle anzeigen\". Klicken Sie auf die gewünschte Rechnung. Auf der letzten Seite des PDFs finden Sie Vertragsbeginn und Kündigungsfrist.\\n• Auf der AuftragsbestätigungDiese\\xa0erhalten Sie vor Vertragsbeginn per E-Mail oder per Post.\\n\\n'), Document(metadata={'source': 'data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vorteile_empfehlen_freundschaftswerbung.txt'}, page_content='Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vorteile/empfehlen-freundschaftswerbung\\nTelekom > Hilfe & Service > Vertrag & Rechnung > Telekom > Vorteile > Freundschaftswerbung\\n\\nQuestion: Was ist \"Die Freundschaftswerbung der Telekom\"?\\nAnswer: Wenn Sie an Verwandte oder Bekannte einen Telekom Tarif oder Vertrag empfehlen, erhalten Sie bei erfolgreichem Vertragsabschluss eine Prämie.\\n\\n'), Document(metadata={'source': 'data/Vertrag & Rechnung/youtube_Frag Magenta - Kennst Du es bereits？ #shorts #telekom #magenta #fragmagenta #tech #service #help.txt'}, page_content='Question:\\n\"Wie kann ich fragmagenta nutzen, um Fragen zu meiner Rechnung zu stellen oder die Laufzeit meines Vertrages zu überprüfen?\"\\n\\nAnswer:\\nHallo, ich bin Vanessa, eine Mitarbeiterin von Telekom-Service. Mit unserem Service \"FragMagenta\" können Sie Fragen zu Ihrer Rechnung stellen, die Laufzeit Ihres Vertrages erfragen oder den nächsten Telekom-Shop finden, und das alles ohne Wartezeit. Sie finden \"FragMagenta\" in der \"Mein Magenta\" App auf unserer Homepage oder in unseren Messaging-Kanälen. Außerdem bin ich oft im Telekom-Shop zu finden.\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": question}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for Generating Alternative Questions\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "# Create a prompt template for generating multiple perspectives of the user's question\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define a pipeline for generating alternative queries\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))  # Split the generated output into individual queries\n",
    ")\n",
    "\n",
    "# Asynchronous function to print generated queries\n",
    "async def print_generated_queries(question):\n",
    "    \"\"\"\n",
    "    Generates and prints multiple search queries related to the input question.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The input query for which related search queries are generated.\n",
    "    \"\"\"\n",
    "    queries = generate_queries.invoke({\"question\": question})\n",
    "    print(\"\\nGenerated Questions:\")\n",
    "    for q in queries:\n",
    "        print(f\"{q}\")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    \"\"\"\n",
    "    Returns a unique union of retrieved documents.\n",
    "\n",
    "    This function takes a list of lists of documents, flattens it, and removes duplicates\n",
    "    to ensure each document is unique.\n",
    "\n",
    "    Args:\n",
    "        documents (list of lists): A list where each element is a list of documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique documents.\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists of documents\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Remove duplicates by converting to a set and then back to a list\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Deserialize the documents back into their original form\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Define the retrieval chain, which includes generating queries, retrieving documents, and removing duplicates\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "# Retrieve multiple documents based on the input question\n",
    "multi_query_docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "def format_docs(docs, query_embedding):\n",
    "    \"\"\"\n",
    "    Formats the retrieved documents with their source and cosine similarity score.\n",
    "\n",
    "    This function takes a list of documents and formats them to include the source of each document\n",
    "    and its cosine similarity to the query embedding.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents retrieved from the database.\n",
    "        query_embedding (numpy array): The embedding of the user's query.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the source, similarity score, and content of each document.\n",
    "    \"\"\"\n",
    "    # Initialize a set to track unique sources\n",
    "    unique_sources = set()\n",
    "    formatted_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Retrieve the source of the document from its metadata\n",
    "        source = doc.metadata.get(\"source\")\n",
    "        # Check if the source is unique\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            # Compute the embedding of the document's content\n",
    "            document_embedding = embedding.embed_query(doc.page_content)\n",
    "            # Calculate cosine similarity between the query and document embeddings\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)\n",
    "            # Use a placeholder message if the document content is empty\n",
    "            content = doc.page_content.strip() or \"This document content is empty.\"\n",
    "            # Format the document's source, similarity score, and content\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "\n",
    "    # Join the formatted documents into a single string\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Define a retrieval and generation (RAG) chain for processing the question and context\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} \n",
    "    | prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async def retrieve_and_format_docs(question):\n",
    "    \"\"\"\n",
    "    Asynchronously retrieves and formats documents for the given question.\n",
    "\n",
    "    This function retrieves documents relevant to the user's question and formats them with their\n",
    "    source information and cosine similarity scores.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the answer and formatted documents.\n",
    "    \"\"\"\n",
    "    # Compute the embedding for the user's question\n",
    "    query_embedding = embedding.embed_query(question)\n",
    "    # Format the retrieved documents with their cosine similarity scores\n",
    "    formatted_docs = format_docs(multi_query_docs, query_embedding)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to retrieve an answer using the RAG chain asynchronously\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Fallback in case of TypeError, invoke the RAG chain synchronously\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    # Return the answer and the formatted documents\n",
    "    return answer, formatted_docs\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    The main asynchronous function to run the complete flow.\n",
    "\n",
    "    This function handles the process of generating alternative queries, retrieving and formatting\n",
    "    documents, and printing the final answer along with the source documents.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Retrieve and format documents, then get the answer\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "\n",
    "    # Print the final answer\n",
    "    print(\"\\nAnswer:\", answer)\n",
    "     # Generate and print alternative queries\n",
    "    await print_generated_queries(question)\n",
    "    # Print the source documents used for the answer\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "# Execute the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for generating multiple search queries based on a single input query.\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create a chain for generating four related search queries\n",
    "generate_fusion_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "async def print_generated_fusion_queries(question):\n",
    "    \"\"\"\n",
    "    Generates and prints multiple search queries related to the input question.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The input query for which related search queries are generated.\n",
    "    \"\"\"\n",
    "    queries = generate_fusion_queries.invoke({\"question\": question})\n",
    "    print(\"\\nGenerated Questions:\")\n",
    "    for q in queries:\n",
    "        print(f\"{q}\")\n",
    "\n",
    "# Function for Reciprocal Rank Fusion (RRF)\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"\n",
    "    Applies Reciprocal Rank Fusion (RRF) to combine multiple lists of ranked documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - results (list[list]): A list of lists where each inner list contains ranked documents.\n",
    "    - k (int): An optional parameter for the RRF formula, default is 60.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of tuples where each tuple contains a document and its fused score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to store the fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Serialize the document to a string format to use as a key\n",
    "            doc_str = dumps(doc)\n",
    "            # Initialize the document's score if not already present\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the document's score using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort documents based on their fused scores in descending order\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples\n",
    "    return reranked_results\n",
    "\n",
    "# Create a retrieval chain that generates queries, retrieves documents, and applies RRF\n",
    "retrieval_chain_rag_fusion = generate_fusion_queries | retriever.map() | reciprocal_rank_fusion\n",
    "fusion_docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "# Function to get embeddings for a document's content\n",
    "async def get_document_embeddings(doc):\n",
    "    \"\"\"\n",
    "    Retrieves the embeddings for a document's content asynchronously.\n",
    "    \n",
    "    Parameters:\n",
    "    - doc (Document): The document object whose content embeddings are to be retrieved.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: The embeddings of the document's content.\n",
    "    \"\"\"\n",
    "    return embedding.embed_query(doc.page_content)\n",
    "\n",
    "# Function to format fusion_docs as a readable string with similarity scores\n",
    "async def format_fusion_docs_with_similarity(fusion_docs):\n",
    "    \"\"\"\n",
    "    Formats the fusion documents with their scores and cosine similarity to the question.\n",
    "    \n",
    "    Parameters:\n",
    "    - fusion_docs (list[tuple]): A list of tuples containing documents and their scores.\n",
    "    \n",
    "    Returns:\n",
    "    - str: A formatted string containing each document's source, fusion score, cosine similarity, and content.\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    question_embedding = embedding.embed_query(question)\n",
    "    \n",
    "    for doc, score in fusion_docs:\n",
    "        doc_embedding = await get_document_embeddings(doc)\n",
    "        similarity = cosine_similarity(question_embedding, doc_embedding)\n",
    "        source = doc.metadata.get(\"source\", \"No source\")\n",
    "        content = doc.page_content\n",
    "        formatted_docs.append(f\"Source: {source}\\nFusion Score: {score:.4f}\\nCosine Similarity: {similarity:.4f}\\nContent: {content}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "\n",
    "# Create a chain that uses context and question to generate an answer\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")} \n",
    "    | prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Asynchronous function to retrieve and format documents, then get an answer\n",
    "async def retrieve_and_format_docs(question):\n",
    "    \"\"\"\n",
    "    Retrieves and formats documents, then obtains an answer to the question.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The query for which answers and document formats are required.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing the answer and the formatted documents.\n",
    "    \"\"\"\n",
    "    formatted_docs = await format_fusion_docs_with_similarity(fusion_docs)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to get the answer asynchronously\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Fallback to synchronous invocation if asynchronous fails\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "\n",
    "# Main function to run the sequence of operations\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the entire process: generating queries, retrieving and formatting documents, and getting answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    answer, formatted_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"\\nAnswer:\", answer)\n",
    "    await print_generated_fusion_queries(question)\n",
    "    print(\"\\nSources:\")\n",
    "    print(formatted_docs)  # Print the formatted version of fusion_docs with similarity scores\n",
    "\n",
    "# Execute the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step Back\n",
    "\n",
    "###### cosine similarity ve token sayisi eksik sadece calisiyor suan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Transform examples into example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate step-back queries\n",
    "generate_queries_step_back = prompt | model | StrOutputParser()\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Original Question: {question}\")\n",
    "print(f\"Step-Back Question: {step_back_question}\")\n",
    "\n",
    "# Response prompt template\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "def get_retrieved_content(retrieved_documents):\n",
    "    \"\"\"Format retrieved documents as a string with source information.\"\"\"\n",
    "    seen_sources = set()\n",
    "    content_list = []\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            content = (\n",
    "                f\"Source: {source}\\n\"\n",
    "                f\"Content:\\n{doc.page_content}\\n\"\n",
    "                \"------------------------------\\n\"\n",
    "            )\n",
    "            content_list.append(content)\n",
    "    return \"\\n\".join(content_list)\n",
    "\n",
    "def format_retrieved_context(query):\n",
    "    \"\"\"Retrieve and format context for the given query.\"\"\"\n",
    "    # Retrieve documents using the 'invoke' method\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    return get_retrieved_content(retrieved_docs)\n",
    "\n",
    "# Construct the chain to retrieve and generate the response\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": lambda x: format_retrieved_context(x[\"question\"]),\n",
    "        \"step_back_context\": lambda x: format_retrieved_context(x[\"step_back_question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "result = chain.invoke({\"question\": question, \"step_back_question\": step_back_question})\n",
    "\n",
    "# Display the final response\n",
    "print(\"\\nAnswer:\\n\", result)\n",
    "print(\"\\nNormal Context:\\n\", format_retrieved_context(question))\n",
    "print(\"\\nStep-Back Context:\\n\", format_retrieved_context(step_back_question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "# This list provides example pairs of input questions and their corresponding step-back questions for model training.\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create a prompt template for examples.\n",
    "# This template formats example messages for the model to learn from.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),  # Input from the user\n",
    "        (\"ai\", \"{output}\"),    # Model's response to the input\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a few-shot prompt template that includes example prompts.\n",
    "# This helps the model understand the context by providing example inputs and outputs.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Define the final prompt template.\n",
    "# This includes system instructions and integrates the few-shot prompt.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),  # Input question from the user\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate step-back queries using the defined prompt.\n",
    "# This involves processing the original question to generate a more general query.\n",
    "generate_queries_step_back = prompt | model | StrOutputParser()\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "\n",
    "# Response prompt template\n",
    "# This template is used to generate the final response based on the retrieved context and the original question.\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "def get_retrieved_content(retrieved_documents):\n",
    "    \"\"\"\n",
    "    Format retrieved documents as a string with source information.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_documents (list): List of documents retrieved based on the query.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string containing source and content of retrieved documents.\n",
    "    \"\"\"\n",
    "    seen_sources = set()  # Track unique sources\n",
    "    content_list = []      # List to accumulate formatted content\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown')  # Get source of the document\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            content = (\n",
    "                f\"Source: {source}\\n\"\n",
    "                f\"Content:\\n{doc.page_content}\\n\"\n",
    "                \"------------------------------\\n\"\n",
    "            )\n",
    "            content_list.append(content)\n",
    "    return \"\\n\".join(content_list)\n",
    "\n",
    "def format_retrieved_context(query):\n",
    "    \"\"\"\n",
    "    Retrieve and format context for the given query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query for which context needs to be retrieved.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string containing context relevant to the query.\n",
    "    \"\"\"\n",
    "    # Retrieve documents using the 'invoke' method\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    return get_retrieved_content(retrieved_docs)\n",
    "\n",
    "# Construct the chain to retrieve and generate the response.\n",
    "# This chain combines context retrieval and response generation.\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": lambda x: format_retrieved_context(x[\"question\"]),\n",
    "        \"step_back_context\": lambda x: format_retrieved_context(x[\"step_back_question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain to get the final response.\n",
    "result = chain.invoke({\"question\": question, \"step_back_question\": step_back_question})\n",
    "\n",
    "# Display the final response along with normal and step-back contexts.\n",
    "print(\"Answer:\", result)\n",
    "print(f\"\\n\\nOriginal Question: {question}\")\n",
    "print(f\"\\nStep-Back Question: {step_back_question}\")\n",
    "print(\"\\nNormal Context:\\n\", format_retrieved_context(question))\n",
    "print(\"\\nStep-Back Context:\\n\", format_retrieved_context(step_back_question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE document generation\n",
    "template = \"\"\"You are creating professional and customer-focused web page content and texts for a major telecommunications provider like Telekom.de. \n",
    "Your content is very brief, very clear, and informative. Please write a text for the following question\n",
    "Question: {question}\n",
    "text:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run HyDE generation\n",
    "try:\n",
    "    hyde_output = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "    print(f\"HyDE hypothetical answer:\\n{hyde_output.strip()}\\n\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating documents for retrieval: {e}\")\n",
    "    raise\n",
    "\n",
    "# Retrieve documents\n",
    "try:\n",
    "    retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "    retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Print retrieved documents, deduplicated\n",
    "    seen_sources = set()\n",
    "    print(\"Retrieved sources:\")\n",
    "    for doc in retrieved_docs:\n",
    "        source = doc.metadata.get('source', 'Unknown Source')\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            print(f\"\\nDocument Source: {source}\")\n",
    "            print(f\"Document Content:\\n{doc.page_content.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "try:\n",
    "    final_answer = final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "    print(f\"\\nFinal RAG Answer:\\n{final_answer.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating final RAG answer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE Document Generation\n",
    "# This section is responsible for creating professional and customer-focused content\n",
    "# for a major telecommunications provider based on a given question.\n",
    "\n",
    "# Define a template for generating content.\n",
    "# The template specifies that the content should be brief, clear, and informative.\n",
    "template = \"\"\"You are creating professional and customer-focused web page content and texts for a major telecommunications provider like Telekom.de. \n",
    "Your content is very brief, very clear, and informative. Please write a text for the following question:\n",
    "Question: {question}\n",
    "text:\"\"\"\n",
    "\n",
    "# Create a prompt template using the defined template.\n",
    "# This template will be used to generate content for a given question.\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define a chain to generate documents for retrieval.\n",
    "# This chain uses the prompt template, a language model, and an output parser.\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run HyDE document generation to produce content for the given question.\n",
    "# The try-except block handles potential errors during document generation.\n",
    "try:\n",
    "    hyde_output = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "    print(f\"HyDE hypothetical context:\\n{hyde_output.strip()}\\n\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating documents for retrieval: {e}\")\n",
    "    raise\n",
    "\n",
    "# Retrieve Documents\n",
    "# This section retrieves documents based on the generated content and prints them.\n",
    "\n",
    "# Define a chain to retrieve documents using the generated content.\n",
    "# The chain combines the document generation process with a retriever.\n",
    "try:\n",
    "    retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "    retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Print retrieved documents and deduplicate them based on source information.\n",
    "    seen_sources = set()\n",
    "    print(\"Retrieved sources:\")\n",
    "    for doc in retrieved_docs:\n",
    "        source = doc.metadata.get('source', 'Unknown Source')  # Get the source of the document\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            print(f\"\\nSource file: {source}\")\n",
    "            print(f\"Document Content:\\n{doc.page_content.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define a chain to generate the final answer using the RAG process.\n",
    "# The chain combines the prompt template, a language model, and an output parser.\n",
    "final_rag_chain = (\n",
    "    prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Generate the final answer using the RAG process.\n",
    "# The try-except block handles potential errors during the final answer generation.\n",
    "try:\n",
    "    final_answer = final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "    print(f\"\\nFinal Answer:\\n{final_answer.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating final RAG answer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### !!Decomposition\n",
    "###### Calismadi olmadi maalesef, asnwer sadece 3. sorunun cevabini veriyor, stratch den baska kaynakalara bakip cözüm bulmak lazim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts and chains\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answered in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | model | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer recursion\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | model\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer_decomposition = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer_decomposition)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
