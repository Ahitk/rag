{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testdata Generation - Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing test directory\n",
      "[INFO] Found 190 files in test directory\n",
      "[INFO] Splitting complete, 552 chunks created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.                     \n",
      "Generating: 100%|██████████| 10/10 [00:26<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Testset generated with size 10\n",
      "[INFO] Results saved to /Users/taha/Desktop/rag/test_data_naive/_testset_character_chunksize500_overlap25.csv\n",
      "[INFO] Completed processing test directory\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import initials\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "\n",
    "# Initialize TestsetGenerator with embedding and model from initials\n",
    "generator = TestsetGenerator.from_langchain(generator_llm=initials.model, critic_llm=initials.model, embeddings=initials.embedding)\n",
    "\n",
    "# Character splitting settings\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='',\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=25,\n",
    ")\n",
    "\n",
    "# CharacterTextSplitter with separator\n",
    "text_splitter_separator = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "#RecursiveCharacterTextSplitter\n",
    "text_splitter_recursive = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Semantic Splitting\n",
    "text_splitter_semantic = SemanticChunker(initials.embedding)\n",
    "\n",
    "# Define test directory path\n",
    "test_directory = \"/Users/taha/Desktop/rag/test_data_naive\"\n",
    "\n",
    "TEST_SIZE = 10\n",
    "\n",
    "# Output CSV file path\n",
    "output_file_path = os.path.join(test_directory, \"_testset_hyde_semantic.csv\")\n",
    "\n",
    "# Skip if CSV already exists\n",
    "if os.path.exists(output_file_path):\n",
    "    print(f\"[INFO] CSV already exists, skipping...\")\n",
    "else:\n",
    "    print(f\"\\n[INFO] Processing test directory\")\n",
    "\n",
    "    # Collect all .txt files in the test directory\n",
    "    txt_files = glob.glob(os.path.join(test_directory, \"*.txt\"))\n",
    "\n",
    "    print(f\"[INFO] Found {len(txt_files)} files in test directory\")\n",
    "\n",
    "    # Load all files\n",
    "    documents = []\n",
    "    for file_path in txt_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            documents.append(f.read())   \n",
    "\n",
    "    # Apply splitting\n",
    "    chunks = text_splitter_semantic.create_documents(documents)\n",
    "\n",
    "    print(f\"[INFO] Splitting complete, {len(chunks)} chunks created.\")\n",
    "\n",
    "    # Create a test set of N items\n",
    "    testset = generator.generate_with_langchain_docs(chunks, test_size=TEST_SIZE)\n",
    "    testset_df = testset.to_pandas()\n",
    "    print(f\"[INFO] Testset generated with size\", TEST_SIZE)\n",
    "\n",
    "    # Save DataFrame as CSV\n",
    "    testset_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"[INFO] Results saved to {output_file_path}\")\n",
    "\n",
    "    print(f\"[INFO] Completed processing test directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
