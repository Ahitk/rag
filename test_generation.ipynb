{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_chroma import Chroma\n",
    "import prompts as prompts\n",
    "import initials as initials\n",
    "import indexing\n",
    "import evaluation\n",
    "import chromadb\n",
    "\n",
    "VECTORSTORE_WEIGHT = 0.5\n",
    "\n",
    "test_routing_directory = '/Users/taha/Desktop/rag/test_data_routing/Mobilfunk'\n",
    "test_directory = '/Users/taha/Desktop/rag/test_data'\n",
    "\n",
    "# Define testset path\n",
    "input_csv_path = '/Users/taha/Desktop/rag/test_data_routing/Mobilfunk/_testset_advanced_routing_semantic.csv'  # Input CSV file path\n",
    "\n",
    "# Define output CSV path including the filename\n",
    "output_csv_path = 'test_data_routing/_evaluation_advanced_fusion_densex_routing.csv'  # Output file will be created here\n",
    "\n",
    "# Function to create the output CSV file at the beginning\n",
    "def initialize_output_csv(output_path):\n",
    "    # Directly create the file with the correct header\n",
    "    with open(output_path, 'w') as file:\n",
    "        header = (\n",
    "            \"Question,Response,Contexts,Ground Truth,\"\n",
    "            \"Token Count,Total Cost (USD),Completion Tokens,Number of Retrieved documents,\"\n",
    "            \"Response time,answer_relevancy,context_precision,\"\n",
    "            \"context_recall,faithfulness,BleuScore,RougeScore\\n\"\n",
    "        )\n",
    "        file.write(header)\n",
    "    print(f\"Created output file at: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RAG get_response functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response with error handling\n",
    "def get_response_hyde(user_input):\n",
    "    try:\n",
    "        # Dosyaları listele\n",
    "        all_txt_files = [file for file in glob.glob(os.path.join(test_routing_directory, \"*.txt\")) if not file.endswith(\"_summary.txt\")]\n",
    "        question_history = []\n",
    "        # Seçilen dosyaların içeriklerini oku ve birleştir\n",
    "        all_texts = []\n",
    "        for file_path in all_txt_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                all_texts.append(f.read())\n",
    " \n",
    "        # Semantic Splitting\n",
    "        text_splitter_semantic = SemanticChunker(initials.embedding)\n",
    "        \n",
    "        #retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8, 'k': 5}) #add cos similarity filter, and k documents\n",
    "        \n",
    "        #==========================   HyDE   =============================\n",
    "\n",
    "        chunks = text_splitter_semantic.create_documents(all_texts)\n",
    "        print(\"==========   CHUNKS CREATED  ==========\")\n",
    "        vectorstore = Chroma.from_documents(documents=chunks, embedding=initials.embedding)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        hyde_docs = (prompts.prompt_hyde | initials.model | StrOutputParser())\n",
    "        retrieval_chain_hyde = hyde_docs | retriever \n",
    "        retrieved_docs = retrieval_chain_hyde.invoke({\"question\": user_input, \"question_history\": question_history})\n",
    "    \n",
    "        hyde_rag_chain = (prompts.prompt_telekom | initials.model | StrOutputParser())\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            response = hyde_rag_chain.invoke({\n",
    "                \"context\": retrieved_docs, \n",
    "                \"question\": user_input,\n",
    "                \"chat_history\": []\n",
    "            }) if retrieved_docs else \"No relevant documents found.\"\n",
    "        \n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        return response, retrieved_docs, total_cost, total_tokens, completion_tokens\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get response with error handling\n",
    "def get_response_hyde_hybrid(user_input):\n",
    "    try:\n",
    "        # Dosyaları listele\n",
    "        all_txt_files = [file for file in glob.glob(os.path.join(test_routing_directory, \"*.txt\")) if not file.endswith(\"_summary.txt\")]\n",
    "        question_history = []\n",
    "        # Seçilen dosyaların içeriklerini oku ve birleştir\n",
    "        all_texts = []\n",
    "        for file_path in all_txt_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                all_texts.append(f.read())\n",
    " \n",
    "        # Semantic Splitting\n",
    "        text_splitter_semantic = SemanticChunker(initials.embedding)\n",
    "        \n",
    "        #retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8, 'k': 5}) #add cos similarity filter, and k documents\n",
    "\n",
    "        #==========================   HyDE - Hybrid  =============================\n",
    " \n",
    "        chunks = text_splitter_semantic.create_documents(all_texts)\n",
    "        print(\"==========   CHUNKS CREATED  ==========\")\n",
    "        vectorstore = Chroma.from_documents(documents=chunks, embedding=initials.embedding)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        keyword_retriever = BM25Retriever.from_documents(chunks)\n",
    "        hybrid_retriever = EnsembleRetriever(retrievers=[keyword_retriever, retriever], weights=[1-VECTORSTORE_WEIGHT, VECTORSTORE_WEIGHT])\n",
    "        print(\"==================CONTROL=================\")\n",
    "        hyde_docs = (prompts.prompt_hyde | initials.model | StrOutputParser())\n",
    "        retrieval_chain_hyde = hyde_docs | hybrid_retriever \n",
    "        retrieved_docs = retrieval_chain_hyde.invoke({\"question\": user_input, \"question_history\": question_history})\n",
    "    \n",
    "        hyde_rag_chain = (prompts.prompt_telekom | initials.model | StrOutputParser())\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            response = hyde_rag_chain.invoke({\n",
    "                \"context\": retrieved_docs, \n",
    "                \"question\": user_input,\n",
    "                \"chat_history\": []\n",
    "            }) if retrieved_docs else \"No relevant documents found.\"\n",
    "\n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    " \n",
    "        return response, retrieved_docs, total_cost, total_tokens, completion_tokens\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_hyde_densex_routing(user_input):\n",
    "    try:\n",
    "        # Dosyaları listele\n",
    "        all_txt_files = [file for file in glob.glob(os.path.join(test_routing_directory, \"*.txt\")) if not file.endswith(\"_summary.txt\")]\n",
    "        question_history = []\n",
    "        # Seçilen dosyaların içeriklerini oku ve birleştir\n",
    "        all_texts = []\n",
    "        for file_path in all_txt_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                all_texts.append(f.read())\n",
    " \n",
    "        # Semantic Splitting\n",
    "        text_splitter_semantic = SemanticChunker(initials.embedding)\n",
    "        \n",
    "        #retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8, 'k': 5}) #add cos similarity filter, and k documents\n",
    "\n",
    "        #==========================   HyDE - DenseX   =============================\n",
    "\n",
    "        #DenseX vectors store\n",
    "        densex_vectorstore = indexing.generate_final_vectorstore_with_chunks(user_input, test_routing_directory, initials.embedding)        \n",
    "        densex_retriever = densex_vectorstore.as_retriever()\n",
    "\n",
    "        hyde_docs = (prompts.prompt_hyde | initials.model | StrOutputParser())\n",
    "        retrieval_chain_hyde = hyde_docs | densex_retriever \n",
    "        retrieved_docs = retrieval_chain_hyde.invoke({\"question\": user_input, \"question_history\": question_history})\n",
    "    \n",
    "        hyde_rag_chain = (prompts.prompt_telekom | initials.model | StrOutputParser())\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            response = hyde_rag_chain.invoke({\n",
    "                \"context\": retrieved_docs, \n",
    "                \"question\": user_input,\n",
    "                \"chat_history\": []\n",
    "            }) if retrieved_docs else \"No relevant documents found.\"\n",
    "        \n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        return response, retrieved_docs, total_cost, total_tokens, completion_tokens\n",
    "\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_fusion(user_input):\n",
    "    try:\n",
    "        # Dosyaları listele\n",
    "        all_txt_files = [file for file in glob.glob(os.path.join(test_directory, \"*.txt\")) if not file.endswith(\"_summary.txt\")]\n",
    "        question_history = []\n",
    "        # Seçilen dosyaların içeriklerini oku ve birleştir\n",
    "        all_texts = []\n",
    "        for file_path in all_txt_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                all_texts.append(f.read())\n",
    " \n",
    "        # Semantic Splitting\n",
    "        text_splitter_semantic = SemanticChunker(initials.embedding)\n",
    "        \n",
    "        #retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8, 'k': 5}) #add cos similarity filter, and k documents\n",
    "       \n",
    "        #==========================   RAG Fusion   =============================\n",
    "\n",
    "        #Generate multiple queries using the multi_query_prompt and model\n",
    "        chunks = text_splitter_semantic.create_documents(all_texts)\n",
    "        print(\"==========   CHUNKS CREATED  ==========\")\n",
    "        vectorstore = Chroma.from_documents(documents=chunks, embedding=initials.embedding)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        generate_multi_queries = (\n",
    "            prompts.multi_query_prompt \n",
    "            | initials.model \n",
    "            | StrOutputParser() \n",
    "            | (lambda x: x.split(\"\\n\"))\n",
    "        )\n",
    "\n",
    "        retrieval_chain_rag_fusion = generate_multi_queries | retriever.map() | initials.reciprocal_rank_fusion\n",
    "\n",
    "        fusion_docs = retrieval_chain_rag_fusion.invoke({\"question\": user_input, \"question_history\": question_history})\n",
    "        document_list = [doc[0] for doc in fusion_docs]\n",
    "\n",
    "        fusion_rag_chain = (prompts.prompt_telekom | initials.model | StrOutputParser())\n",
    "        \n",
    "        with get_openai_callback() as cb:\n",
    "            response = fusion_rag_chain.invoke({\n",
    "                \"context\": document_list, \n",
    "                \"question\": user_input,\n",
    "                \"chat_history\": []\n",
    "            }) if document_list else \"No relevant documents found.\"\n",
    "\n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        return response, document_list, total_cost, total_tokens, completion_tokens\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_fusion_routing(user_input):\n",
    "    try:\n",
    "        # Dosyaları listele\n",
    "        all_txt_files = [file for file in glob.glob(os.path.join(test_routing_directory, \"*.txt\")) if not file.endswith(\"_summary.txt\")]\n",
    "        question_history = []\n",
    "        # Seçilen dosyaların içeriklerini oku ve birleştir\n",
    "        all_texts = []\n",
    "        for file_path in all_txt_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                all_texts.append(f.read())\n",
    " \n",
    "        # Semantic Splitting\n",
    "        text_splitter_semantic = SemanticChunker(initials.embedding)\n",
    "        \n",
    "        #retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8, 'k': 5}) #add cos similarity filter, and k documents\n",
    "       \n",
    "        #==========================   RAG Fusion   =============================\n",
    "\n",
    "        #Generate multiple queries using the multi_query_prompt and model\n",
    "        chunks = text_splitter_semantic.create_documents(all_texts)\n",
    "        print(\"==========   CHUNKS CREATED  ==========\")\n",
    "        vectorstore = Chroma.from_documents(documents=chunks, embedding=initials.embedding)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        generate_multi_queries = (\n",
    "            prompts.multi_query_prompt \n",
    "            | initials.model \n",
    "            | StrOutputParser() \n",
    "            | (lambda x: x.split(\"\\n\"))\n",
    "        )\n",
    "\n",
    "        retrieval_chain_rag_fusion = generate_multi_queries | retriever.map() | initials.reciprocal_rank_fusion\n",
    "\n",
    "        fusion_docs = retrieval_chain_rag_fusion.invoke({\"question\": user_input, \"question_history\": question_history})\n",
    "        document_list = [doc[0] for doc in fusion_docs]\n",
    "\n",
    "        fusion_rag_chain = (prompts.prompt_telekom | initials.model | StrOutputParser())\n",
    "        \n",
    "        with get_openai_callback() as cb:\n",
    "            response = fusion_rag_chain.invoke({\n",
    "                \"context\": document_list, \n",
    "                \"question\": user_input,\n",
    "                \"chat_history\": []\n",
    "            }) if document_list else \"No relevant documents found.\"\n",
    "\n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        return response, document_list, total_cost, total_tokens, completion_tokens\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_fusion_densex_routing(user_input):\n",
    "    try:\n",
    "\n",
    "        #==========================   RAG Fusion - DenseX   =============================\n",
    "\n",
    "        #DenseX vectors store\n",
    "        densex_vectorstore = indexing.generate_final_vectorstore_with_chunks(user_input, test_routing_directory, initials.embedding)\n",
    "        densex_retriever = densex_vectorstore.as_retriever()\n",
    "\n",
    "        # Generate multiple queries using the multi_query_prompt and model\n",
    "        generate_multi_queries = (\n",
    "            prompts.multi_query_prompt \n",
    "            | initials.model \n",
    "            | StrOutputParser() \n",
    "            | (lambda x: x.split(\"\\n\"))\n",
    "        )\n",
    "\n",
    "        retrieval_chain_rag_fusion = generate_multi_queries | densex_retriever.map() | initials.reciprocal_rank_fusion\n",
    "        question_history = []\n",
    "        fusion_docs = retrieval_chain_rag_fusion.invoke({\"question\": user_input, \"question_history\": question_history})\n",
    "        document_list = [doc[0] for doc in fusion_docs]\n",
    "\n",
    "        fusion_rag_chain = (prompts.prompt_telekom | initials.model | StrOutputParser())\n",
    "        \n",
    "        with get_openai_callback() as cb:\n",
    "            response = fusion_rag_chain.invoke({\n",
    "                \"context\": document_list, \n",
    "                \"question\": user_input,\n",
    "                \"chat_history\": []\n",
    "            }) if document_list else \"No relevant documents found.\"\n",
    "\n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        return response, document_list, total_cost, total_tokens, completion_tokens\n",
    "        \n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get response with error handling\n",
    "def get_response_multiquery(user_input):\n",
    "    try:\n",
    "        # Dosyaları listele\n",
    "        all_txt_files = [file for file in glob.glob(os.path.join(test_routing_directory, \"*.txt\")) if not file.endswith(\"_summary.txt\")]\n",
    "        question_history = []\n",
    "        # Seçilen dosyaların içeriklerini oku ve birleştir\n",
    "        all_texts = []\n",
    "        for file_path in all_txt_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                all_texts.append(f.read())\n",
    " \n",
    "        # Semantic Splitting\n",
    "        text_splitter_semantic = SemanticChunker(initials.embedding)\n",
    "        \n",
    "        #retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8, 'k': 5}) #add cos similarity filter, and k documents\n",
    "        \n",
    "        #==========================   Multi-Query   =============================\n",
    "   \n",
    "        chunks = text_splitter_semantic.create_documents(all_texts)\n",
    "        print(\"==========   CHUNKS CREATED  ==========\")\n",
    "        vectorstore = Chroma.from_documents(documents=chunks, embedding=initials.embedding)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        # Generate multiple queries using the multi_query_prompt and model\n",
    "        generate_multi_queries = (\n",
    "            prompts.multi_query_prompt \n",
    "            | initials.model \n",
    "            | StrOutputParser() \n",
    "            | (lambda x: x.split(\"\\n\"))  # Split the generated output into individual queries\n",
    "        )\n",
    "\n",
    "        # Generate the multiple queries based on user input\n",
    "        multiple_queries = generate_multi_queries.invoke({\"question\": user_input, \"question_history\": question_history})\n",
    "\n",
    "        # Now, use the generated queries to retrieve documents\n",
    "        # Now, use the generated queries to retrieve documents\n",
    "        if multiple_queries:\n",
    "            # Use retriever to fetch documents for each query\n",
    "            documents_text = []\n",
    "            for query in multiple_queries:\n",
    "                retrieved_docs = retriever.get_relevant_documents(query)\n",
    "                # Join all retrieved documents into a single text string for each query result\n",
    "                docs_texts = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "                documents_text.append(docs_texts)\n",
    "\n",
    "        # Create prompt for final response generation\n",
    "        multi_query_rag_chain = (prompts.prompt_telekom | initials.model | StrOutputParser())\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            response = multi_query_rag_chain.invoke({\n",
    "                \"context\": documents_text, \n",
    "                \"question\": user_input,\n",
    "                \"chat_history\": []\n",
    "            }) if documents_text else \"No relevant documents found.\"\n",
    "\n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        return response, documents_text, total_cost, total_tokens, completion_tokens\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response with error handling\n",
    "def get_response_multiquery_densex_routing(user_input):\n",
    "    try:\n",
    "        #==========================   Multi-Query - DenseX   =============================\n",
    "        #'''\n",
    "        #DenseX vectors store\n",
    "        densex_vectorstore = indexing.generate_final_vectorstore_with_chunks(user_input, test_routing_directory, initials.embedding)\n",
    "        densex_retriever = densex_vectorstore.as_retriever()\n",
    "\n",
    "        # Generate multiple queries using the multi_query_prompt and model\n",
    "        generate_multi_queries = (\n",
    "            prompts.multi_query_prompt \n",
    "            | initials.model \n",
    "            | StrOutputParser() \n",
    "            | (lambda x: x.split(\"\\n\"))  # Split the generated output into individual queries\n",
    "        )\n",
    "\n",
    "        # Generate the multiple queries based on user input\n",
    "        multiple_queries = generate_multi_queries.invoke({\"question\": user_input, \"question_history\": question_history})\n",
    "\n",
    "        # Now, use the generated queries to retrieve documents\n",
    "        # Now, use the generated queries to retrieve documents\n",
    "        if multiple_queries:\n",
    "            # Use retriever to fetch documents for each query\n",
    "            documents_text = []\n",
    "            for query in multiple_queries:\n",
    "                retrieved_docs = densex_retriever.get_relevant_documents(query)\n",
    "                # Join all retrieved documents into a single text string for each query result\n",
    "                docs_texts = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "                documents_text.append(docs_texts)\n",
    "\n",
    "        # Create prompt for final response generation\n",
    "        multi_query_rag_chain = (prompts.prompt_telekom | initials.model | StrOutputParser())\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            response = multi_query_rag_chain.invoke({\n",
    "                \"context\": documents_text, \n",
    "                \"question\": user_input,\n",
    "                \"chat_history\": []\n",
    "            }) if documents_text else \"No relevant documents found.\"\n",
    "\n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        return response, documents_text, total_cost, total_tokens, completion_tokens\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response with error handling\n",
    "def get_response_stepback(user_input):\n",
    "    try:\n",
    "        # Dosyaları listele\n",
    "        all_txt_files = [file for file in glob.glob(os.path.join(test_routing_directory, \"*.txt\")) if not file.endswith(\"_summary.txt\")]\n",
    "        question_history = []\n",
    "        # Seçilen dosyaların içeriklerini oku ve birleştir\n",
    "        all_texts = []\n",
    "        for file_path in all_txt_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                all_texts.append(f.read())\n",
    " \n",
    "        # Semantic Splitting\n",
    "        text_splitter_semantic = SemanticChunker(initials.embedding)\n",
    "        \n",
    "        #retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8, 'k': 5}) #add cos similarity filter, and k documents\n",
    "        \n",
    "        #==========================   Stepback   =============================\n",
    "\n",
    "        chunks = text_splitter_semantic.create_documents(all_texts)\n",
    "        print(\"==========   CHUNKS CREATED  ==========\")\n",
    "        vectorstore = Chroma.from_documents(documents=chunks, embedding=initials.embedding)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        # Generate step-back queries\n",
    "        generate_stepback_question = prompts.step_back_prompt | initials.model | StrOutputParser()\n",
    "        step_back_question = generate_stepback_question.invoke({\"question\": user_input, \"question_history\": question_history })\n",
    "        normal_context = retriever.invoke(user_input)\n",
    "        step_back_chain = (\n",
    "        {\n",
    "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "            \"normal_context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"step_back_context\": lambda x: retriever.invoke(x[\"step_back_question\"]),\n",
    "            \"question_history\": lambda x: x[\"question_history\"],\n",
    "        }\n",
    "            | prompts.stepback_response_prompt\n",
    "            | initials.model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        # OpenAI callback ile maliyet ve token takibi\n",
    "        with get_openai_callback() as cb:\n",
    "            response = step_back_chain.invoke({ \n",
    "                \"chat_history\": [],\n",
    "                \"question\": user_input,\n",
    "                \"step_back_question\": step_back_question,\n",
    "                \"question_history\": []\n",
    "            })\n",
    "\n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        return response, normal_context, total_cost, total_tokens, completion_tokens\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response with error handling\n",
    "def get_response_stepback_hybrid(user_input):\n",
    "    try:\n",
    "        # Dosyaları listele\n",
    "        all_txt_files = [file for file in glob.glob(os.path.join(test_routing_directory, \"*.txt\")) if not file.endswith(\"_summary.txt\")]\n",
    "        question_history = []\n",
    "        # Seçilen dosyaların içeriklerini oku ve birleştir\n",
    "        all_texts = []\n",
    "        for file_path in all_txt_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                all_texts.append(f.read())\n",
    " \n",
    "        # Semantic Splitting\n",
    "        text_splitter_semantic = SemanticChunker(initials.embedding)\n",
    "        \n",
    "        #retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8, 'k': 5}) #add cos similarity filter, and k documents\n",
    "        \n",
    "        #==========================   Stepback - Hybrid   =============================\n",
    "\n",
    "        chunks = text_splitter_semantic.create_documents(all_texts)\n",
    "        print(\"==========   CHUNKS CREATED  ==========\")\n",
    "        vectorstore = Chroma.from_documents(documents=chunks, embedding=initials.embedding)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        keyword_retriever = BM25Retriever.from_documents(chunks)\n",
    "        hybrid_retriever = EnsembleRetriever(retrievers=[keyword_retriever, retriever], weights=[1-VECTORSTORE_WEIGHT, VECTORSTORE_WEIGHT])\n",
    "        # Generate step-back queries\n",
    "        generate_stepback_question = prompts.step_back_prompt | initials.model | StrOutputParser()\n",
    "        step_back_question = generate_stepback_question.invoke({\"question\": user_input, \"question_history\": question_history })\n",
    "        normal_context = retriever.invoke(user_input)\n",
    "        step_back_chain = (\n",
    "        {\n",
    "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "            \"normal_context\": lambda x: hybrid_retriever.invoke(x[\"question\"]),\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"step_back_context\": lambda x: hybrid_retriever.invoke(x[\"step_back_question\"]),\n",
    "            \"question_history\": lambda x: x[\"question_history\"],\n",
    "        }\n",
    "            | prompts.stepback_response_prompt\n",
    "            | initials.model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        # OpenAI callback ile maliyet ve token takibi\n",
    "        with get_openai_callback() as cb:\n",
    "            response = step_back_chain.invoke({ \n",
    "                \"chat_history\": [],\n",
    "                \"question\": user_input,\n",
    "                \"step_back_question\": step_back_question,\n",
    "                \"question_history\": []\n",
    "            })\n",
    "\n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        return response, normal_context, total_cost, total_tokens, completion_tokens\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response with error handling\n",
    "def get_response_stepback_densex_routing(user_input):\n",
    "    try:\n",
    "        # Dosyaları listele\n",
    "        all_txt_files = [file for file in glob.glob(os.path.join(test_routing_directory, \"*.txt\")) if not file.endswith(\"_summary.txt\")]\n",
    "        question_history = []\n",
    "        # Seçilen dosyaların içeriklerini oku ve birleştir\n",
    "        all_texts = []\n",
    "        for file_path in all_txt_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                all_texts.append(f.read())\n",
    " \n",
    "        # Semantic Splitting\n",
    "        text_splitter_semantic = SemanticChunker(initials.embedding)\n",
    "        \n",
    "        #retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8, 'k': 5}) #add cos similarity filter, and k documents\n",
    "        \n",
    "        #==========================   Stepback - DenseX  =============================\n",
    "        #DenseX vectors store\n",
    "        densex_vectorstore = indexing.generate_final_vectorstore_with_chunks(user_input, test_routing_directory, initials.embedding)\n",
    "        densex_retriever = densex_vectorstore.as_retriever()\n",
    "\n",
    "        # Generate step-back queries\n",
    "        generate_stepback_question = prompts.step_back_prompt | initials.model | StrOutputParser()\n",
    "        step_back_question = generate_stepback_question.invoke({\"question\": user_input, \"question_history\": question_history })\n",
    "        normal_context = densex_retriever.invoke(user_input)\n",
    "        step_back_chain = (\n",
    "        {\n",
    "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "            \"normal_context\": lambda x: densex_retriever.invoke(x[\"question\"]),\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"step_back_context\": lambda x: densex_retriever.invoke(x[\"step_back_question\"]),\n",
    "            \"question_history\": lambda x: x[\"question_history\"],\n",
    "        }\n",
    "            | prompts.stepback_response_prompt\n",
    "            | initials.model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        # OpenAI callback ile maliyet ve token takibi\n",
    "        with get_openai_callback() as cb:\n",
    "            response = step_back_chain.invoke({ \n",
    "                \"chat_history\": [],\n",
    "                \"question\": user_input,\n",
    "                \"step_back_question\": step_back_question,\n",
    "                \"question_history\": []\n",
    "            })\n",
    "\n",
    "        total_tokens = cb.total_tokens\n",
    "        total_cost = cb.total_cost\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        return response, normal_context, total_cost, total_tokens, completion_tokens\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Documents could not be loaded. Please check the data directory path.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE get_response functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save evaluation data to CSV\n",
    "def save_evaluation_to_csv(evaluation_data, filename):\n",
    "    df = pd.DataFrame([evaluation_data])\n",
    "    df.to_csv(filename, mode='a', index=False, header=False)\n",
    "\n",
    "# Main execution\n",
    "def run_evaluations_from_csv(input_csv, output_csv):\n",
    "    # Directly create the output CSV file with headers at the beginning\n",
    "    initialize_output_csv(output_csv)\n",
    "\n",
    "    # Load questions from the CSV file\n",
    "    questions_df = pd.read_csv(input_csv)\n",
    "    \n",
    "    for index, row in questions_df.iterrows():\n",
    "        user_query = row['question']\n",
    "        start_time = time.time()  # Start timing\n",
    "        print(f\"Processing question {index + 1}/{len(questions_df)}: {user_query}\")\n",
    "\n",
    "        try:\n",
    "            # Get the response, generated queries, and retrieved documents\n",
    "            response, context, total_cost, total_tokens, completion_tokens = get_response_fusion_densex_routing(user_query)\n",
    "            print(\"==========  GENERATION   ==========\")\n",
    "\n",
    "            # Initialize metrics_results\n",
    "            metrics_results = None\n",
    "\n",
    "            print(\"==========   EVALUATION  ==========\")\n",
    "            # Evaluate metrics and retrieve dataset\n",
    "            metrics_results, dataset = evaluation.evaluate_result(user_query, response, context, input_csv)\n",
    "            print(f\"Metrics for question '{user_query}': {metrics_results}\")\n",
    "\n",
    "            if response:\n",
    "                # Calculate response time\n",
    "                response_time = time.time() - start_time\n",
    "                # Clear the system cache after processing the response\n",
    "                chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
    "\n",
    "                # Prepare data for CSV\n",
    "                if metrics_results is not None:\n",
    "                    # Extract contexts and ground_truth from the dataset\n",
    "                    contexts = dataset[\"contexts\"][0]  # Access first row's 'contexts'\n",
    "                    ground_truth = dataset[\"ground_truth\"][0]  # Access first row's 'ground_truth'\n",
    "                    \n",
    "                    evaluation_data = {\n",
    "                        'Question': user_query,\n",
    "                        'Response': response,\n",
    "                        'Contexts': contexts,\n",
    "                        'Ground Truth': ground_truth,\n",
    "                        'Token Count': total_tokens,\n",
    "                        'Total Cost (USD)': total_cost,\n",
    "                        'Completion Tokens': completion_tokens,\n",
    "                        'Number of Retrieved documents': len(context),\n",
    "                        'Response time': response_time,\n",
    "                        'answer_relevancy': metrics_results.get('answer_relevancy'),\n",
    "                        'context_precision': metrics_results.get('context_precision'),\n",
    "                        'context_recall': metrics_results.get('context_recall'),\n",
    "                        'faithfulness': metrics_results.get('faithfulness'),\n",
    "                        'BleuScore': metrics_results.get('bleu_score'),\n",
    "                        'RougeScore': metrics_results.get('rouge_score'),\n",
    "\n",
    "                    }\n",
    "\n",
    "                    # Save the evaluation data to CSV\n",
    "                    save_evaluation_to_csv(evaluation_data, output_csv)\n",
    "                    print(f\"Evaluation metrics saved for question '{user_query}'.\")\n",
    "\n",
    "            print(\"==========   PROCESS ENDED  ==========\\n\")\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError for question {index + 1}: {ve}\")\n",
    "            print(\"Skipping to the next question...\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for question {index + 1}: {e}\")\n",
    "            print(\"Skipping to the next question...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output file at: test_data_routing/_evaluation_advanced_fusion_densex_routing.csv\n",
      "Processing question 1/10: What ist die Bedeutung der eSIM für die Verbindung der Samsung Galaxy Watch mit dem Mobilfunknetz?\n",
      "========== NUMBER OF DOCUMENTS RETRIEVED: 1 ==========\n",
      "Warning: Only 1 unique results were found after 5 iterations.\n",
      "========== 1 DOCUMENTS SUCCESSFULLY LOADED ==========\n",
      "========== SEMANTIC CHUNKING IN PROGRESS ==========\n",
      "========== TOTAL CHUNKS CREATED: 2 ==========\n",
      "========== FINAL VECTOR STORE WITH CHUNKS CREATED ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========  GENERATION   ==========\n",
      "==========   EVALUATION  ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:11<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for question 'What ist die Bedeutung der eSIM für die Verbindung der Samsung Galaxy Watch mit dem Mobilfunknetz?': {'answer_relevancy': 0.9348684240306048, 'context_precision': 0.9999999999, 'context_recall': 1.0, 'faithfulness': 1.0, 'bleu_score': 0.24660313247404905, 'rouge_score': 0.4297520661157025}\n",
      "Evaluation metrics saved for question 'What ist die Bedeutung der eSIM für die Verbindung der Samsung Galaxy Watch mit dem Mobilfunknetz?'.\n",
      "==========   PROCESS ENDED  ==========\n",
      "\n",
      "Processing question 2/10: Kann ich eine PlusKarte ohne Mindestvertragslaufzeit buchen?\n",
      "========== NUMBER OF DOCUMENTS RETRIEVED: 30 ==========\n",
      "========== 30 DOCUMENTS SUCCESSFULLY LOADED ==========\n",
      "========== SEMANTIC CHUNKING IN PROGRESS ==========\n",
      "========== TOTAL CHUNKS CREATED: 62 ==========\n",
      "========== FINAL VECTOR STORE WITH CHUNKS CREATED ==========\n",
      "==========  GENERATION   ==========\n",
      "==========   EVALUATION  ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:14<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for question 'Kann ich eine PlusKarte ohne Mindestvertragslaufzeit buchen?': {'answer_relevancy': 0.9999999999999994, 'context_precision': 0.8261904761739525, 'context_recall': 1.0, 'faithfulness': 1.0, 'bleu_score': 0.1782921965574844, 'rouge_score': 0.4406779661016949}\n",
      "Evaluation metrics saved for question 'Kann ich eine PlusKarte ohne Mindestvertragslaufzeit buchen?'.\n",
      "==========   PROCESS ENDED  ==========\n",
      "\n",
      "Processing question 3/10: What advantages does the A15 Bionic Prozessor offer in the iPhone 13 series compared to its predecessor?\n",
      "========== NUMBER OF DOCUMENTS RETRIEVED: 28 ==========\n",
      "Warning: Only 28 unique results were found after 5 iterations.\n",
      "========== 28 DOCUMENTS SUCCESSFULLY LOADED ==========\n",
      "========== SEMANTIC CHUNKING IN PROGRESS ==========\n",
      "========== TOTAL CHUNKS CREATED: 61 ==========\n",
      "========== FINAL VECTOR STORE WITH CHUNKS CREATED ==========\n",
      "==========  GENERATION   ==========\n",
      "==========   EVALUATION  ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:28<00:00,  7.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for question 'What advantages does the A15 Bionic Prozessor offer in the iPhone 13 series compared to its predecessor?': {'answer_relevancy': 0.9657319139749733, 'context_precision': 0.49999999995, 'context_recall': 1.0, 'faithfulness': 0.0, 'bleu_score': 0.009192577995194304, 'rouge_score': 0.09090909090909091}\n",
      "Evaluation metrics saved for question 'What advantages does the A15 Bionic Prozessor offer in the iPhone 13 series compared to its predecessor?'.\n",
      "==========   PROCESS ENDED  ==========\n",
      "\n",
      "Processing question 4/10: What steps are involved in updating bank connections in the Kundencenter and MeinMagenta App?\n",
      "========== NUMBER OF DOCUMENTS RETRIEVED: 30 ==========\n",
      "========== 30 DOCUMENTS SUCCESSFULLY LOADED ==========\n",
      "========== SEMANTIC CHUNKING IN PROGRESS ==========\n",
      "========== TOTAL CHUNKS CREATED: 60 ==========\n",
      "========== FINAL VECTOR STORE WITH CHUNKS CREATED ==========\n",
      "==========  GENERATION   ==========\n",
      "==========   EVALUATION  ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:23<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for question 'What steps are involved in updating bank connections in the Kundencenter and MeinMagenta App?': {'answer_relevancy': 0.9430972584807084, 'context_precision': 0.5888888888692593, 'context_recall': 1.0, 'faithfulness': 1.0, 'bleu_score': 0.0022866565934025434, 'rouge_score': 0.24242424242424243}\n",
      "Evaluation metrics saved for question 'What steps are involved in updating bank connections in the Kundencenter and MeinMagenta App?'.\n",
      "==========   PROCESS ENDED  ==========\n",
      "\n",
      "Processing question 5/10: What sind die Kosten für die Mitnahme meiner Mobilfunk-Rufnummer zu einem anderen Anbieter?\n",
      "========== NUMBER OF DOCUMENTS RETRIEVED: 23 ==========\n",
      "Warning: Only 23 unique results were found after 5 iterations.\n",
      "========== 23 DOCUMENTS SUCCESSFULLY LOADED ==========\n",
      "========== SEMANTIC CHUNKING IN PROGRESS ==========\n",
      "========== TOTAL CHUNKS CREATED: 53 ==========\n",
      "========== FINAL VECTOR STORE WITH CHUNKS CREATED ==========\n",
      "==========  GENERATION   ==========\n",
      "==========   EVALUATION  ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for question 'What sind die Kosten für die Mitnahme meiner Mobilfunk-Rufnummer zu einem anderen Anbieter?': {'answer_relevancy': 0.9607084945792637, 'context_precision': 0.4499999999775, 'context_recall': 1.0, 'faithfulness': 1.0, 'bleu_score': 0.22950595360157608, 'rouge_score': 0.4444444444444445}\n",
      "Evaluation metrics saved for question 'What sind die Kosten für die Mitnahme meiner Mobilfunk-Rufnummer zu einem anderen Anbieter?'.\n",
      "==========   PROCESS ENDED  ==========\n",
      "\n",
      "Processing question 6/10: Wie aktiviere ich VoLTE, wenn LTE fehlt?\n",
      "========== NUMBER OF DOCUMENTS RETRIEVED: 27 ==========\n",
      "Warning: Only 27 unique results were found after 5 iterations.\n",
      "========== 27 DOCUMENTS SUCCESSFULLY LOADED ==========\n",
      "========== SEMANTIC CHUNKING IN PROGRESS ==========\n",
      "========== TOTAL CHUNKS CREATED: 62 ==========\n",
      "========== FINAL VECTOR STORE WITH CHUNKS CREATED ==========\n",
      "==========  GENERATION   ==========\n",
      "==========   EVALUATION  ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:21<00:00,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for question 'Wie aktiviere ich VoLTE, wenn LTE fehlt?': {'answer_relevancy': 0.8833387918075685, 'context_precision': 0.99999999995, 'context_recall': 1.0, 'faithfulness': 0.9285714285714286, 'bleu_score': 0.0015654934906083598, 'rouge_score': 0.054945054945054944}\n",
      "Evaluation metrics saved for question 'Wie aktiviere ich VoLTE, wenn LTE fehlt?'.\n",
      "==========   PROCESS ENDED  ==========\n",
      "\n",
      "Processing question 7/10: Wie kann die eSIM der Galaxy Watch online ohne Smartphone sein?\n",
      "========== NUMBER OF DOCUMENTS RETRIEVED: 19 ==========\n",
      "Warning: Only 19 unique results were found after 5 iterations.\n",
      "========== 19 DOCUMENTS SUCCESSFULLY LOADED ==========\n",
      "========== SEMANTIC CHUNKING IN PROGRESS ==========\n",
      "========== TOTAL CHUNKS CREATED: 42 ==========\n",
      "========== FINAL VECTOR STORE WITH CHUNKS CREATED ==========\n",
      "==========  GENERATION   ==========\n",
      "==========   EVALUATION  ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:17<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for question 'Wie kann die eSIM der Galaxy Watch online ohne Smartphone sein?': {'answer_relevancy': 0.8738311653523044, 'context_precision': 0.6666666666444444, 'context_recall': 1.0, 'faithfulness': 1.0, 'bleu_score': 0.06527622361665518, 'rouge_score': 0.20408163265306123}\n",
      "Evaluation metrics saved for question 'Wie kann die eSIM der Galaxy Watch online ohne Smartphone sein?'.\n",
      "==========   PROCESS ENDED  ==========\n",
      "\n",
      "Processing question 8/10: Why was the 10th Sept meeting on the mobile plan canceled?\n",
      "========== NUMBER OF DOCUMENTS RETRIEVED: 29 ==========\n",
      "Warning: Only 29 unique results were found after 5 iterations.\n",
      "========== 29 DOCUMENTS SUCCESSFULLY LOADED ==========\n",
      "========== SEMANTIC CHUNKING IN PROGRESS ==========\n",
      "========== TOTAL CHUNKS CREATED: 58 ==========\n",
      "========== FINAL VECTOR STORE WITH CHUNKS CREATED ==========\n",
      "==========  GENERATION   ==========\n",
      "==========   EVALUATION  ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:19<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for question 'Why was the 10th Sept meeting on the mobile plan canceled?': {'answer_relevancy': 0.0, 'context_precision': 0.49999999995, 'context_recall': 1.0, 'faithfulness': 0.3333333333333333, 'bleu_score': 0.004867213525910928, 'rouge_score': 0.05194805194805195}\n",
      "Evaluation metrics saved for question 'Why was the 10th Sept meeting on the mobile plan canceled?'.\n",
      "==========   PROCESS ENDED  ==========\n",
      "\n",
      "Processing question 9/10: Wie wird die eSIM zugestellt und was ist die Rolle des QR-Codes bei der Installation?\n",
      "========== NUMBER OF DOCUMENTS RETRIEVED: 30 ==========\n",
      "========== 30 DOCUMENTS SUCCESSFULLY LOADED ==========\n",
      "========== SEMANTIC CHUNKING IN PROGRESS ==========\n",
      "========== TOTAL CHUNKS CREATED: 68 ==========\n",
      "========== FINAL VECTOR STORE WITH CHUNKS CREATED ==========\n",
      "==========  GENERATION   ==========\n",
      "==========   EVALUATION  ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:12<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for question 'Wie wird die eSIM zugestellt und was ist die Rolle des QR-Codes bei der Installation?': {'answer_relevancy': 0.9620994766805503, 'context_precision': 0.8333333332916666, 'context_recall': 1.0, 'faithfulness': 0.875, 'bleu_score': 0.1394434502652766, 'rouge_score': 0.2975206611570248}\n",
      "Evaluation metrics saved for question 'Wie wird die eSIM zugestellt und was ist die Rolle des QR-Codes bei der Installation?'.\n",
      "==========   PROCESS ENDED  ==========\n",
      "\n",
      "Processing question 10/10: What MagentaMobil plans are available for no-commitment users?\n",
      "========== NUMBER OF DOCUMENTS RETRIEVED: 29 ==========\n",
      "Warning: Only 29 unique results were found after 5 iterations.\n",
      "========== 29 DOCUMENTS SUCCESSFULLY LOADED ==========\n",
      "========== SEMANTIC CHUNKING IN PROGRESS ==========\n",
      "========== TOTAL CHUNKS CREATED: 61 ==========\n",
      "========== FINAL VECTOR STORE WITH CHUNKS CREATED ==========\n",
      "==========  GENERATION   ==========\n",
      "==========   EVALUATION  ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:15<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for question 'What MagentaMobil plans are available for no-commitment users?': {'answer_relevancy': 0.8964403151650057, 'context_precision': 0.0, 'context_recall': 0.5, 'faithfulness': 0.8888888888888888, 'bleu_score': 0.0020632861907783676, 'rouge_score': 0.09876543209876544}\n",
      "Evaluation metrics saved for question 'What MagentaMobil plans are available for no-commitment users?'.\n",
      "==========   PROCESS ENDED  ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluations\n",
    "run_evaluations_from_csv(input_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Formatted averages saved to test_data_routing/_results__evaluation_advanced_fusion_densex_routing.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'test_data_routing/_evaluation_advanced_fusion_densex_routing.csv'  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# List of numeric columns to calculate averages\n",
    "numeric_columns = [\n",
    "    'Token Count', 'Total Cost (USD)', 'Completion Tokens',\n",
    "    'Number of Retrieved documents', 'Response time',\n",
    "    'answer_relevancy', 'context_precision', 'context_recall',\n",
    "    'faithfulness', 'BleuScore', 'RougeScore'\n",
    "]\n",
    "\n",
    "# Calculate the mean for each numeric column\n",
    "averages = df[numeric_columns].mean()\n",
    "\n",
    "# Formatting the averages according to your requirements\n",
    "formatted_averages = {\n",
    "    'Token Count': f\"{averages['Token Count']:.0f}\",  # No decimal places\n",
    "    'Total Cost (USD)': f\"{averages['Total Cost (USD)']:.5f}\",  # Keep as is\n",
    "    'Completion Tokens': f\"{averages['Completion Tokens']:.0f}\",  # No decimal places\n",
    "    'Number of Retrieved documents': f\"{averages['Number of Retrieved documents']}\",  # Keep as is\n",
    "    'Response time': f\"{averages['Response time']:.2f}\",  # One decimal place\n",
    "    'answer_relevancy': f\"{averages['answer_relevancy']:.4f}\",  # Four decimal places\n",
    "    'context_precision': f\"{averages['context_precision']:.4f}\",  # Four decimal places\n",
    "    'context_recall': f\"{averages['context_recall']:.4f}\",  # Four decimal places\n",
    "    'faithfulness': f\"{averages['faithfulness']:.4f}\",  # Four decimal places\n",
    "    'BleuScore': f\"{averages['BleuScore']:.4f}\",  # Four decimal places\n",
    "    'RougeScore': f\"{averages['RougeScore']:.4f}\"  # Four decimal places\n",
    "}\n",
    "\n",
    "# Convert formatted averages to a DataFrame for saving\n",
    "formatted_averages_df = pd.DataFrame([formatted_averages])\n",
    "\n",
    "# Define the output file path by adding \"results_\" prefix\n",
    "output_file_path = os.path.join(\n",
    "    os.path.dirname(file_path), \n",
    "    f\"_results_{os.path.basename(file_path)}\"\n",
    ")\n",
    "\n",
    "# Save the formatted averages to CSV\n",
    "formatted_averages_df.to_csv(output_file_path, index=False)\n",
    "print(f\"[INFO] Formatted averages saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
