{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web data from __[telekom.de](https://www.telekom.de/hilfe)__\n",
    "\n",
    "\n",
    "### Description\n",
    "This function processes `.txt` files in a specified directory to extract relevant information and write it to different output files based on their content. It categorizes the files into three categories:\n",
    "1. **Files with valid questions and answers**: Written to `web_data.txt`.\n",
    "2. **Files that could not be processed**: Written to `filtered.txt`.\n",
    "3. **Files with questions containing `?` but not ending with `?`**: Written to `inspecting.txt`.\n",
    "\n",
    "### Inputs\n",
    "- **`directory_path`**: Path to the directory containing `.txt` files.\n",
    "- **`web_data_file`**: Path to the output file for valid questions and answers.\n",
    "- **`filtered_file`**: Path to the output file for files that couldn't be processed.\n",
    "- **`inspecting_file`**: Path to the output file for questions containing `?` but not ending with `?`.\n",
    "\n",
    "### Outputs\n",
    "- **`web_data.txt`**: Contains the valid question and answer data.\n",
    "- **`filtered.txt`**: Contains filenames of files that couldn't be processed.\n",
    "- **`inspecting.txt`**: Contains questions with `?` but not ending with `?` along with their details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Define fixed paths for the input directory and output files\n",
    "directory_path = \"/Users/taha/Desktop/scrapeV2/output_folder\"\n",
    "web_data_directory = \"web_data\"\n",
    "web_data_file = os.path.join(web_data_directory, \"web_data.txt\")\n",
    "filtered_file = os.path.join(web_data_directory, \"filtered.txt\")\n",
    "inspecting_file = os.path.join(web_data_directory, \"inspecting.txt\")\n",
    "\n",
    "def extract_and_write_data(directory_path, web_data_file, filtered_file, inspecting_file):\n",
    "    \"\"\"\n",
    "    Extracts and categorizes data from .txt files in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path (str): Path to the directory containing .txt files.\n",
    "    - web_data_file (str): Path to the output file for valid questions and answers.\n",
    "    - filtered_file (str): Path to the output file for files that couldn't be processed.\n",
    "    - inspecting_file (str): Path to the output file for questions containing `?` but not ending with `?`.\n",
    "    \n",
    "    Outputs:\n",
    "    - Writes valid questions and answers to web_data_file.\n",
    "    - Writes filenames of unprocessed files to filtered_file.\n",
    "    - Writes questions with `?` but not ending with `?` to inspecting_file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Regex pattern to find the section starting with \"...Telekom\" and ending with two spaces\n",
    "    pattern = r\"\\.\\.\\.Telekom.*?\\s{2}\"\n",
    "    \n",
    "    # Initialize counters and lists to keep track of files and their processing\n",
    "    total_files = 0\n",
    "    processed_files = 0\n",
    "    processed_files_data = []\n",
    "    unprocessed_files = []\n",
    "    inspecting_files = []\n",
    "\n",
    "    def process_file(filename):\n",
    "        \"\"\"\n",
    "        Processes each .txt file to extract and categorize content based on patterns.\n",
    "        \n",
    "        Parameters:\n",
    "        - filename (str): The name of the file to be processed.\n",
    "        \"\"\"\n",
    "        nonlocal processed_files\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as source:\n",
    "                content = source.read()\n",
    "                match = re.search(pattern, content, re.DOTALL)\n",
    "                if match:\n",
    "                    processed_files += 1\n",
    "                    matched_text = match.group(0)\n",
    "                    start_index = match.end()\n",
    "\n",
    "                    # Clean and format the matched text\n",
    "                    cleaned_text = re.sub(r'\\.\\.\\.+', '\\n', matched_text.strip(\".\").strip())\n",
    "                    \n",
    "                    # Extract the text after the matched pattern\n",
    "                    post_pattern_text = content[start_index:]\n",
    "                    \n",
    "                    # Find paragraphs separated by multiple newlines\n",
    "                    paragraph_pattern = r'([^\\n]+(?:\\n[^\\n]+)*)(?:\\n{2,})'\n",
    "                    paragraphs = re.findall(paragraph_pattern, post_pattern_text)\n",
    "                    \n",
    "                    if len(paragraphs) >= 2:\n",
    "                        question = paragraphs[0].strip()\n",
    "                        answer = paragraphs[1].strip()\n",
    "                        \n",
    "                        if question.endswith('?'):\n",
    "                            processed_files_data.append((filename, cleaned_text, question, answer))\n",
    "                        else:\n",
    "                            if '?' in question:\n",
    "                                inspecting_files.append(f\"File: {filename}\\nNavigation:\\n{cleaned_text}\\n\\nQuestion: {question}\\n\\nAnswer: {answer}\\n\")\n",
    "                    else:\n",
    "                        unprocessed_files.append(filename)\n",
    "                else:\n",
    "                    unprocessed_files.append(filename)\n",
    "        except Exception as e:\n",
    "            unprocessed_files.append(filename)\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"Directory {directory_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Create web_data directory if it does not exist\n",
    "    os.makedirs(web_data_directory, exist_ok=True)\n",
    "\n",
    "    # Process each .txt file in the directory\n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            total_files += 1\n",
    "            process_file(filename)\n",
    "\n",
    "    # Write valid question and answer data to web_data_file\n",
    "    with open(web_data_file, \"w\", encoding=\"utf-8\") as web_data_txt:\n",
    "        for filename, navigation, question, answer in sorted(processed_files_data, key=lambda x: x[0]):\n",
    "            web_data_txt.write(f\"File: {filename}\\n\")\n",
    "            web_data_txt.write(f\"Navigation:\\n{navigation}\\n\\n\")\n",
    "            web_data_txt.write(f\"Question: {question}\\n\\n\")\n",
    "            web_data_txt.write(f\"Answer: {answer}\\n\")\n",
    "            web_data_txt.write(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # Write filenames of unprocessed files to filtered_file\n",
    "    with open(filtered_file, \"w\", encoding=\"utf-8\") as filtered_txt:\n",
    "        for filename in sorted(unprocessed_files):\n",
    "            filtered_txt.write(f\"{filename}\\n\")\n",
    "\n",
    "    # Write questions with `?` but not ending with `?` to inspecting_file\n",
    "    with open(inspecting_file, \"w\", encoding=\"utf-8\") as inspecting_txt:\n",
    "        inspecting_txt.writelines(inspecting_files)\n",
    "\n",
    "    # Calculate numbers for print statements\n",
    "    num_unprocessed_files = len(unprocessed_files)\n",
    "    num_inspecting_files = len(inspecting_files)\n",
    "    num_processed_files = len(processed_files_data)\n",
    "    missing_files = total_files - (num_processed_files + num_unprocessed_files + num_inspecting_files)\n",
    "\n",
    "    # Print summary of the processing\n",
    "    print(\"Process completed.\")\n",
    "    print(f\"Total number of .txt files in the folder: {total_files}\")\n",
    "    print(f\"Number of .txt files processed and written to {web_data_file}: {num_processed_files}\")\n",
    "    print(f\"Unprocessed files have been written to {filtered_file}. Number of unprocessed files: {num_unprocessed_files}\")\n",
    "    print(f\"Files with questions containing '?' but not ending with '?' have been written to {inspecting_file}. Number of inspecting files: {num_inspecting_files}\")\n",
    "    print(f\"Number of missing or unaccounted files: {missing_files}\")\n",
    "\n",
    "# Call the function with the specified parameters\n",
    "extract_and_write_data(directory_path, web_data_file, filtered_file, inspecting_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML üzerinden metinleri arayan bir kod, calisiyor incelenecek..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 1. Sitemap'ten URL'leri alma\n",
    "sitemap_url = 'https://www.telekom.de/ueber-das-unternehmen/robots/sitemap'\n",
    "print(f\"Sitemap URL'sine istek gönderiliyor: {sitemap_url}\")\n",
    "response = requests.get(sitemap_url)\n",
    "\n",
    "# XML parser olarak 'lxml' kullanarak sitemap içeriğini işleme\n",
    "soup = BeautifulSoup(response.content, 'xml')\n",
    "print(\"Sitemap XML içeriği işleniyor...\")\n",
    "\n",
    "# 'loc' etiketlerini bulma\n",
    "urls = [url.text for url in soup.find_all('loc') if url is not None]\n",
    "print(f\"{len(urls)} URL bulundu.\")\n",
    "\n",
    "def extract_question_answer(soup):\n",
    "    question_answer_pairs = []\n",
    "    \n",
    "    # Cevapları toplamak için kullanılan liste\n",
    "    def get_text_from_element(element):\n",
    "        text = ''\n",
    "        # <p> ve <ul><li> elementlerinden metinleri toplama\n",
    "        for p in element.find_all('p'):\n",
    "            text += p.get_text(strip=True) + '\\n'\n",
    "        for ul in element.find_all('ul'):\n",
    "            for li in ul.find_all('li'):\n",
    "                text += f\"• {li.get_text(strip=True)}\\n\"\n",
    "        return text.strip()\n",
    "\n",
    "    # Hariç tutulacak sınıflar\n",
    "    excluded_classes = [\n",
    "        \"chf-navigation-bar\",\n",
    "        \"direct-access-container\",\n",
    "        \"direct-access-content\",\n",
    "        \"collection-wrapper collection collection-standard\",\n",
    "        \"collection-wrapper collection collection-standard l-outer l-outer--solutionPage\"\n",
    "    ]\n",
    "    \n",
    "    # Hariç tutulacak sınıflara sahip elemanları kaldır\n",
    "    def remove_excluded_elements(soup):\n",
    "        for class_name in excluded_classes:\n",
    "            for element in soup.find_all(class_=class_name):\n",
    "                if element:  # Element geçerli olup olmadığını kontrol et\n",
    "                    element.decompose()\n",
    "    \n",
    "    # Elementlerin içeriğini de kontrol etme\n",
    "    def remove_nested_excluded_elements(soup):\n",
    "        for element in soup.find_all(True):\n",
    "            if isinstance(element, BeautifulSoup):  # Elementin BeautifulSoup nesnesi olup olmadığını kontrol et\n",
    "                classes = element.get('class', [])\n",
    "                if classes and any(cls in ' '.join(classes) for cls in excluded_classes):\n",
    "                    element.decompose()\n",
    "    \n",
    "    remove_excluded_elements(soup)\n",
    "    remove_nested_excluded_elements(soup)\n",
    "\n",
    "    # Soru ve cevapları belirli elementlerden çekme\n",
    "    questions = soup.find_all(['h1', 'h2', 'h3'])\n",
    "    \n",
    "    for question in questions:\n",
    "        question_text = question.get_text(strip=True)\n",
    "        if question_text.endswith('?'):\n",
    "            answer_text = ''\n",
    "            # Önce div.outerRichtextDiv içinde cevap arama\n",
    "            next_div = question.find_next('div', class_='outerRichtextDiv')\n",
    "            if next_div:\n",
    "                answer_text = get_text_from_element(next_div)\n",
    "            # Eğer div.outerRichtextDiv içinde cevap bulunamadıysa, diğer elemanlarda arama\n",
    "            if not answer_text:\n",
    "                next_div = question.find_next('div')\n",
    "                if next_div and not any(cls in ' '.join(next_div.get('class', [])) for cls in excluded_classes):\n",
    "                    answer_text = get_text_from_element(next_div)\n",
    "            if answer_text:\n",
    "                question_answer_pairs.append({'question': question_text, 'answer': answer_text})\n",
    "\n",
    "    return question_answer_pairs\n",
    "\n",
    "# 3. Tüm URL'leri ziyaret edip soru-cevapları çekme ve yalnızca soru-cevap içeren sayfalar için dosya oluşturma\n",
    "output_dir = Path(\"data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)  # Klasör oluşturma\n",
    "\n",
    "for idx, url in enumerate(urls, 1):\n",
    "    print(f\"{idx}/{len(urls)} URL işleniyor: {url}\")\n",
    "    \n",
    "    try:\n",
    "        # Redirect döngüsünden kaçınmak için yönlendirmeleri devre dışı bırak\n",
    "        response = requests.get(url, allow_redirects=False)\n",
    "        \n",
    "        # Eğer yönlendirme varsa, durum kodunu kontrol et\n",
    "        if response.status_code == 301 or response.status_code == 302:\n",
    "            print(\"   Yönlendirme tespit edildi, URL kontrol ediliyor.\")\n",
    "            final_url = response.headers.get('Location')\n",
    "            if final_url:\n",
    "                response = requests.get(final_url)\n",
    "        elif response.status_code == 200:\n",
    "            response = requests.get(url)\n",
    "        else:\n",
    "            print(\"   Hatalı URL veya erişim problemi.\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        qa_pairs = extract_question_answer(soup)\n",
    "        print(f\"   {len(qa_pairs)} soru-cevap çifti bulundu.\")\n",
    "        \n",
    "        if qa_pairs:\n",
    "            file_name = re.sub(r'\\W+', '_', url) + \".txt\"\n",
    "            output_file = output_dir / file_name\n",
    "            \n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(f\"Kaynak URL: {url}\\n\\n\")\n",
    "                for idx, qa in enumerate(qa_pairs, 1):\n",
    "                    file.write(f\"{idx}. Soru: {qa['question']}\\n   Cevap: {qa['answer']}\\n\\n\")\n",
    "            \n",
    "            print(f\"   Sonuçlar '{output_file}' dosyasına kaydedildi.\")\n",
    "        else:\n",
    "            print(\"   Soru-cevap çifti bulunamadı, dosya oluşturulmayacak.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"   Hata oluştu: {e}\")\n",
    "\n",
    "print(\"İşlem tamamlandı! Sadece soru-cevap içeren sayfalar için sonuçlar 'data' klasörüne kaydedildi.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
