{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for Question-Answer Extraction\n",
    "\n",
    "This notebook demonstrates how to scrape web pages for question-answer pairs, excluding specific sections and classes using Python. It utilizes `requests` and `BeautifulSoup` libraries for HTTP requests and HTML parsing.\n",
    "\n",
    "The code performs the following steps:\n",
    "\n",
    "1. **Fetch URLs from Sitemap**:\n",
    "   - Requests the sitemap URL to get a list of URLs to scrape.\n",
    "\n",
    "2. **Extract Question-Answer Pairs**:\n",
    "   - Defines a function to extract question-answer pairs from the HTML content.\n",
    "   - Removes specific sections and elements that should be excluded based on their class names.\n",
    "   - Collects questions and their corresponding answers from the page.\n",
    "\n",
    "3. **Process Each URL**:\n",
    "   - Iterates through the list of URLs.\n",
    "   - Requests each URL and processes the HTML content.\n",
    "   - Saves the extracted question-answer pairs to text files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 1. Fetch URLs from Sitemap\n",
    "sitemap_url = 'https://www.telekom.de/ueber-das-unternehmen/robots/sitemap'\n",
    "print(f\"Requesting sitemap URL: {sitemap_url}\")\n",
    "response = requests.get(sitemap_url)\n",
    "\n",
    "# Parse XML sitemap\n",
    "soup = BeautifulSoup(response.content, 'xml')\n",
    "print(\"Processing sitemap XML...\")\n",
    "\n",
    "# Extract URLs\n",
    "urls = [url.text for url in soup.find_all('loc') if url is not None]\n",
    "print(f\"{len(urls)} URLs found.\")\n",
    "\n",
    "def extract_question_answer(soup):\n",
    "    question_answer_pairs = []\n",
    "    \n",
    "    # Function to get text from element\n",
    "    def get_text_from_element(element):\n",
    "        text = ''\n",
    "        for p in element.find_all('p'):\n",
    "            text += p.get_text(strip=True) + '\\n'\n",
    "        for ul in element.find_all('ul'):\n",
    "            for li in ul.find_all('li'):\n",
    "                text += f\"• {li.get_text(strip=True)}\\n\"\n",
    "        return text.strip()\n",
    "\n",
    "    # Define excluded classes\n",
    "    excluded_classes = [\n",
    "        \"chf-navigation-bar\",\n",
    "        \"direct-access-container\",\n",
    "        \"direct-access-content\",\n",
    "        \"collection-wrapper collection collection-standard\",\n",
    "        \"collection-wrapper collection collection-standard l-outer l-outer--solutionPage\"\n",
    "    ]\n",
    "    \n",
    "    # Function to remove excluded elements\n",
    "    def remove_excluded_elements(soup):\n",
    "        for class_name in excluded_classes:\n",
    "            for element in soup.find_all(class_=class_name):\n",
    "                if element:\n",
    "                    element.decompose()\n",
    "    \n",
    "    # Function to remove nested excluded elements\n",
    "    def remove_nested_excluded_elements(soup):\n",
    "        for element in soup.find_all(True):\n",
    "            if isinstance(element, BeautifulSoup):\n",
    "                classes = element.get('class', [])\n",
    "                if classes and any(cls in ' '.join(classes) for cls in excluded_classes):\n",
    "                    element.decompose()\n",
    "    \n",
    "    remove_excluded_elements(soup)\n",
    "    remove_nested_excluded_elements(soup)\n",
    "\n",
    "    # Extract questions and answers\n",
    "    questions = soup.find_all(['h1', 'h2', 'h3'])\n",
    "    \n",
    "    for question in questions:\n",
    "        question_text = question.get_text(strip=True)\n",
    "        if question_text.endswith('?'):\n",
    "            answer_text = ''\n",
    "            # Check for answer in next div\n",
    "            next_div = question.find_next('div', class_='outerRichtextDiv')\n",
    "            if next_div:\n",
    "                answer_text = get_text_from_element(next_div)\n",
    "            if not answer_text:\n",
    "                next_div = question.find_next('div')\n",
    "                if next_div and not any(cls in ' '.join(next_div.get('class', [])) for cls in excluded_classes):\n",
    "                    answer_text = get_text_from_element(next_div)\n",
    "            if answer_text:\n",
    "                question_answer_pairs.append({'question': question_text, 'answer': answer_text})\n",
    "\n",
    "    return question_answer_pairs\n",
    "\n",
    "# 3. Process each URL\n",
    "output_dir = Path(\"data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)  # Create directory\n",
    "\n",
    "for idx, url in enumerate(urls, 1):\n",
    "    print(f\"Processing URL {idx}/{len(urls)}: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, allow_redirects=False)\n",
    "        \n",
    "        if response.status_code == 301 or response.status_code == 302:\n",
    "            print(\"   Redirect detected, checking URL.\")\n",
    "            final_url = response.headers.get('Location')\n",
    "            if final_url:\n",
    "                response = requests.get(final_url)\n",
    "        elif response.status_code == 200:\n",
    "            response = requests.get(url)\n",
    "        else:\n",
    "            print(\"   Invalid URL or access problem.\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        qa_pairs = extract_question_answer(soup)\n",
    "        print(f\"   {len(qa_pairs)} question-answer pairs found.\")\n",
    "        \n",
    "        if qa_pairs:\n",
    "            file_name = re.sub(r'\\W+', '_', url) + \".txt\"\n",
    "            output_file = output_dir / file_name\n",
    "            \n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(f\"Source URL: {url}\\n\\n\")\n",
    "                for idx, qa in enumerate(qa_pairs, 1):\n",
    "                    file.write(f\"{idx}. Question: {qa['question']}\\n   Answer: {qa['answer']}\\n\\n\")\n",
    "            \n",
    "            print(f\"   Results saved to '{output_file}'.\")\n",
    "        else:\n",
    "            print(\"   No question-answer pairs found, file will not be created.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"   Error occurred: {e}\")\n",
    "\n",
    "print(\"Processing completed! Results for pages with question-answer pairs saved in 'data' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YEDEK OLARAK TUTUYORUM, GÜNCEL DEGIL! Accordion listeleri ekledim koda, 1200 cekiyordu simdi daha fazla olmali daha sonra kontrol et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 1. Fetch URLs from Sitemap\n",
    "sitemap_url = 'https://www.telekom.de/ueber-das-unternehmen/robots/sitemap'\n",
    "print(f\"Requesting sitemap URL: {sitemap_url}\")\n",
    "response = requests.get(sitemap_url)\n",
    "\n",
    "# Parse XML sitemap\n",
    "soup = BeautifulSoup(response.content, 'xml')\n",
    "print(\"Processing sitemap XML...\")\n",
    "\n",
    "# Extract URLs\n",
    "urls = [url.text for url in soup.find_all('loc') if url is not None]\n",
    "print(f\"{len(urls)} URLs found.\")\n",
    "\n",
    "# Function to extract question-answer pairs from the page, including accordion lists\n",
    "def extract_question_answer(soup):\n",
    "    question_answer_pairs = []\n",
    "\n",
    "    # Function to get text from element\n",
    "    def get_text_from_element(element):\n",
    "        text = ''\n",
    "        for p in element.find_all('p'):\n",
    "            text += p.get_text(strip=True) + '\\n'\n",
    "        for ul in element.find_all('ul'):\n",
    "            for li in ul.find_all('li'):\n",
    "                text += f\"• {li.get_text(strip=True)}\\n\"\n",
    "        return text.strip()\n",
    "\n",
    "    # Function to extract accordion items\n",
    "    def extract_accordion_items(accordion_list):\n",
    "        for item in accordion_list.find_all('li', class_='accordion-item'):\n",
    "            question = item.find('p', class_='accordion-item__title')\n",
    "            answer = item.find('div', class_='accordion-item__content')\n",
    "            if question and answer:\n",
    "                question_text = question.get_text(strip=True)\n",
    "                answer_text = get_text_from_element(answer)\n",
    "                question_answer_pairs.append({'question': question_text, 'answer': answer_text})\n",
    "\n",
    "    # Define excluded classes\n",
    "    excluded_classes = [\n",
    "        \"chf-navigation-bar\",\n",
    "        \"direct-access-container\",\n",
    "        \"direct-access-content\",\n",
    "        \"collection-wrapper collection collection-standard\",\n",
    "        \"collection-wrapper collection collection-standard l-outer l-outer--solutionPage\"\n",
    "    ]\n",
    "    \n",
    "    # Function to remove excluded elements\n",
    "    def remove_excluded_elements(soup):\n",
    "        for class_name in excluded_classes:\n",
    "            for element in soup.find_all(class_=class_name):\n",
    "                if element:\n",
    "                    element.decompose()\n",
    "    \n",
    "    # Function to remove nested excluded elements\n",
    "    def remove_nested_excluded_elements(soup):\n",
    "        for element in soup.find_all(True):\n",
    "            if isinstance(element, BeautifulSoup):\n",
    "                classes = element.get('class', [])\n",
    "                if classes and any(cls in ' '.join(classes) for cls in excluded_classes):\n",
    "                    element.decompose()\n",
    "    \n",
    "    remove_excluded_elements(soup)\n",
    "    remove_nested_excluded_elements(soup)\n",
    "\n",
    "    # Extract questions and answers from headings and next divs\n",
    "    questions = soup.find_all(['h1', 'h2', 'h3'])\n",
    "    for question in questions:\n",
    "        question_text = question.get_text(strip=True)\n",
    "        if question_text.endswith('?'):\n",
    "            answer_text = ''\n",
    "            next_div = question.find_next('div', class_='outerRichtextDiv')\n",
    "            if next_div:\n",
    "                answer_text = get_text_from_element(next_div)\n",
    "            if not answer_text:\n",
    "                next_div = question.find_next('div')\n",
    "                if next_div and not any(cls in ' '.join(next_div.get('class', [])) for cls in excluded_classes):\n",
    "                    answer_text = get_text_from_element(next_div)\n",
    "            if answer_text:\n",
    "                question_answer_pairs.append({'question': question_text, 'answer': answer_text})\n",
    "\n",
    "    # Extract questions and answers from accordion lists\n",
    "    accordion_list = soup.find('ul', class_='accordion-list')\n",
    "    if accordion_list:\n",
    "        extract_accordion_items(accordion_list)\n",
    "\n",
    "    return question_answer_pairs\n",
    "\n",
    "# 3. Process each URL\n",
    "output_dir = Path(\"data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)  # Create directory\n",
    "\n",
    "for idx, url in enumerate(urls, 1):\n",
    "    print(f\"Processing URL {idx}/{len(urls)}: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, allow_redirects=False)\n",
    "        \n",
    "        if response.status_code == 301 or response.status_code == 302:\n",
    "            print(\"   Redirect detected, checking URL.\")\n",
    "            final_url = response.headers.get('Location')\n",
    "            if final_url:\n",
    "                response = requests.get(final_url)\n",
    "        elif response.status_code == 200:\n",
    "            response = requests.get(url)\n",
    "        else:\n",
    "            print(\"   Invalid URL or access problem.\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract question-answer pairs\n",
    "        qa_pairs = extract_question_answer(soup)\n",
    "        print(f\"   {len(qa_pairs)} question-answer pairs found.\")\n",
    "        \n",
    "        if qa_pairs:  # Only create file if there are question-answer pairs\n",
    "            file_name = re.sub(r'\\W+', '_', url) + \".txt\"\n",
    "            output_file = output_dir / file_name\n",
    "            \n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(f\"Source URL: {url}\\n\\n\")\n",
    "                for idx, qa in enumerate(qa_pairs, 1):\n",
    "                    file.write(f\"{idx}. Question: {qa['question']}\\n   Answer: {qa['answer']}\\n\\n\")\n",
    "            \n",
    "            print(f\"   Results saved to '{output_file}'.\")\n",
    "        else:\n",
    "            print(\"   No question-answer pairs found, file will not be created.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"   Error occurred: {e}\")\n",
    "\n",
    "print(\"Processing completed! Results for pages with question-answer pairs saved in 'data' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAST VERSION! Gpt biraz daha iyilestirdi, cok daha hizli calisiyor!\n",
    "### Navigation yok!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import logging\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to create a session with retry logic\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.1, status_forcelist=[502, 503, 504])\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "# 1. Fetch URLs from Sitemap\n",
    "sitemap_url = 'https://www.telekom.de/ueber-das-unternehmen/robots/sitemap'\n",
    "logger.info(f\"Requesting sitemap URL: {sitemap_url}\")\n",
    "session = create_session()\n",
    "\n",
    "try:\n",
    "    response = session.get(sitemap_url)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.RequestException as e:\n",
    "    logger.error(f\"Error fetching sitemap: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Parse XML sitemap\n",
    "soup = BeautifulSoup(response.content, 'xml')\n",
    "logger.info(\"Processing sitemap XML...\")\n",
    "\n",
    "# Extract URLs\n",
    "urls = [url.text for url in soup.find_all('loc') if url is not None]\n",
    "logger.info(f\"{len(urls)} URLs found.\")\n",
    "\n",
    "# Function to extract question-answer pairs from the page\n",
    "def extract_question_answer(soup):\n",
    "    question_answer_pairs = []\n",
    "\n",
    "    def get_text_from_element(element):\n",
    "        text = ''\n",
    "        for p in element.find_all('p'):\n",
    "            text += p.get_text(strip=True) + '\\n'\n",
    "        for ul in element.find_all('ul'):\n",
    "            for li in ul.find_all('li'):\n",
    "                text += f\"• {li.get_text(strip=True)}\\n\"\n",
    "        return text.strip()\n",
    "\n",
    "    def extract_accordion_items(accordion_list):\n",
    "        for item in accordion_list.find_all('li', class_='accordion-item'):\n",
    "            question = item.find('p', class_='accordion-item__title')\n",
    "            answer = item.find('div', class_='accordion-item__content')\n",
    "            if question and answer:\n",
    "                question_text = question.get_text(strip=True)\n",
    "                answer_text = get_text_from_element(answer)\n",
    "                question_answer_pairs.append({'question': question_text, 'answer': answer_text})\n",
    "\n",
    "    excluded_classes = [\n",
    "        \"chf-navigation-bar\",\n",
    "        \"direct-access-container\",\n",
    "        \"direct-access-content\",\n",
    "        \"collection-wrapper collection collection-standard\",\n",
    "        \"collection-wrapper collection collection-standard l-outer l-outer--solutionPage\"\n",
    "    ]\n",
    "    \n",
    "    def remove_excluded_elements(soup):\n",
    "        for class_name in excluded_classes:\n",
    "            for element in soup.find_all(class_=class_name):\n",
    "                if element:\n",
    "                    element.decompose()\n",
    "\n",
    "    def remove_nested_excluded_elements(soup):\n",
    "        for element in soup.find_all(True):\n",
    "            if isinstance(element, BeautifulSoup):\n",
    "                classes = element.get('class', [])\n",
    "                if classes and any(cls in ' '.join(classes) for cls in excluded_classes):\n",
    "                    element.decompose()\n",
    "    \n",
    "    remove_excluded_elements(soup)\n",
    "    remove_nested_excluded_elements(soup)\n",
    "\n",
    "    questions = soup.find_all(['h1', 'h2', 'h3'])\n",
    "    for question in questions:\n",
    "        question_text = question.get_text(strip=True)\n",
    "        if question_text.endswith('?'):\n",
    "            answer_text = ''\n",
    "            next_div = question.find_next('div', class_='outerRichtextDiv')\n",
    "            if next_div:\n",
    "                answer_text = get_text_from_element(next_div)\n",
    "            if not answer_text:\n",
    "                next_div = question.find_next('div')\n",
    "                if next_div and not any(cls in ' '.join(next_div.get('class', [])) for cls in excluded_classes):\n",
    "                    answer_text = get_text_from_element(next_div)\n",
    "            if answer_text:\n",
    "                question_answer_pairs.append({'question': question_text, 'answer': answer_text})\n",
    "\n",
    "    accordion_list = soup.find('ul', class_='accordion-list')\n",
    "    if accordion_list:\n",
    "        extract_accordion_items(accordion_list)\n",
    "\n",
    "    return question_answer_pairs\n",
    "\n",
    "# 3. Process each URL\n",
    "output_dir = Path(\"data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for idx, url in enumerate(urls, 1):\n",
    "    logger.info(f\"Processing URL {idx}/{len(urls)}: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, allow_redirects=False)\n",
    "        \n",
    "        if response.status_code in {301, 302}:\n",
    "            logger.info(\"   Redirect detected, checking URL.\")\n",
    "            final_url = response.headers.get('Location')\n",
    "            if final_url:\n",
    "                response = session.get(final_url)\n",
    "        elif response.status_code != 200:\n",
    "            logger.warning(\"   Invalid URL or access problem.\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        qa_pairs = extract_question_answer(soup)\n",
    "        logger.info(f\"   {len(qa_pairs)} question-answer pairs found.\")\n",
    "        \n",
    "        if qa_pairs:\n",
    "            file_name = re.sub(r'\\W+', '_', url) + \".txt\"\n",
    "            output_file = output_dir / file_name\n",
    "            \n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(f\"Source URL: {url}\\n\\n\")\n",
    "                for idx, qa in enumerate(qa_pairs, 1):\n",
    "                    file.write(f\"{idx}. Question: {qa['question']}\\n   Answer: {qa['answer']}\\n\\n\")\n",
    "            \n",
    "            logger.info(f\"   Results saved to '{output_file}'.\")\n",
    "        else:\n",
    "            logger.info(\"   No question-answer pairs found, file will not be created.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"   Error occurred: {e}\")\n",
    "\n",
    "logger.info(\"Processing completed! Results for pages with question-answer pairs saved in 'data' directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
