{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for Question-Answer Extraction\n",
    "\n",
    "This notebook demonstrates how to scrape web pages for question-answer pairs, excluding specific sections and classes using Python. It utilizes `requests` and `BeautifulSoup` libraries for HTTP requests and HTML parsing.\n",
    "\n",
    "The code performs the following steps:\n",
    "\n",
    "1. **Fetch URLs from Sitemap**:\n",
    "   - Requests the sitemap URL to get a list of URLs to scrape.\n",
    "\n",
    "2. **Extract Question-Answer Pairs**:\n",
    "   - Defines a function to extract question-answer pairs from the HTML content.\n",
    "   - Removes specific sections and elements that should be excluded based on their class names.\n",
    "   - Collects questions and their corresponding answers from the page.\n",
    "\n",
    "3. **Process Each URL**:\n",
    "   - Iterates through the list of URLs.\n",
    "   - Requests each URL and processes the HTML content.\n",
    "   - Saves the extracted question-answer pairs to text files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soru-cevap cekme\n",
    "### Soru cevap iceren her bir web sayfasindan bunlari ve url'leri cekip ayri ayri txt dosyasina yaziyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import logging\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import time\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to create a session with retry logic\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.1, status_forcelist=[502, 503, 504])\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'})\n",
    "    return session\n",
    "\n",
    "# 1. Fetch URLs from Sitemap\n",
    "sitemap_url = 'https://www.telekom.de/ueber-das-unternehmen/robots/sitemap'\n",
    "logger.info(f\"Requesting sitemap URL: {sitemap_url}\")\n",
    "session = create_session()\n",
    "\n",
    "try:\n",
    "    response = session.get(sitemap_url)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.RequestException as e:\n",
    "    logger.error(f\"Error fetching sitemap: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Parse XML sitemap\n",
    "soup = BeautifulSoup(response.content, 'xml')\n",
    "logger.info(\"Processing sitemap XML...\")\n",
    "\n",
    "# Extract URLs\n",
    "urls = [url.text for url in soup.find_all('loc') if url is not None]\n",
    "logger.info(f\"{len(urls)} URLs found.\")\n",
    "\n",
    "# Function to extract question-answer pairs from the page\n",
    "def extract_question_answer(soup):\n",
    "    question_answer_pairs = []\n",
    "\n",
    "    def get_text_from_element(element):\n",
    "        text = ''\n",
    "        for p in element.find_all('p'):\n",
    "            text += p.get_text(strip=True) + '\\n'\n",
    "        for ul in element.find_all('ul'):\n",
    "            for li in ul.find_all('li'):\n",
    "                text += f\"• {li.get_text(strip=True)}\\n\"\n",
    "        return text.strip()\n",
    "\n",
    "    def extract_accordion_items(accordion_list):\n",
    "        for item in accordion_list.find_all('li', class_='accordion-item'):\n",
    "            question = item.find('p', class_='accordion-item__title')\n",
    "            answer = item.find('div', class_='accordion-item__content')\n",
    "            if question and answer:\n",
    "                question_text = question.get_text(strip=True)\n",
    "                answer_text = get_text_from_element(answer)\n",
    "                question_answer_pairs.append({'question': question_text, 'answer': answer_text})\n",
    "\n",
    "    excluded_classes = [\n",
    "        \"chf-navigation-bar\",\n",
    "        \"direct-access-container\",\n",
    "        \"direct-access-content\",\n",
    "        \"collection-wrapper collection collection-standard\",\n",
    "        \"collection-wrapper collection collection-standard l-outer l-outer--solutionPage\"\n",
    "    ]\n",
    "    \n",
    "    def remove_excluded_elements(soup):\n",
    "        for class_name in excluded_classes:\n",
    "            for element in soup.find_all(class_=class_name):\n",
    "                if element:\n",
    "                    element.decompose()\n",
    "\n",
    "    def remove_nested_excluded_elements(soup):\n",
    "        for element in soup.find_all(True):\n",
    "            if isinstance(element, BeautifulSoup):\n",
    "                classes = element.get('class', [])\n",
    "                if classes and any(cls in ' '.join(classes) for cls in excluded_classes):\n",
    "                    element.decompose()\n",
    "    \n",
    "    remove_excluded_elements(soup)\n",
    "    remove_nested_excluded_elements(soup)\n",
    "\n",
    "    questions = soup.find_all(['h1', 'h2', 'h3'])\n",
    "    for question in questions:\n",
    "        question_text = question.get_text(strip=True)\n",
    "        if question_text.endswith('?'):\n",
    "            answer_text = ''\n",
    "            next_div = question.find_next('div', class_='outerRichtextDiv')\n",
    "            if next_div:\n",
    "                answer_text = get_text_from_element(next_div)\n",
    "            if not answer_text:\n",
    "                next_div = question.find_next('div')\n",
    "                if next_div and not any(cls in ' '.join(next_div.get('class', [])) for cls in excluded_classes):\n",
    "                    answer_text = get_text_from_element(next_div)\n",
    "            if answer_text:\n",
    "                question_answer_pairs.append({'question': question_text, 'answer': answer_text})\n",
    "\n",
    "    accordion_list = soup.find('ul', class_='accordion-list')\n",
    "    if accordion_list:\n",
    "        extract_accordion_items(accordion_list)\n",
    "\n",
    "    return question_answer_pairs\n",
    "\n",
    "# 3. Process each URL\n",
    "output_dir = Path(\"data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for idx, url in enumerate(urls, 1):\n",
    "    logger.info(f\"Processing URL {idx}/{len(urls)}: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, allow_redirects=False)\n",
    "        \n",
    "        if response.status_code in {301, 302}:\n",
    "            logger.info(\"   Redirect detected, checking URL.\")\n",
    "            final_url = response.headers.get('Location')\n",
    "            if final_url:\n",
    "                response = session.get(final_url)\n",
    "        elif response.status_code != 200:\n",
    "            logger.warning(\"   Invalid URL or access problem.\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        qa_pairs = extract_question_answer(soup)\n",
    "        logger.info(f\"   {len(qa_pairs)} question-answer pairs found.\")\n",
    "        \n",
    "        if qa_pairs:\n",
    "            file_name = re.sub(r'\\W+', '_', url) + \".txt\"\n",
    "            output_file = output_dir / file_name\n",
    "            \n",
    "            try:\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(f\"Source URL: {url}\\n\\n\")\n",
    "                    for qa in qa_pairs:\n",
    "                        file.write(f\"Question: {qa['question']}\\nAnswer: {qa['answer']}\\n\\n\")\n",
    "                \n",
    "                logger.info(f\"   Results saved to '{output_file}'.\")\n",
    "            except IOError as e:\n",
    "                logger.error(f\"   Error writing to file: {e}\")\n",
    "        else:\n",
    "            logger.info(\"   No question-answer pairs found, file will not be created.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"   Error occurred: {e}\")\n",
    "    \n",
    "    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "logger.info(\"Processing completed! Results for pages with question-answer pairs saved in 'data' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation.txt olusturma\n",
    "### output_folder'dan tek tek txt dosyalarindan\n",
    "### navigation bilgisini cekip navigation.txt dosyasina liste olarak ekliyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='processing.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define fixed paths for the input directory and output file\n",
    "directory_path = \"web_data/scraped_data\"\n",
    "web_data_directory = \"web_data\"\n",
    "navigation_file = os.path.join(web_data_directory, \"navigation.txt\")\n",
    "\n",
    "def format_navigation_text(text):\n",
    "    \"\"\"\n",
    "    Formats the navigation text by replacing newlines and multiple spaces with ' > '.\n",
    "    Special cases:\n",
    "    - Replaces ' > & >' with '&'\n",
    "    - Replaces ' > und >' with ' und '\n",
    "    - Replaces ' > bei >' with ' bei '\n",
    "    - Ensures no extra '>' signs after the third '>'\n",
    "    - Ensures no extra spaces are present\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The text to be formatted.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The formatted text with ' > ' as separators, with special case adjustments.\n",
    "    \"\"\"\n",
    "    # Replace newlines and multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Replace single spaces with ' > ' for navigation format\n",
    "    formatted_text = re.sub(r'\\s* \\s*', ' > ', text)\n",
    "    \n",
    "    # Handle special cases\n",
    "    formatted_text = re.sub(r'>\\s*&\\s*>', ' & ', formatted_text)\n",
    "    formatted_text = re.sub(r'>\\s*und\\s*>', ' und ', formatted_text)\n",
    "    formatted_text = re.sub(r'>\\s*bei\\s*>', ' bei ', formatted_text)\n",
    "    \n",
    "    # Ensure no extra '>' signs after the third '>'\n",
    "    formatted_text = re.sub(r'(\\s*>\\s*){3,}', ' > ', formatted_text)\n",
    "    \n",
    "    # Cleanup spaces around '>'\n",
    "    formatted_text = re.sub(r'\\s*>\\s*', ' > ', formatted_text).strip()\n",
    "    \n",
    "    # Ensure no double spaces in the final text\n",
    "    formatted_text = re.sub(r' {2,}', ' ', formatted_text)\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "def extract_navigation_data(directory_path, navigation_file):\n",
    "    \"\"\"\n",
    "    Extracts navigation data from .txt files in the specified directory and writes it to an output file.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path (str): Path to the directory containing .txt files.\n",
    "    - navigation_file (str): Path to the output file for navigation data.\n",
    "    \n",
    "    Outputs:\n",
    "    - Writes navigation data to navigation_file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Regex pattern to find the section starting with \"...Telekom\" and ending with two spaces\n",
    "    pattern = r\"\\.\\.\\.Telekom.*?\\s{2}\"\n",
    "    \n",
    "    # Initialize lists to keep track of processed and unprocessed files\n",
    "    processed_files_data = []\n",
    "    unprocessed_files = []\n",
    "\n",
    "    def process_file(filename):\n",
    "        \"\"\"\n",
    "        Processes each .txt file to extract navigation content based on patterns.\n",
    "        \n",
    "        Parameters:\n",
    "        - filename (str): The name of the file to be processed.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as source:\n",
    "                content = source.read()\n",
    "                match = re.search(pattern, content, re.DOTALL)\n",
    "                if match:\n",
    "                    matched_text = match.group(0)\n",
    "                    start_index = match.end()\n",
    "\n",
    "                    # Clean and format the matched text\n",
    "                    cleaned_text = re.sub(r'\\.\\.\\.+', '\\n', matched_text.strip(\".\").strip())\n",
    "                    \n",
    "                    # Extract the text after the matched pattern\n",
    "                    post_pattern_text = content[start_index:]\n",
    "                    \n",
    "                    # Find paragraphs separated by multiple newlines\n",
    "                    paragraph_pattern = r'([^\\n]+(?:\\n[^\\n]+)*)(?:\\n{2,})'\n",
    "                    paragraphs = re.findall(paragraph_pattern, post_pattern_text)\n",
    "                    \n",
    "                    if len(paragraphs) >= 1:\n",
    "                        # Format navigation text\n",
    "                        navigation = format_navigation_text(cleaned_text)\n",
    "                        processed_files_data.append((filename, navigation))\n",
    "                    else:\n",
    "                        unprocessed_files.append(filename)\n",
    "                else:\n",
    "                    unprocessed_files.append(filename)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {filename}: {e}\")\n",
    "            unprocessed_files.append(filename)\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        logging.error(f\"Directory {directory_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Create web_data directory if it does not exist\n",
    "    os.makedirs(web_data_directory, exist_ok=True)\n",
    "\n",
    "    # Process each .txt file in the directory\n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            process_file(filename)\n",
    "\n",
    "    # Write navigation data to the navigation_file\n",
    "    with open(navigation_file, \"w\", encoding=\"utf-8\") as nav_file:\n",
    "        for filename, navigation in sorted(processed_files_data, key=lambda x: x[0]):\n",
    "            # Prepend \"https_\" to the filename\n",
    "            nav_file.write(f\"https_{filename}\\n\")\n",
    "            nav_file.write(f\"{navigation}\\n\")\n",
    "            nav_file.write(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "    # Print summary of the processing\n",
    "    total_files = len(processed_files_data) + len(unprocessed_files)\n",
    "    num_processed_files = len(processed_files_data)\n",
    "    num_unprocessed_files = len(unprocessed_files)\n",
    "    missing_files = total_files - (num_processed_files + num_unprocessed_files)\n",
    "\n",
    "    print(\"Process completed.\")\n",
    "    print(f\"Total number of .txt files in the folder: {total_files}\")\n",
    "    print(f\"Number of .txt files processed and written to {navigation_file}: {num_processed_files}\")\n",
    "    print(f\"Unprocessed files: {num_unprocessed_files}\")\n",
    "    print(f\"Number of missing or unaccounted files: {missing_files}\")\n",
    "\n",
    "    # Log the results\n",
    "    logging.info(f\"Total files: {total_files}\")\n",
    "    logging.info(f\"Processed files: {num_processed_files}\")\n",
    "    logging.info(f\"Unprocessed files: {num_unprocessed_files}\")\n",
    "    logging.info(f\"Missing files: {missing_files}\")\n",
    "\n",
    "# Call the function with the specified parameters\n",
    "extract_navigation_data(directory_path, navigation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation'lari ekleme\n",
    "### Navigation bilgisi olmayan ham data'da her bir txt dosyasinin ikinci satirina\n",
    "### path ekliyor. Eger path zaten varsa ekleme yapmiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define paths for the directories and files\n",
    "data_directory = \"data\"  # Directory containing the .txt files to be updated\n",
    "navigation_file = \"web_data/navigation.txt\"  # File containing navigation data\n",
    "\n",
    "def load_navigation_data(navigation_file):\n",
    "    \"\"\"\n",
    "    Loads navigation data from the navigation_file into a dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - navigation_file (str): Path to the file containing filenames and navigation data.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are filenames and values are navigation data.\n",
    "    \"\"\"\n",
    "    navigation_data = {}\n",
    "    \n",
    "    try:\n",
    "        # Open the navigation file and read lines\n",
    "        with open(navigation_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            \n",
    "            # Process lines in pairs: filename and navigation data\n",
    "            for i in range(0, len(lines), 2):\n",
    "                filename = lines[i].strip()  # Filename is the first line of each pair\n",
    "                navigation = lines[i + 1].strip() if i + 1 < len(lines) else \"\"  # Navigation data is the second line\n",
    "                navigation_data[filename] = navigation\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading navigation file: {e}\")\n",
    "    \n",
    "    return navigation_data\n",
    "\n",
    "def append_navigation_to_files(data_directory, navigation_data):\n",
    "    \"\"\"\n",
    "    Appends navigation data to .txt files in the data_directory based on the provided navigation_data mapping.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_directory (str): Path to the directory containing .txt files to be updated.\n",
    "    - navigation_data (dict): Dictionary where keys are filenames and values are navigation data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Iterate through each file in the directory\n",
    "        for filename in os.listdir(data_directory):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(data_directory, filename)\n",
    "                \n",
    "                # Adjust filename key to match with navigation data (handle 'https_' prefix)\n",
    "                key_filename_with_https = filename if filename.startswith(\"https_\") else \"https_\" + filename\n",
    "                key_filename_without_https = filename[6:] if filename.startswith(\"https_\") else filename\n",
    "                \n",
    "                # Determine the correct key for navigation data\n",
    "                if key_filename_with_https in navigation_data:\n",
    "                    navigation_key = key_filename_with_https\n",
    "                elif key_filename_without_https in navigation_data:\n",
    "                    navigation_key = key_filename_without_https\n",
    "                else:\n",
    "                    navigation_key = None\n",
    "                \n",
    "                if navigation_key:\n",
    "                    print(f\"Appending navigation to {file_path}...\")  # Debugging output\n",
    "                    try:\n",
    "                        # Open the file in read and write mode\n",
    "                        with open(file_path, \"r+\", encoding=\"utf-8\") as file:\n",
    "                            content = file.readlines()  # Read all lines of the file\n",
    "                            \n",
    "                            # Check if navigation data is already present\n",
    "                            if len(content) > 1 and content[1].startswith(\"Telekom\"):\n",
    "                                print(f\"Navigation data already present in {file_path}\")\n",
    "                                continue  # Skip appending if navigation data is already there\n",
    "\n",
    "                            # Insert navigation data after the first line\n",
    "                            if \"Source URL:\" in content[0]:\n",
    "                                content.insert(1, navigation_data[navigation_key] + \"\\n\")\n",
    "                            else:\n",
    "                                content.insert(0, navigation_data[navigation_key] + \"\\n\")\n",
    "\n",
    "                            # Write updated content back to the file\n",
    "                            file.seek(0)  # Move to the beginning of the file\n",
    "                            file.writelines(content)\n",
    "                            print(f\"Successfully appended navigation to {file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error appending to file {file_path}: {e}\")\n",
    "                else:\n",
    "                    print(f\"No navigation data found for {filename}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing directory or files: {e}\")\n",
    "\n",
    "# Load navigation data from the navigation file\n",
    "navigation_data = load_navigation_data(navigation_file)\n",
    "\n",
    "# Append navigation data to files in the data directory\n",
    "append_navigation_to_files(data_directory, navigation_data)\n",
    "\n",
    "print(\"Script completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kategorilere ayirma\n",
    "### önemli!!: bu güncel olmali. data_directory = \"/Users/taha/Desktop/rag/rag_data/website/data\", output_directory = \"/Users/taha/Desktop/rag/data\"\n",
    "### kategorisine göre txt dosyalarini grupluyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rag/rag_data/website/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Run the function with the specified parameters\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[43morganize_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m, in \u001b[0;36morganize_files\u001b[0;34m(directory_path, output_directory, valid_folders)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mOrganizes .txt files into directories based on the navigation text up to and including the third '>' character.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m- valid_folders (list of str): List of valid folder names.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m setup_directories(output_directory, valid_folders)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     70\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_path, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rag/rag_data/website/data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "#data_directory = \"data\"\n",
    "#output_directory = \"organized_data\"\n",
    "\n",
    "# Define paths\n",
    "data_directory = \"/Users/taha/Desktop/rag/rag_data/website/data\"\n",
    "output_directory = \"/Users/taha/Desktop/rag/data\"\n",
    "\n",
    "# List of valid folder names\n",
    "folder_names = [\n",
    "    \"Geräte & Zubehör\",\n",
    "    \"Hilfe bei Störungen\",\n",
    "    \"Internet & Telefonie\",\n",
    "    \"MagentaEINS\",\n",
    "    \"Mobilfunk\",\n",
    "    \"TV\",\n",
    "    \"Vertrag & Rechnung\",\n",
    "    \"Apps & Dienste\"\n",
    "]\n",
    "\n",
    "def setup_directories(base_directory, folders):\n",
    "    \"\"\"\n",
    "    Create the base directory and subdirectories based on the folder names provided.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_directory (str): The base directory where folders will be created.\n",
    "    - folders (list of str): List of folder names to create.\n",
    "    \"\"\"\n",
    "    os.makedirs(base_directory, exist_ok=True)\n",
    "    for folder in folders:\n",
    "        os.makedirs(os.path.join(base_directory, folder), exist_ok=True)\n",
    "    # Create the 'Others' directory\n",
    "    os.makedirs(os.path.join(base_directory, \"Others\"), exist_ok=True)\n",
    "\n",
    "def extract_navigation_part(navigation_text):\n",
    "    \"\"\"\n",
    "    Extracts the navigation part of the text up to and including the third '>' character.\n",
    "    \n",
    "    Parameters:\n",
    "    - navigation_text (str): The navigation text to extract part from.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The extracted navigation part.\n",
    "    \"\"\"\n",
    "    parts = navigation_text.split('>')\n",
    "    \n",
    "    # Ensure there are at least three '>' characters\n",
    "    if len(parts) >= 3:\n",
    "        # Combine parts up to and including the third '>'\n",
    "        return ' > '.join(part.strip() for part in parts[:3]) + ' >'\n",
    "    else:\n",
    "        return navigation_text.strip()\n",
    "\n",
    "def organize_files(directory_path, output_directory, valid_folders):\n",
    "    \"\"\"\n",
    "    Organizes .txt files into directories based on the navigation text up to and including the third '>' character.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path (str): Path to the directory containing .txt files.\n",
    "    - output_directory (str): Path to the directory where organized files will be saved.\n",
    "    - valid_folders (list of str): List of valid folder names.\n",
    "    \"\"\"\n",
    "    setup_directories(output_directory, valid_folders)\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    lines = file.readlines()\n",
    "                    \n",
    "                    # Check if there are at least two lines in the file\n",
    "                    if len(lines) >= 2:\n",
    "                        # Read the second line as the navigation text\n",
    "                        navigation_text = lines[1].strip()\n",
    "                        \n",
    "                        # Extract the part of the navigation text up to and including the third '>'\n",
    "                        navigation_part = extract_navigation_part(navigation_text)\n",
    "                        \n",
    "                        # Determine the appropriate folder\n",
    "                        target_folder = \"Others\"\n",
    "                        for folder in valid_folders:\n",
    "                            if folder in navigation_part:\n",
    "                                target_folder = folder\n",
    "                                break\n",
    "                        \n",
    "                        # Copy the file to the appropriate folder\n",
    "                        target_directory = os.path.join(output_directory, target_folder)\n",
    "                        shutil.copy(file_path, os.path.join(target_directory, filename))\n",
    "                        print(f\"Copied {filename} to {target_directory}\")\n",
    "                    else:\n",
    "                        print(f\"File {filename} does not have enough lines to extract navigation data.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "# Run the function with the specified parameters\n",
    "organize_files(data_directory, output_directory, folder_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
