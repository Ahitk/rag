{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler for PDF Downloading from Telekom Hilfe\n",
    "\n",
    " is designed to crawl the Telekom Hilfe website, identify all PDF links, and download these files to a specified directory. The script utilizes asynchronous programming with `aiohttp` and `BeautifulSoup` to efficiently process pages and handle large volumes of data. Logs are generated for tracking the crawling and downloading processes, ensuring traceability and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script performs a web crawl starting from a specified URL (Telekom Hilfe website) to find and download PDF files. \n",
    "It utilizes asynchronous programming for efficient web scraping and downloading, relying on the aiohttp and BeautifulSoup libraries. \n",
    "Downloaded PDFs are saved in a designated directory, with a logging setup to monitor the process and handle errors gracefully.\n",
    "\"\"\"\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import logging\n",
    "\n",
    "# Directory to save downloaded PDF files\n",
    "DOWNLOAD_DIR = 'data/pdf_files'\n",
    "# Ensure the download directory exists\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# Starting URL for the web crawl\n",
    "START_URL = 'https://www.telekom.de/hilfe'\n",
    "\n",
    "# Set to keep track of already downloaded PDF filenames to avoid duplicates\n",
    "downloaded_files = set()\n",
    "\n",
    "# Configure logger settings for information and error tracking\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "async def find_pdfs(url, session):\n",
    "    \"\"\"\n",
    "    Fetches the HTML content of a page and identifies all links to PDF files.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the page to scrape.\n",
    "        session (aiohttp.ClientSession): The aiohttp session used for making HTTP requests.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of URLs that link to PDF files found on the page.\n",
    "    \"\"\"\n",
    "    pdf_links = []\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            # Process only if the response content is HTML\n",
    "            if 'text/html' in response.headers.get('Content-Type', '').lower():\n",
    "                soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "                # Extract links ending in .pdf\n",
    "                pdf_links = [\n",
    "                    urljoin(url, a['href'])\n",
    "                    for a in soup.find_all('a', href=True)\n",
    "                    if a['href'].lower().endswith('.pdf')\n",
    "                ]\n",
    "            else:\n",
    "                logger.error(f'{url} is not an HTML page.')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to fetch {url}. Error: {str(e)}')\n",
    "    \n",
    "    return pdf_links\n",
    "\n",
    "async def download_pdf(url, session):\n",
    "    \"\"\"\n",
    "    Downloads a PDF file from the specified URL and saves it to the download directory.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the PDF file to download.\n",
    "        session (aiohttp.ClientSession): The aiohttp session used for making HTTP requests.\n",
    "    \"\"\"\n",
    "    filename = url.split('/')[-1]\n",
    "    # Skip download if the file has already been processed\n",
    "    if filename in downloaded_files:\n",
    "        logger.info(f'Already downloaded: {filename}')\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            # Proceed if the HTTP response status is 200 (OK)\n",
    "            if response.status == 200:\n",
    "                file_path = os.path.join(DOWNLOAD_DIR, filename)\n",
    "                # Write the PDF file to disk\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(await response.read())\n",
    "                downloaded_files.add(filename)\n",
    "                logger.info(f'Downloaded: {file_path}')\n",
    "            else:\n",
    "                logger.error(f'Failed to download {url}. Status code: {response.status}')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to download {url}. Error: {str(e)}')\n",
    "\n",
    "async def crawl_site(start_url):\n",
    "    \"\"\"\n",
    "    Initiates a web crawl from the starting URL, finds PDF links on each page, and downloads them.\n",
    "\n",
    "    Args:\n",
    "        start_url (str): The root URL to begin crawling.\n",
    "    \"\"\"\n",
    "    urls_to_visit = {start_url}\n",
    "    visited_urls = set()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        while urls_to_visit:\n",
    "            url = urls_to_visit.pop()\n",
    "            # Skip if URL has already been visited\n",
    "            if url in visited_urls:\n",
    "                continue\n",
    "            visited_urls.add(url)\n",
    "            logger.info(f'Crawling: {url}')\n",
    "\n",
    "            # Find and download PDFs from the current page\n",
    "            pdf_links = await find_pdfs(url, session)\n",
    "            for link in pdf_links:\n",
    "                await download_pdf(link, session)\n",
    "\n",
    "            try:\n",
    "                async with session.get(url) as response:\n",
    "                    # Proceed if the response status is 200 (OK)\n",
    "                    if response.status == 200:\n",
    "                        soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "                        # Add new URLs within the same domain to visit\n",
    "                        urls_to_visit.update(\n",
    "                            urljoin(url, a['href'])\n",
    "                            for a in soup.find_all('a', href=True)\n",
    "                            if a['href'].startswith('/') and urljoin(url, a['href']) not in visited_urls\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                logger.error(f'Failed to crawl {url}. Error: {str(e)}')\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    The main function that initiates the web crawling and PDF downloading process.\n",
    "    \"\"\"\n",
    "    await crawl_site(START_URL)\n",
    "\n",
    "def run_async(coro):\n",
    "    \"\"\"\n",
    "    Executes an asynchronous coroutine using the event loop, adjusting for whether the loop is already running.\n",
    "\n",
    "    Args:\n",
    "        coro (coroutine): The coroutine function to execute.\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # If the event loop is active, schedule the coroutine\n",
    "        asyncio.ensure_future(coro)\n",
    "    else:\n",
    "        # If the event loop is inactive, run the coroutine until completion\n",
    "        loop.run_until_complete(coro)\n",
    "\n",
    "# Start the script\n",
    "if __name__ == '__main__':\n",
    "    run_async(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Filter\n",
    "\n",
    "filters PDF files by checking for specific keywords related to privacy and legal information on the first page. Files that pass the filter are copied into a separate directory. The script handles encryption, image-only pages, and unreadable PDFs, ensuring a reliable dataset for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script filters PDF files based on specific keywords found on the first page of each document. \n",
    "The primary purpose is to identify and copy PDFs that do not contain any of the specified keywords \n",
    "related to privacy and legal information, into a separate directory. \n",
    "\n",
    "Directory Structure:\n",
    "- `data/pdf_files`: The source directory containing the original PDFs to be processed.\n",
    "- `data/keyword_filter`: The destination directory for PDFs that pass the filtering criteria.\n",
    "\n",
    "Filtering Criteria:\n",
    "The script checks each PDF file to see if the first page contains any of the specified keywords \n",
    "(e.g., \"privacy\", \"legal notice\", etc.). If a keyword is found, the file is excluded. Additionally, \n",
    "PDFs that are encrypted, have image-only pages, or cannot be read are also excluded from copying.\n",
    "\n",
    "Requirements:\n",
    "- pypdf: For PDF processing and text extraction from pages.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pypdf import PdfReader\n",
    "from pathlib import Path\n",
    "\n",
    "# Define directories for source and filtered PDFs\n",
    "pdf_dir = \"data/pdf_files\"\n",
    "first_filter_dir = \"data/keyword_filter\"\n",
    "\n",
    "# Define keywords for filtering PDF files\n",
    "filter_keywords = [\n",
    "    \"Datenschutzhinweis\", \"Datenschutzhinweise\", \"Datenschutzrichtlinie\", \"Datenschutz\", \"Data Privacy\", \"Data privacy\", \n",
    "    \"Data privacy information\", \"Ergänzende Bedingungen\", \"End-User License\", \"Firmware-Änderungen\", \n",
    "    \"Firmwareänderungen\", \"Firmware\", \"Geschäftsbedingungen\", \"Konformitätserklärung\", \"LEGAL NOTICE\", \n",
    "    \"LIZENZTEXTE\", \"LICENSES\", \"LIZENZ\", \"LICENCE\", \"privacy\", \"Privacy\", \"RECHTLICHE HINWEISE\"\n",
    "]\n",
    "\n",
    "# Ensure the filtered directory exists\n",
    "Path(first_filter_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def check_first_page(pdf_path):\n",
    "    \"\"\"\n",
    "    Checks the first page of a PDF to determine if it meets the criteria for filtering.\n",
    "    The function performs the following:\n",
    "    - Decrypts the PDF if it is encrypted.\n",
    "    - Extracts text from the first page and checks if it is image-only.\n",
    "    - Searches for specific keywords on the first page.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the PDF passes the filter criteria (no keywords found), False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "\n",
    "        # Attempt to decrypt the PDF if encrypted\n",
    "        if reader.is_encrypted:\n",
    "            try:\n",
    "                reader.decrypt(\"\")  # Attempt decryption with an empty password\n",
    "                print(f\"Decrypted PDF: {pdf_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not decrypt {pdf_path}: {e}\")\n",
    "                return False\n",
    "\n",
    "        # Extract the first page\n",
    "        first_page = reader.pages[0]\n",
    "\n",
    "        # Check if the first page contains images but no extractable text\n",
    "        has_images = bool(first_page.images)\n",
    "        first_page_text = first_page.extract_text()\n",
    "\n",
    "        if has_images and not first_page_text:\n",
    "            print(f\"Image-only page detected in {pdf_path}\")\n",
    "            return False\n",
    "\n",
    "        if not first_page_text:\n",
    "            print(f\"Unreadable or image-only first page in {pdf_path}\")\n",
    "            return False\n",
    "\n",
    "        # Check for the presence of any filter keywords in the first page text\n",
    "        for keyword in filter_keywords:\n",
    "            if keyword in first_page_text:\n",
    "                return False  # Keyword found, exclude this file\n",
    "\n",
    "        return True  # No keywords found, file passes the filter\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False  # Return False if any exception occurs (unreadable or inaccessible file)\n",
    "\n",
    "# Process each PDF in the source directory\n",
    "for file_name in os.listdir(pdf_dir):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_dir, file_name)\n",
    "        \n",
    "        # Check if the PDF meets the filtering criteria\n",
    "        if check_first_page(file_path):\n",
    "            # Define the destination path in the filtered directory\n",
    "            destination_path = os.path.join(first_filter_dir, file_name)\n",
    "            \n",
    "            # Copy the file if it does not already exist in the filtered directory\n",
    "            if not os.path.exists(destination_path):\n",
    "                shutil.copy(file_path, destination_path)\n",
    "                print(f\"Copied: {file_name}\")\n",
    "            else:\n",
    "                print(f\"Skipped (already exists): {file_name}\")\n",
    "\n",
    "# Output the count of successfully filtered files\n",
    "filtered_files = os.listdir(first_filter_dir)\n",
    "print(f\"Number of files in the filtered directory: {len(filtered_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Keyword Filter\n",
    "\n",
    " filters PDF files from a specified directory based on the presence of predefined keywords. If a PDF file's name or the text on its first page includes any of the target keywords, the entire file is copied to a target directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kopyalandı (dosya adı ile): magentamobil-speedbox-young.pdf\n",
      "Kopyalandı (ilk sayfa ile): konfiguration-zyxel-speedlink-5501.pdf\n",
      "Kopyalandı (ilk sayfa ile): inbetriebnahme-router-mit-reset.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-speedbox-flex-young.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-xl.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-speedbox-flex.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-xl-flex.pdf\n",
      "Kopyalandı (ilk sayfa ile): inbetriebnahme-frtzbox-mit-reset.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-xl-young.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-special-m-flex.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-basic.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-data-s.pdf\n",
      "Kopyalandı (dosya adı ile): magentazuhause-xl.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-pluskarte-flex.pdf\n",
      "Kopyalandı (dosya adı ile): installationsanleitung-mein-magenta-app.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-special-m-eins.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-s-flex-young.pdf\n",
      "Kopyalandı (dosya adı ile): manuelle-konfiguration-magentazuhause-regio-zyxel-speedlink-5501.pdf\n",
      "Kopyalandı (dosya adı ile): magentatv-downloads-fernbedienung.pdf\n",
      "Kopyalandı (dosya adı ile): konfiguration-magentazuhause-regio-digitalsierungsbox-basic.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-s.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-26 12:41:46,677] [ WARNING] _utils.py:463 - incorrect startxref pointer(3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kopyalandı (dosya adı ile): magentamobil-prepaid-m.pdf\n",
      "Kopyalandı (ilk sayfa ile): esim-profil-apple-watch.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-26 12:41:47,140] [ WARNING] _utils.py:463 - incorrect startxref pointer(3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kopyalandı (dosya adı ile): magentamobil-prepaid-l.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-special-m-flex-young.pdf\n",
      "Kopyalandı (dosya adı ile): bedienungsanleitung-magenta-tv-stick.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-26 12:41:47,680] [ WARNING] _utils.py:463 - incorrect startxref pointer(3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kopyalandı (ilk sayfa ile): inbetriebnahme-router-ohne-reset.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-prepaid-max.pdf\n",
      "Kopyalandı (dosya adı ile): kurzbedienungsanleitung-magenta-tv-box.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-xl-flex-young.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-m-young.pdf\n",
      "Kopyalandı (dosya adı ile): konfiguration-magentazuhause-regio-zyxel-speedlink-6501.pdf\n",
      "Kopyalandı (ilk sayfa ile): checkliste-neuer-hausanschluss.pdf\n",
      "Kopyalandı (ilk sayfa ile): esim-aktivierung-apple-watch.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-s-young.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-26 12:41:52,178] [ WARNING] _utils.py:463 - incorrect startxref pointer(3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kopyalandı (dosya adı ile): magentamobil-m.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-prepaid-xl.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-special-m-young.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-26 12:41:52,414] [ WARNING] _utils.py:463 - incorrect startxref pointer(3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kopyalandı (dosya adı ile): MagentaTV_2.0.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-prepaid-s.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-l.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-s-flex.pdf\n",
      "Kopyalandı (ilk sayfa ile): inbetriebnahme-frtzbox-ohne-reset.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-xl-premium.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-basic-flex.pdf\n",
      "Kopyalandı (ilk sayfa ile): wechselmatrix.pdf\n",
      "Kopyalandı (ilk sayfa ile): lte-sofort.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-l-young.pdf\n",
      "Kopyalandı (ilk sayfa ile): kurzbedienungsanleitung-media-reiceiver-401.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-xs.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-m-flex-young.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-l-flex-young.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-26 12:41:58,417] [ WARNING] _utils.py:463 - incorrect startxref pointer(3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kopyalandı (ilk sayfa ile): unterstuetzte-kameras.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-prepaid-jahrestarif.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-data-l.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-data-m.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-m-flex.pdf\n",
      "Kopyalandı (dosya adı ile): magentamobil-l-flex.pdf\n",
      "Kopyalandı (dosya adı ile): manuelle-konfiguration-digitalisierungsbox-magentazuhause-regio.pdf\n",
      "Hedef klasördeki dosya sayısı: 59\n"
     ]
    }
   ],
   "source": [
    "# Description: This script searches for PDF files in a specified directory that contain specific keywords.\n",
    "# If a PDF file contains one of the target keywords either in its filename or on the first page,\n",
    "# the script copies the entire PDF to a target directory for further use. \n",
    "# This is especially useful for filtering large PDF datasets by content relevance.\n",
    "\n",
    "import os\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from pathlib import Path\n",
    "\n",
    "# Directories\n",
    "pdf_dir = \"data/keyword_filter\"      # Directory containing PDF files to be filtered\n",
    "target_dir = \"data/magenta_files\"     # Target directory to save matched PDF files\n",
    "\n",
    "# Keywords to search for in PDF filenames or content\n",
    "keywords = [\"magenta\"]\n",
    "\n",
    "# Ensure the target directory exists; create if it does not\n",
    "Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to check if any keyword is present in the given text\n",
    "def contains_keyword(text, keywords):\n",
    "    \"\"\"\n",
    "    Checks if the given text contains any of the specified keywords.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to search within.\n",
    "        keywords (list of str): List of keywords to check for in the text.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if any keyword is found in the text, False otherwise.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    return any(keyword.lower() in text for keyword in keywords)\n",
    "\n",
    "# Function to save all pages of a PDF to a new PDF file\n",
    "def save_all_pages(pdf_path, destination_path):\n",
    "    \"\"\"\n",
    "    Copies all pages from the source PDF file to a new PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the source PDF file.\n",
    "        destination_path (str): Path to save the new PDF file.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # Add all pages from the source PDF to the new PDF\n",
    "    for page in reader.pages:\n",
    "        writer.add_page(page)\n",
    "\n",
    "    # Write the new PDF to the destination path\n",
    "    with open(destination_path, \"wb\") as output_pdf:\n",
    "        writer.write(output_pdf)\n",
    "\n",
    "# Iterate through each file in the PDF directory to check for keywords\n",
    "for file_name in os.listdir(pdf_dir):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_dir, file_name)\n",
    "        \n",
    "        # Check if the file name contains any of the target keywords\n",
    "        if contains_keyword(file_name, keywords):\n",
    "            destination_path = os.path.join(target_dir, file_name)\n",
    "            # Copy the PDF file to the target directory\n",
    "            save_all_pages(file_path, destination_path)\n",
    "            print(f\"Copied (by filename match): {file_name}\")\n",
    "            continue  # Skip further checks for this file since it's already matched\n",
    "\n",
    "        # Check the content of the first page for keywords if filename does not match\n",
    "        try:\n",
    "            reader = PdfReader(file_path)\n",
    "\n",
    "            # Check if the PDF is encrypted and attempt to decrypt it\n",
    "            if reader.is_encrypted:\n",
    "                try:\n",
    "                    reader.decrypt(\"\")  # Attempt to decrypt with an empty password\n",
    "                except Exception as e:\n",
    "                    print(f\"Unable to decrypt: {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Extract and check text from the first page\n",
    "            first_page = reader.pages[0]\n",
    "            first_page_text = first_page.extract_text()\n",
    "\n",
    "            # Check if the first page contains any of the keywords\n",
    "            if first_page_text and contains_keyword(first_page_text, keywords):\n",
    "                destination_path = os.path.join(target_dir, file_name)\n",
    "                # Save the entire PDF if the first page contains a keyword\n",
    "                save_all_pages(file_path, destination_path)\n",
    "                print(f\"Copied (by first page content): {file_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {file_path}: {e}\")\n",
    "\n",
    "# Print the number of files in the target directory\n",
    "filtered_files = os.listdir(target_dir)\n",
    "print(f\"Number of files in target directory: {len(filtered_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Text Extraction and Summarization\n",
    "\n",
    " extracts and summarizes text from PDF files. It converts PDF pages to images, applies OCR for text extraction, and uses GPT-4 Vision to generate concise summaries. Outputs are saved as text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/26 22:33:55] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/taha/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/taha/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/Users/taha/Desktop/rag/venv/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/Users/taha/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "Skipping magentamobil-speedbox-young.pdf, output file already exists: data/text_outputs/magentamobil-speedbox-young.txt\n",
      "Skipping konfiguration-zyxel-speedlink-5501.pdf, output file already exists: data/text_outputs/konfiguration-zyxel-speedlink-5501.txt\n",
      "Skipping inbetriebnahme-router-mit-reset.pdf, output file already exists: data/text_outputs/inbetriebnahme-router-mit-reset.txt\n",
      "Skipping magentamobil-speedbox-flex-young.pdf, output file already exists: data/text_outputs/magentamobil-speedbox-flex-young.txt\n",
      "Skipping magentamobil-xl.pdf, output file already exists: data/text_outputs/magentamobil-xl.txt\n",
      "Skipping magentamobil-speedbox-flex.pdf, output file already exists: data/text_outputs/magentamobil-speedbox-flex.txt\n",
      "Skipping magentamobil-xl-flex.pdf, output file already exists: data/text_outputs/magentamobil-xl-flex.txt\n",
      "Skipping inbetriebnahme-frtzbox-mit-reset.pdf, output file already exists: data/text_outputs/inbetriebnahme-frtzbox-mit-reset.txt\n",
      "Skipping magentamobil-xl-young.pdf, output file already exists: data/text_outputs/magentamobil-xl-young.txt\n",
      "Skipping magentamobil-special-m-flex.pdf, output file already exists: data/text_outputs/magentamobil-special-m-flex.txt\n",
      "Skipping magentamobil-basic.pdf, output file already exists: data/text_outputs/magentamobil-basic.txt\n",
      "Skipping magentamobil-data-s.pdf, output file already exists: data/text_outputs/magentamobil-data-s.txt\n",
      "Skipping magentazuhause-xl.pdf, output file already exists: data/text_outputs/magentazuhause-xl.txt\n",
      "Skipping magentamobil-pluskarte-flex.pdf, output file already exists: data/text_outputs/magentamobil-pluskarte-flex.txt\n",
      "Skipping installationsanleitung-mein-magenta-app.pdf, output file already exists: data/text_outputs/installationsanleitung-mein-magenta-app.txt\n",
      "Skipping magentamobil-special-m-eins.pdf, output file already exists: data/text_outputs/magentamobil-special-m-eins.txt\n",
      "Skipping magentamobil-s-flex-young.pdf, output file already exists: data/text_outputs/magentamobil-s-flex-young.txt\n",
      "Skipping manuelle-konfiguration-magentazuhause-regio-zyxel-speedlink-5501.pdf, output file already exists: data/text_outputs/manuelle-konfiguration-magentazuhause-regio-zyxel-speedlink-5501.txt\n",
      "Skipping magentatv-downloads-fernbedienung.pdf, output file already exists: data/text_outputs/magentatv-downloads-fernbedienung.txt\n",
      "Skipping konfiguration-magentazuhause-regio-digitalsierungsbox-basic.pdf, output file already exists: data/text_outputs/konfiguration-magentazuhause-regio-digitalsierungsbox-basic.txt\n",
      "Skipping magentamobil-s.pdf, output file already exists: data/text_outputs/magentamobil-s.txt\n",
      "Skipping magentamobil-prepaid-m.pdf, output file already exists: data/text_outputs/magentamobil-prepaid-m.txt\n",
      "Skipping esim-profil-apple-watch.pdf, output file already exists: data/text_outputs/esim-profil-apple-watch.txt\n",
      "Skipping magentamobil-prepaid-l.pdf, output file already exists: data/text_outputs/magentamobil-prepaid-l.txt\n",
      "Skipping magentamobil-special-m-flex-young.pdf, output file already exists: data/text_outputs/magentamobil-special-m-flex-young.txt\n",
      "[2024/09/26 22:33:57] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.3781239986419678\n",
      "[2024/09/26 22:33:57] ppocr DEBUG: cls num  : 4, elapsed : 0.02264690399169922\n",
      "[2024/09/26 22:33:58] ppocr DEBUG: rec_res num  : 4, elapsed : 0.7519011497497559\n",
      "Calling GPT-4 Vision API with image: temp_page_0.png\n",
      "[2024/09/26 22:34:02] ppocr DEBUG: dt_boxes num : 29, elapsed : 0.28969597816467285\n",
      "[2024/09/26 22:34:02] ppocr DEBUG: cls num  : 29, elapsed : 0.13739824295043945\n",
      "[2024/09/26 22:34:06] ppocr DEBUG: rec_res num  : 29, elapsed : 4.188778877258301\n",
      "Calling GPT-4 Vision API with image: temp_page_1.png\n",
      "[2024/09/26 22:34:13] ppocr DEBUG: dt_boxes num : 16, elapsed : 0.36603498458862305\n",
      "[2024/09/26 22:34:13] ppocr DEBUG: cls num  : 16, elapsed : 0.10202288627624512\n",
      "[2024/09/26 22:34:15] ppocr DEBUG: rec_res num  : 16, elapsed : 2.1615028381347656\n",
      "Calling GPT-4 Vision API with image: temp_page_2.png\n",
      "[2024/09/26 22:34:20] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.3165147304534912\n",
      "[2024/09/26 22:34:20] ppocr DEBUG: cls num  : 4, elapsed : 0.04240083694458008\n",
      "[2024/09/26 22:34:20] ppocr DEBUG: rec_res num  : 4, elapsed : 0.2982289791107178\n",
      "Calling GPT-4 Vision API with image: temp_page_3.png\n",
      "[2024/09/26 22:34:23] ppocr DEBUG: dt_boxes num : 8, elapsed : 0.3186349868774414\n",
      "[2024/09/26 22:34:23] ppocr DEBUG: cls num  : 8, elapsed : 0.06277894973754883\n",
      "[2024/09/26 22:34:25] ppocr DEBUG: rec_res num  : 8, elapsed : 1.5071990489959717\n",
      "Calling GPT-4 Vision API with image: temp_page_4.png\n",
      "[2024/09/26 22:34:30] ppocr DEBUG: dt_boxes num : 5, elapsed : 0.32670068740844727\n",
      "[2024/09/26 22:34:30] ppocr DEBUG: cls num  : 5, elapsed : 0.04615664482116699\n",
      "[2024/09/26 22:34:31] ppocr DEBUG: rec_res num  : 5, elapsed : 1.1987228393554688\n",
      "Calling GPT-4 Vision API with image: temp_page_5.png\n",
      "[2024/09/26 22:34:37] ppocr DEBUG: dt_boxes num : 8, elapsed : 0.28710198402404785\n",
      "[2024/09/26 22:34:37] ppocr DEBUG: cls num  : 8, elapsed : 0.038906097412109375\n",
      "[2024/09/26 22:34:38] ppocr DEBUG: rec_res num  : 8, elapsed : 0.6560971736907959\n",
      "Calling GPT-4 Vision API with image: temp_page_6.png\n",
      "[2024/09/26 22:34:43] ppocr DEBUG: dt_boxes num : 21, elapsed : 0.28395819664001465\n",
      "[2024/09/26 22:34:43] ppocr DEBUG: cls num  : 21, elapsed : 0.09919166564941406\n",
      "[2024/09/26 22:34:45] ppocr DEBUG: rec_res num  : 21, elapsed : 2.0241286754608154\n",
      "Calling GPT-4 Vision API with image: temp_page_7.png\n",
      "[2024/09/26 22:34:51] ppocr DEBUG: dt_boxes num : 23, elapsed : 0.28519701957702637\n",
      "[2024/09/26 22:34:51] ppocr DEBUG: cls num  : 23, elapsed : 0.10825967788696289\n",
      "[2024/09/26 22:34:58] ppocr DEBUG: rec_res num  : 23, elapsed : 6.197633981704712\n",
      "Calling GPT-4 Vision API with image: temp_page_8.png\n",
      "[2024/09/26 22:36:04] ppocr DEBUG: dt_boxes num : 21, elapsed : 0.41213417053222656\n",
      "[2024/09/26 22:36:04] ppocr DEBUG: cls num  : 21, elapsed : 0.12374043464660645\n",
      "[2024/09/26 22:36:09] ppocr DEBUG: rec_res num  : 21, elapsed : 5.408863067626953\n",
      "Calling GPT-4 Vision API with image: temp_page_9.png\n",
      "[2024/09/26 22:36:18] ppocr DEBUG: dt_boxes num : 8, elapsed : 0.31627917289733887\n",
      "[2024/09/26 22:36:18] ppocr DEBUG: cls num  : 8, elapsed : 0.06214714050292969\n",
      "[2024/09/26 22:36:20] ppocr DEBUG: rec_res num  : 8, elapsed : 2.2427408695220947\n",
      "Calling GPT-4 Vision API with image: temp_page_10.png\n",
      "[2024/09/26 22:36:25] ppocr DEBUG: dt_boxes num : 20, elapsed : 0.32694196701049805\n",
      "[2024/09/26 22:36:25] ppocr DEBUG: cls num  : 20, elapsed : 0.11777853965759277\n",
      "[2024/09/26 22:36:30] ppocr DEBUG: rec_res num  : 20, elapsed : 5.645209074020386\n",
      "Calling GPT-4 Vision API with image: temp_page_11.png\n",
      "[2024/09/26 22:36:36] ppocr DEBUG: dt_boxes num : 21, elapsed : 0.37785983085632324\n",
      "[2024/09/26 22:36:36] ppocr DEBUG: cls num  : 21, elapsed : 0.12265706062316895\n",
      "[2024/09/26 22:36:42] ppocr DEBUG: rec_res num  : 21, elapsed : 6.117992877960205\n",
      "Calling GPT-4 Vision API with image: temp_page_12.png\n",
      "[2024/09/26 22:36:49] ppocr DEBUG: dt_boxes num : 18, elapsed : 0.32056593894958496\n",
      "[2024/09/26 22:36:49] ppocr DEBUG: cls num  : 18, elapsed : 0.10619282722473145\n",
      "[2024/09/26 22:36:54] ppocr DEBUG: rec_res num  : 18, elapsed : 5.258967876434326\n",
      "Calling GPT-4 Vision API with image: temp_page_13.png\n",
      "[2024/09/26 22:37:01] ppocr DEBUG: dt_boxes num : 11, elapsed : 0.2875950336456299\n",
      "[2024/09/26 22:37:01] ppocr DEBUG: cls num  : 11, elapsed : 0.05316162109375\n",
      "[2024/09/26 22:37:05] ppocr DEBUG: rec_res num  : 11, elapsed : 3.8621010780334473\n",
      "Calling GPT-4 Vision API with image: temp_page_14.png\n",
      "[2024/09/26 22:37:11] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.28527164459228516\n",
      "[2024/09/26 22:37:11] ppocr DEBUG: cls num  : 1, elapsed : 0.005629062652587891\n",
      "[2024/09/26 22:37:11] ppocr DEBUG: rec_res num  : 1, elapsed : 0.0765371322631836\n",
      "Calling GPT-4 Vision API with image: temp_page_15.png\n",
      "[2024/09/26 22:37:15] ppocr DEBUG: dt_boxes num : 19, elapsed : 0.28558778762817383\n",
      "[2024/09/26 22:37:16] ppocr DEBUG: cls num  : 19, elapsed : 0.09086990356445312\n",
      "[2024/09/26 22:37:18] ppocr DEBUG: rec_res num  : 19, elapsed : 2.8808372020721436\n",
      "Calling GPT-4 Vision API with image: temp_page_16.png\n",
      "[2024/09/26 22:37:26] ppocr DEBUG: dt_boxes num : 20, elapsed : 0.28466200828552246\n",
      "[2024/09/26 22:37:26] ppocr DEBUG: cls num  : 20, elapsed : 0.09521245956420898\n",
      "[2024/09/26 22:37:31] ppocr DEBUG: rec_res num  : 20, elapsed : 4.485996961593628\n",
      "Calling GPT-4 Vision API with image: temp_page_17.png\n",
      "[2024/09/26 22:37:37] ppocr DEBUG: dt_boxes num : 9, elapsed : 0.2829289436340332\n",
      "[2024/09/26 22:37:37] ppocr DEBUG: cls num  : 9, elapsed : 0.04461526870727539\n",
      "[2024/09/26 22:37:38] ppocr DEBUG: rec_res num  : 9, elapsed : 1.1249499320983887\n",
      "Calling GPT-4 Vision API with image: temp_page_18.png\n",
      "[2024/09/26 22:37:45] ppocr DEBUG: dt_boxes num : 10, elapsed : 0.28342676162719727\n",
      "[2024/09/26 22:37:45] ppocr DEBUG: cls num  : 10, elapsed : 0.04929518699645996\n",
      "[2024/09/26 22:37:48] ppocr DEBUG: rec_res num  : 10, elapsed : 2.795022964477539\n",
      "Calling GPT-4 Vision API with image: temp_page_19.png\n",
      "[2024/09/26 22:37:52] ppocr DEBUG: dt_boxes num : 21, elapsed : 0.28327226638793945\n",
      "[2024/09/26 22:37:52] ppocr DEBUG: cls num  : 21, elapsed : 0.09868192672729492\n",
      "[2024/09/26 22:37:56] ppocr DEBUG: rec_res num  : 21, elapsed : 4.241507053375244\n",
      "Calling GPT-4 Vision API with image: temp_page_20.png\n",
      "[2024/09/26 22:38:00] ppocr DEBUG: dt_boxes num : 10, elapsed : 0.2822718620300293\n",
      "[2024/09/26 22:38:00] ppocr DEBUG: cls num  : 10, elapsed : 0.04768204689025879\n",
      "[2024/09/26 22:38:03] ppocr DEBUG: rec_res num  : 10, elapsed : 2.879796028137207\n",
      "Calling GPT-4 Vision API with image: temp_page_21.png\n",
      "[2024/09/26 22:38:08] ppocr DEBUG: dt_boxes num : 13, elapsed : 0.2853119373321533\n",
      "[2024/09/26 22:38:08] ppocr DEBUG: cls num  : 13, elapsed : 0.06194591522216797\n",
      "[2024/09/26 22:38:11] ppocr DEBUG: rec_res num  : 13, elapsed : 3.4198219776153564\n",
      "Calling GPT-4 Vision API with image: temp_page_22.png\n",
      "[2024/09/26 22:38:16] ppocr DEBUG: dt_boxes num : 14, elapsed : 0.28316783905029297\n",
      "[2024/09/26 22:38:16] ppocr DEBUG: cls num  : 14, elapsed : 0.0677490234375\n",
      "[2024/09/26 22:38:20] ppocr DEBUG: rec_res num  : 14, elapsed : 3.858581066131592\n",
      "Calling GPT-4 Vision API with image: temp_page_23.png\n",
      "[2024/09/26 22:38:26] ppocr DEBUG: dt_boxes num : 12, elapsed : 0.28296518325805664\n",
      "[2024/09/26 22:38:26] ppocr DEBUG: cls num  : 12, elapsed : 0.057343244552612305\n",
      "[2024/09/26 22:38:30] ppocr DEBUG: rec_res num  : 12, elapsed : 3.6207668781280518\n",
      "Calling GPT-4 Vision API with image: temp_page_24.png\n",
      "[2024/09/26 22:38:35] ppocr DEBUG: dt_boxes num : 14, elapsed : 0.28501415252685547\n",
      "[2024/09/26 22:38:35] ppocr DEBUG: cls num  : 14, elapsed : 0.06699275970458984\n",
      "[2024/09/26 22:38:38] ppocr DEBUG: rec_res num  : 14, elapsed : 3.7384707927703857\n",
      "Calling GPT-4 Vision API with image: temp_page_25.png\n",
      "[2024/09/26 22:38:45] ppocr DEBUG: dt_boxes num : 12, elapsed : 0.28394508361816406\n",
      "[2024/09/26 22:38:45] ppocr DEBUG: cls num  : 12, elapsed : 0.056844234466552734\n",
      "[2024/09/26 22:38:49] ppocr DEBUG: rec_res num  : 12, elapsed : 3.458962917327881\n",
      "Calling GPT-4 Vision API with image: temp_page_26.png\n",
      "[2024/09/26 22:38:55] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.2868809700012207\n",
      "[2024/09/26 22:38:55] ppocr DEBUG: cls num  : 1, elapsed : 0.005552053451538086\n",
      "[2024/09/26 22:38:55] ppocr DEBUG: rec_res num  : 1, elapsed : 0.07721400260925293\n",
      "Calling GPT-4 Vision API with image: temp_page_27.png\n",
      "[2024/09/26 22:38:58] ppocr DEBUG: dt_boxes num : 10, elapsed : 0.2839930057525635\n",
      "[2024/09/26 22:38:58] ppocr DEBUG: cls num  : 10, elapsed : 0.047987937927246094\n",
      "[2024/09/26 22:39:00] ppocr DEBUG: rec_res num  : 10, elapsed : 2.0511159896850586\n",
      "Calling GPT-4 Vision API with image: temp_page_28.png\n",
      "[2024/09/26 22:39:06] ppocr DEBUG: dt_boxes num : 17, elapsed : 0.28426098823547363\n",
      "[2024/09/26 22:39:07] ppocr DEBUG: cls num  : 17, elapsed : 0.08129715919494629\n",
      "[2024/09/26 22:39:10] ppocr DEBUG: rec_res num  : 17, elapsed : 3.2442002296447754\n",
      "Calling GPT-4 Vision API with image: temp_page_29.png\n",
      "[2024/09/26 22:39:15] ppocr DEBUG: dt_boxes num : 19, elapsed : 0.28478193283081055\n",
      "[2024/09/26 22:39:15] ppocr DEBUG: cls num  : 19, elapsed : 0.08956336975097656\n",
      "[2024/09/26 22:39:19] ppocr DEBUG: rec_res num  : 19, elapsed : 4.433862924575806\n",
      "Calling GPT-4 Vision API with image: temp_page_30.png\n",
      "[2024/09/26 22:39:23] ppocr DEBUG: dt_boxes num : 12, elapsed : 0.285444974899292\n",
      "[2024/09/26 22:39:24] ppocr DEBUG: cls num  : 12, elapsed : 0.057267189025878906\n",
      "[2024/09/26 22:39:27] ppocr DEBUG: rec_res num  : 12, elapsed : 3.4832990169525146\n",
      "Calling GPT-4 Vision API with image: temp_page_31.png\n",
      "[2024/09/26 22:39:34] ppocr DEBUG: dt_boxes num : 17, elapsed : 0.2842600345611572\n",
      "[2024/09/26 22:39:34] ppocr DEBUG: cls num  : 17, elapsed : 0.08058023452758789\n",
      "[2024/09/26 22:39:38] ppocr DEBUG: rec_res num  : 17, elapsed : 4.546353816986084\n",
      "Calling GPT-4 Vision API with image: temp_page_32.png\n",
      "[2024/09/26 22:39:44] ppocr DEBUG: dt_boxes num : 23, elapsed : 0.2849910259246826\n",
      "[2024/09/26 22:39:45] ppocr DEBUG: cls num  : 23, elapsed : 0.1075899600982666\n",
      "[2024/09/26 22:39:48] ppocr DEBUG: rec_res num  : 23, elapsed : 3.652541160583496\n",
      "Calling GPT-4 Vision API with image: temp_page_33.png\n",
      "[2024/09/26 22:39:54] ppocr DEBUG: dt_boxes num : 18, elapsed : 0.28371572494506836\n",
      "[2024/09/26 22:39:54] ppocr DEBUG: cls num  : 18, elapsed : 0.08432936668395996\n",
      "[2024/09/26 22:39:58] ppocr DEBUG: rec_res num  : 18, elapsed : 4.150996923446655\n",
      "Calling GPT-4 Vision API with image: temp_page_34.png\n",
      "[2024/09/26 22:40:05] ppocr DEBUG: dt_boxes num : 0, elapsed : 0.285473108291626\n",
      "[2024/09/26 22:40:05] ppocr DEBUG: cls num  : 0, elapsed : 0\n",
      "[2024/09/26 22:40:05] ppocr DEBUG: rec_res num  : 0, elapsed : 0.0\n",
      "Error processing bedienungsanleitung-magenta-tv-stick.pdf: 'NoneType' object is not iterable\n",
      "Skipping inbetriebnahme-router-ohne-reset.pdf, output file already exists: data/text_outputs/inbetriebnahme-router-ohne-reset.txt\n",
      "Skipping magentamobil-prepaid-max.pdf, output file already exists: data/text_outputs/magentamobil-prepaid-max.txt\n",
      "Skipping kurzbedienungsanleitung-magenta-tv-box.pdf, output file already exists: data/text_outputs/kurzbedienungsanleitung-magenta-tv-box.txt\n",
      "Skipping magentamobil-xl-flex-young.pdf, output file already exists: data/text_outputs/magentamobil-xl-flex-young.txt\n",
      "Skipping magentamobil-m-young.pdf, output file already exists: data/text_outputs/magentamobil-m-young.txt\n",
      "Skipping konfiguration-magentazuhause-regio-zyxel-speedlink-6501.pdf, output file already exists: data/text_outputs/konfiguration-magentazuhause-regio-zyxel-speedlink-6501.txt\n",
      "Skipping checkliste-neuer-hausanschluss.pdf, output file already exists: data/text_outputs/checkliste-neuer-hausanschluss.txt\n",
      "Skipping esim-aktivierung-apple-watch.pdf, output file already exists: data/text_outputs/esim-aktivierung-apple-watch.txt\n",
      "Skipping magentamobil-s-young.pdf, output file already exists: data/text_outputs/magentamobil-s-young.txt\n",
      "Skipping magentamobil-m.pdf, output file already exists: data/text_outputs/magentamobil-m.txt\n",
      "Skipping magentamobil-prepaid-xl.pdf, output file already exists: data/text_outputs/magentamobil-prepaid-xl.txt\n",
      "Skipping magentamobil-special-m-young.pdf, output file already exists: data/text_outputs/magentamobil-special-m-young.txt\n",
      "Skipping MagentaTV_2.0.pdf, output file already exists: data/text_outputs/MagentaTV_2.0.txt\n",
      "Skipping magentamobil-prepaid-s.pdf, output file already exists: data/text_outputs/magentamobil-prepaid-s.txt\n",
      "Skipping magentamobil-l.pdf, output file already exists: data/text_outputs/magentamobil-l.txt\n",
      "Skipping magentamobil-s-flex.pdf, output file already exists: data/text_outputs/magentamobil-s-flex.txt\n",
      "Skipping inbetriebnahme-frtzbox-ohne-reset.pdf, output file already exists: data/text_outputs/inbetriebnahme-frtzbox-ohne-reset.txt\n",
      "Skipping magentamobil-xl-premium.pdf, output file already exists: data/text_outputs/magentamobil-xl-premium.txt\n",
      "Skipping magentamobil-basic-flex.pdf, output file already exists: data/text_outputs/magentamobil-basic-flex.txt\n",
      "Skipping wechselmatrix.pdf, output file already exists: data/text_outputs/wechselmatrix.txt\n",
      "Skipping lte-sofort.pdf, output file already exists: data/text_outputs/lte-sofort.txt\n",
      "Skipping magentamobil-l-young.pdf, output file already exists: data/text_outputs/magentamobil-l-young.txt\n",
      "Skipping kurzbedienungsanleitung-media-reiceiver-401.pdf, output file already exists: data/text_outputs/kurzbedienungsanleitung-media-reiceiver-401.txt\n",
      "Skipping magentamobil-xs.pdf, output file already exists: data/text_outputs/magentamobil-xs.txt\n",
      "Skipping magentamobil-m-flex-young.pdf, output file already exists: data/text_outputs/magentamobil-m-flex-young.txt\n",
      "Skipping magentamobil-l-flex-young.pdf, output file already exists: data/text_outputs/magentamobil-l-flex-young.txt\n",
      "Skipping unterstuetzte-kameras.pdf, output file already exists: data/text_outputs/unterstuetzte-kameras.txt\n",
      "Skipping magentamobil-prepaid-jahrestarif.pdf, output file already exists: data/text_outputs/magentamobil-prepaid-jahrestarif.txt\n",
      "Skipping magentamobil-data-l.pdf, output file already exists: data/text_outputs/magentamobil-data-l.txt\n",
      "Skipping magentamobil-data-m.pdf, output file already exists: data/text_outputs/magentamobil-data-m.txt\n",
      "[2024/09/26 22:40:09] ppocr DEBUG: dt_boxes num : 33, elapsed : 0.33454298973083496\n",
      "[2024/09/26 22:40:10] ppocr DEBUG: cls num  : 33, elapsed : 0.16093087196350098\n",
      "[2024/09/26 22:40:15] ppocr DEBUG: rec_res num  : 33, elapsed : 5.368196249008179\n",
      "Calling GPT-4 Vision API with image: temp_page_0.png\n",
      "Processed: magentamobil-m-flex.pdf\n",
      "[2024/09/26 22:40:24] ppocr DEBUG: dt_boxes num : 33, elapsed : 0.33272433280944824\n",
      "[2024/09/26 22:40:24] ppocr DEBUG: cls num  : 33, elapsed : 0.1549825668334961\n",
      "[2024/09/26 22:40:30] ppocr DEBUG: rec_res num  : 33, elapsed : 5.37174916267395\n",
      "Calling GPT-4 Vision API with image: temp_page_0.png\n",
      "Processed: magentamobil-l-flex.pdf\n",
      "[2024/09/26 22:40:39] ppocr DEBUG: dt_boxes num : 7, elapsed : 0.3014092445373535\n",
      "[2024/09/26 22:40:39] ppocr DEBUG: cls num  : 7, elapsed : 0.038400888442993164\n",
      "[2024/09/26 22:40:41] ppocr DEBUG: rec_res num  : 7, elapsed : 1.9324748516082764\n",
      "Calling GPT-4 Vision API with image: temp_page_0.png\n",
      "[2024/09/26 22:40:52] ppocr DEBUG: dt_boxes num : 69, elapsed : 0.27147698402404785\n",
      "[2024/09/26 22:40:53] ppocr DEBUG: cls num  : 69, elapsed : 0.32143402099609375\n",
      "[2024/09/26 22:40:59] ppocr DEBUG: rec_res num  : 69, elapsed : 6.31813383102417\n",
      "Calling GPT-4 Vision API with image: temp_page_1.png\n",
      "[2024/09/26 22:41:10] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.26567506790161133\n",
      "[2024/09/26 22:41:10] ppocr DEBUG: cls num  : 2, elapsed : 0.009955167770385742\n",
      "[2024/09/26 22:41:10] ppocr DEBUG: rec_res num  : 2, elapsed : 0.4269230365753174\n",
      "Calling GPT-4 Vision API with image: temp_page_2.png\n",
      "[2024/09/26 22:41:13] ppocr DEBUG: dt_boxes num : 13, elapsed : 0.26482725143432617\n",
      "[2024/09/26 22:41:13] ppocr DEBUG: cls num  : 13, elapsed : 0.06308317184448242\n",
      "[2024/09/26 22:41:17] ppocr DEBUG: rec_res num  : 13, elapsed : 3.32344126701355\n",
      "Calling GPT-4 Vision API with image: temp_page_3.png\n",
      "[2024/09/26 22:41:23] ppocr DEBUG: dt_boxes num : 17, elapsed : 0.2658712863922119\n",
      "[2024/09/26 22:41:23] ppocr DEBUG: cls num  : 17, elapsed : 0.08116674423217773\n",
      "[2024/09/26 22:41:28] ppocr DEBUG: rec_res num  : 17, elapsed : 5.279139041900635\n",
      "Calling GPT-4 Vision API with image: temp_page_4.png\n",
      "[2024/09/26 22:41:34] ppocr DEBUG: dt_boxes num : 32, elapsed : 0.26934289932250977\n",
      "[2024/09/26 22:41:34] ppocr DEBUG: cls num  : 32, elapsed : 0.1508958339691162\n",
      "[2024/09/26 22:41:39] ppocr DEBUG: rec_res num  : 32, elapsed : 5.113023042678833\n",
      "Calling GPT-4 Vision API with image: temp_page_5.png\n",
      "[2024/09/26 22:41:46] ppocr DEBUG: dt_boxes num : 24, elapsed : 0.2666192054748535\n",
      "[2024/09/26 22:41:46] ppocr DEBUG: cls num  : 24, elapsed : 0.11496520042419434\n",
      "[2024/09/26 22:41:52] ppocr DEBUG: rec_res num  : 24, elapsed : 5.492818832397461\n",
      "Calling GPT-4 Vision API with image: temp_page_6.png\n",
      "[2024/09/26 22:41:57] ppocr DEBUG: dt_boxes num : 21, elapsed : 0.2644937038421631\n",
      "[2024/09/26 22:41:57] ppocr DEBUG: cls num  : 21, elapsed : 0.09894466400146484\n",
      "[2024/09/26 22:42:01] ppocr DEBUG: rec_res num  : 21, elapsed : 3.8620059490203857\n",
      "Calling GPT-4 Vision API with image: temp_page_7.png\n",
      "[2024/09/26 22:42:06] ppocr DEBUG: dt_boxes num : 29, elapsed : 0.2693610191345215\n",
      "[2024/09/26 22:42:06] ppocr DEBUG: cls num  : 29, elapsed : 0.13706707954406738\n",
      "[2024/09/26 22:42:13] ppocr DEBUG: rec_res num  : 29, elapsed : 6.744936943054199\n",
      "Calling GPT-4 Vision API with image: temp_page_8.png\n",
      "[2024/09/26 22:42:20] ppocr DEBUG: dt_boxes num : 20, elapsed : 0.2642548084259033\n",
      "[2024/09/26 22:42:20] ppocr DEBUG: cls num  : 20, elapsed : 0.09551811218261719\n",
      "[2024/09/26 22:42:23] ppocr DEBUG: rec_res num  : 20, elapsed : 3.6937570571899414\n",
      "Calling GPT-4 Vision API with image: temp_page_9.png\n",
      "[2024/09/26 22:42:30] ppocr DEBUG: dt_boxes num : 45, elapsed : 0.266772985458374\n",
      "[2024/09/26 22:42:30] ppocr DEBUG: cls num  : 45, elapsed : 0.211226224899292\n",
      "[2024/09/26 22:42:35] ppocr DEBUG: rec_res num  : 45, elapsed : 4.994521856307983\n",
      "Calling GPT-4 Vision API with image: temp_page_10.png\n",
      "[2024/09/26 22:42:42] ppocr DEBUG: dt_boxes num : 32, elapsed : 0.2665700912475586\n",
      "[2024/09/26 22:42:43] ppocr DEBUG: cls num  : 32, elapsed : 0.1512458324432373\n",
      "[2024/09/26 22:42:49] ppocr DEBUG: rec_res num  : 32, elapsed : 6.443442106246948\n",
      "Calling GPT-4 Vision API with image: temp_page_11.png\n",
      "[2024/09/26 22:42:59] ppocr DEBUG: dt_boxes num : 13, elapsed : 0.26376914978027344\n",
      "[2024/09/26 22:42:59] ppocr DEBUG: cls num  : 13, elapsed : 0.06346678733825684\n",
      "[2024/09/26 22:43:04] ppocr DEBUG: rec_res num  : 13, elapsed : 4.344418048858643\n",
      "Calling GPT-4 Vision API with image: temp_page_12.png\n",
      "[2024/09/26 22:43:09] ppocr DEBUG: dt_boxes num : 37, elapsed : 0.2660391330718994\n",
      "[2024/09/26 22:43:09] ppocr DEBUG: cls num  : 37, elapsed : 0.17299318313598633\n",
      "[2024/09/26 22:43:16] ppocr DEBUG: rec_res num  : 37, elapsed : 6.999415874481201\n",
      "Calling GPT-4 Vision API with image: temp_page_13.png\n",
      "[2024/09/26 22:43:25] ppocr DEBUG: dt_boxes num : 21, elapsed : 0.26741695404052734\n",
      "[2024/09/26 22:43:25] ppocr DEBUG: cls num  : 21, elapsed : 0.10018014907836914\n",
      "[2024/09/26 22:43:29] ppocr DEBUG: rec_res num  : 21, elapsed : 4.5183610916137695\n",
      "Calling GPT-4 Vision API with image: temp_page_14.png\n",
      "[2024/09/26 22:43:36] ppocr DEBUG: dt_boxes num : 36, elapsed : 0.26800990104675293\n",
      "[2024/09/26 22:43:36] ppocr DEBUG: cls num  : 36, elapsed : 0.16883587837219238\n",
      "[2024/09/26 22:43:41] ppocr DEBUG: rec_res num  : 36, elapsed : 5.132073163986206\n",
      "Calling GPT-4 Vision API with image: temp_page_15.png\n",
      "[2024/09/26 22:43:48] ppocr DEBUG: dt_boxes num : 32, elapsed : 0.26692700386047363\n",
      "[2024/09/26 22:43:49] ppocr DEBUG: cls num  : 32, elapsed : 0.14995050430297852\n",
      "[2024/09/26 22:43:55] ppocr DEBUG: rec_res num  : 32, elapsed : 6.292487859725952\n",
      "Calling GPT-4 Vision API with image: temp_page_16.png\n",
      "[2024/09/26 22:44:04] ppocr DEBUG: dt_boxes num : 29, elapsed : 0.26799988746643066\n",
      "[2024/09/26 22:44:04] ppocr DEBUG: cls num  : 29, elapsed : 0.13527870178222656\n",
      "[2024/09/26 22:44:10] ppocr DEBUG: rec_res num  : 29, elapsed : 5.3514769077301025\n",
      "Calling GPT-4 Vision API with image: temp_page_17.png\n",
      "[2024/09/26 22:44:18] ppocr DEBUG: dt_boxes num : 36, elapsed : 0.26679277420043945\n",
      "[2024/09/26 22:44:18] ppocr DEBUG: cls num  : 36, elapsed : 0.16717243194580078\n",
      "[2024/09/26 22:44:24] ppocr DEBUG: rec_res num  : 36, elapsed : 6.170222043991089\n",
      "Calling GPT-4 Vision API with image: temp_page_18.png\n",
      "[2024/09/26 22:44:32] ppocr DEBUG: dt_boxes num : 29, elapsed : 0.3093068599700928\n",
      "[2024/09/26 22:44:32] ppocr DEBUG: cls num  : 29, elapsed : 0.15774202346801758\n",
      "[2024/09/26 22:44:37] ppocr DEBUG: rec_res num  : 29, elapsed : 5.340865135192871\n",
      "Calling GPT-4 Vision API with image: temp_page_19.png\n",
      "[2024/09/26 22:44:44] ppocr DEBUG: dt_boxes num : 38, elapsed : 0.2967510223388672\n",
      "[2024/09/26 22:44:44] ppocr DEBUG: cls num  : 38, elapsed : 0.199232816696167\n",
      "[2024/09/26 22:44:52] ppocr DEBUG: rec_res num  : 38, elapsed : 7.60099196434021\n",
      "Calling GPT-4 Vision API with image: temp_page_20.png\n",
      "[2024/09/26 22:45:00] ppocr DEBUG: dt_boxes num : 24, elapsed : 0.29697299003601074\n",
      "[2024/09/26 22:45:00] ppocr DEBUG: cls num  : 24, elapsed : 0.13298869132995605\n",
      "[2024/09/26 22:45:04] ppocr DEBUG: rec_res num  : 24, elapsed : 3.842550039291382\n",
      "Calling GPT-4 Vision API with image: temp_page_21.png\n",
      "[2024/09/26 22:45:09] ppocr DEBUG: dt_boxes num : 33, elapsed : 0.2936360836029053\n",
      "[2024/09/26 22:45:10] ppocr DEBUG: cls num  : 33, elapsed : 0.17716383934020996\n",
      "[2024/09/26 22:45:16] ppocr DEBUG: rec_res num  : 33, elapsed : 6.602074861526489\n",
      "Calling GPT-4 Vision API with image: temp_page_22.png\n",
      "[2024/09/26 22:45:24] ppocr DEBUG: dt_boxes num : 21, elapsed : 0.29697608947753906\n",
      "[2024/09/26 22:45:24] ppocr DEBUG: cls num  : 21, elapsed : 0.12085795402526855\n",
      "[2024/09/26 22:45:28] ppocr DEBUG: rec_res num  : 21, elapsed : 3.979250907897949\n",
      "Calling GPT-4 Vision API with image: temp_page_23.png\n",
      "[2024/09/26 22:45:38] ppocr DEBUG: dt_boxes num : 33, elapsed : 0.29878783226013184\n",
      "[2024/09/26 22:45:38] ppocr DEBUG: cls num  : 33, elapsed : 0.17632675170898438\n",
      "[2024/09/26 22:45:46] ppocr DEBUG: rec_res num  : 33, elapsed : 7.644052982330322\n",
      "Calling GPT-4 Vision API with image: temp_page_24.png\n",
      "[2024/09/26 22:45:53] ppocr DEBUG: dt_boxes num : 37, elapsed : 0.3015599250793457\n",
      "[2024/09/26 22:45:53] ppocr DEBUG: cls num  : 37, elapsed : 0.19323301315307617\n",
      "[2024/09/26 22:45:59] ppocr DEBUG: rec_res num  : 37, elapsed : 5.976582288742065\n",
      "Calling GPT-4 Vision API with image: temp_page_25.png\n",
      "[2024/09/26 22:46:09] ppocr DEBUG: dt_boxes num : 35, elapsed : 0.30886101722717285\n",
      "[2024/09/26 22:46:09] ppocr DEBUG: cls num  : 35, elapsed : 0.18840980529785156\n",
      "[2024/09/26 22:46:15] ppocr DEBUG: rec_res num  : 35, elapsed : 6.277545928955078\n",
      "Calling GPT-4 Vision API with image: temp_page_26.png\n",
      "[2024/09/26 22:46:26] ppocr DEBUG: dt_boxes num : 46, elapsed : 0.3010828495025635\n",
      "[2024/09/26 22:46:26] ppocr DEBUG: cls num  : 46, elapsed : 0.2351076602935791\n",
      "[2024/09/26 22:46:37] ppocr DEBUG: rec_res num  : 46, elapsed : 10.764263153076172\n",
      "Calling GPT-4 Vision API with image: temp_page_27.png\n",
      "[2024/09/26 22:46:47] ppocr DEBUG: dt_boxes num : 26, elapsed : 0.3222942352294922\n",
      "[2024/09/26 22:46:47] ppocr DEBUG: cls num  : 26, elapsed : 0.15234589576721191\n",
      "[2024/09/26 22:46:52] ppocr DEBUG: rec_res num  : 26, elapsed : 5.453483581542969\n",
      "Calling GPT-4 Vision API with image: temp_page_28.png\n",
      "[2024/09/26 22:46:57] ppocr DEBUG: dt_boxes num : 14, elapsed : 0.3103148937225342\n",
      "[2024/09/26 22:46:57] ppocr DEBUG: cls num  : 14, elapsed : 0.08957791328430176\n",
      "[2024/09/26 22:47:00] ppocr DEBUG: rec_res num  : 14, elapsed : 2.5047218799591064\n",
      "Calling GPT-4 Vision API with image: temp_page_29.png\n",
      "[2024/09/26 22:47:05] ppocr DEBUG: dt_boxes num : 31, elapsed : 0.30930495262145996\n",
      "[2024/09/26 22:47:06] ppocr DEBUG: cls num  : 31, elapsed : 0.171234130859375\n",
      "[2024/09/26 22:47:10] ppocr DEBUG: rec_res num  : 31, elapsed : 4.019534111022949\n",
      "Calling GPT-4 Vision API with image: temp_page_30.png\n",
      "[2024/09/26 22:47:20] ppocr DEBUG: dt_boxes num : 30, elapsed : 0.3276371955871582\n",
      "[2024/09/26 22:47:20] ppocr DEBUG: cls num  : 30, elapsed : 0.1652686595916748\n",
      "[2024/09/26 22:47:27] ppocr DEBUG: rec_res num  : 30, elapsed : 7.131383180618286\n",
      "Calling GPT-4 Vision API with image: temp_page_31.png\n",
      "[2024/09/26 22:47:35] ppocr DEBUG: dt_boxes num : 27, elapsed : 0.3086080551147461\n",
      "[2024/09/26 22:47:35] ppocr DEBUG: cls num  : 27, elapsed : 0.14793872833251953\n",
      "[2024/09/26 22:47:41] ppocr DEBUG: rec_res num  : 27, elapsed : 5.532289743423462\n",
      "Calling GPT-4 Vision API with image: temp_page_32.png\n",
      "[2024/09/26 22:47:48] ppocr DEBUG: dt_boxes num : 36, elapsed : 0.30902719497680664\n",
      "[2024/09/26 22:47:48] ppocr DEBUG: cls num  : 36, elapsed : 0.19353485107421875\n",
      "[2024/09/26 22:47:54] ppocr DEBUG: rec_res num  : 36, elapsed : 6.273850917816162\n",
      "Calling GPT-4 Vision API with image: temp_page_33.png\n",
      "[2024/09/26 22:48:02] ppocr DEBUG: dt_boxes num : 28, elapsed : 0.30765485763549805\n",
      "[2024/09/26 22:48:02] ppocr DEBUG: cls num  : 28, elapsed : 0.15114665031433105\n",
      "[2024/09/26 22:48:07] ppocr DEBUG: rec_res num  : 28, elapsed : 5.0446672439575195\n",
      "Calling GPT-4 Vision API with image: temp_page_34.png\n",
      "[2024/09/26 22:48:14] ppocr DEBUG: dt_boxes num : 25, elapsed : 0.3077809810638428\n",
      "[2024/09/26 22:48:14] ppocr DEBUG: cls num  : 25, elapsed : 0.13947677612304688\n",
      "[2024/09/26 22:48:18] ppocr DEBUG: rec_res num  : 25, elapsed : 4.083926200866699\n",
      "Calling GPT-4 Vision API with image: temp_page_35.png\n",
      "[2024/09/26 22:48:22] ppocr DEBUG: dt_boxes num : 13, elapsed : 0.31081414222717285\n",
      "[2024/09/26 22:48:22] ppocr DEBUG: cls num  : 13, elapsed : 0.08598780632019043\n",
      "[2024/09/26 22:48:25] ppocr DEBUG: rec_res num  : 13, elapsed : 2.5806467533111572\n",
      "Calling GPT-4 Vision API with image: temp_page_36.png\n",
      "[2024/09/26 22:48:33] ppocr DEBUG: dt_boxes num : 23, elapsed : 0.30861902236938477\n",
      "[2024/09/26 22:48:33] ppocr DEBUG: cls num  : 23, elapsed : 0.13059520721435547\n",
      "[2024/09/26 22:48:39] ppocr DEBUG: rec_res num  : 23, elapsed : 6.145391941070557\n",
      "Calling GPT-4 Vision API with image: temp_page_37.png\n",
      "Processed: manuelle-konfiguration-digitalisierungsbox-magentazuhause-regio.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script automates the process of extracting text data from PDF files, specifically from instructional and informational content related to telecommunication products.\n",
    "The primary steps include converting PDF pages to images, performing Optical Character Recognition (OCR) to extract text, and then using the extracted data to query \n",
    "OpenAI's GPT-4 Vision API for a more refined extraction and summarization of relevant information. \n",
    "\n",
    "The script:\n",
    "1. Converts PDF files to images (one image per page).\n",
    "2. Applies OCR to extract textual content from each image.\n",
    "3. Sends the image data to GPT-4 Vision to extract key instructions and details.\n",
    "4. Saves the extracted and summarized information to text files.\n",
    "\n",
    "Requirements:\n",
    "- OpenAI API key in a `.env` file (using dotenv for secure access).\n",
    "- PaddleOCR library for text extraction from images.\n",
    "- pdf2image for converting PDF pages to images.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pdf2image import convert_from_path\n",
    "from paddleocr import PaddleOCR\n",
    "import base64\n",
    "\n",
    "# Load environment variables from .env file for secure API access\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI API client with API key\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define directories for PDF inputs and text outputs\n",
    "pdf_dir = \"data/magenta_files\"      # Directory containing the PDF files\n",
    "output_dir = \"data/text_outputs\"     # Directory to store output text files\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "# Initialize PaddleOCR with English language support\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "# Function to convert an image file to a base64 encoded string\n",
    "def image_to_base64(image_path):\n",
    "    \"\"\"\n",
    "    Converts an image to a base64 encoded string for use in API requests.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: Base64 encoded string of the image.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "# Function to extract text from PDF and send to GPT-4 Vision API\n",
    "def extract_and_send_to_gpt4(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text content from each page of a PDF file using OCR and sends it to the GPT-4 Vision API for \n",
    "    summarization and extraction of key details.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Compiled textual content extracted and processed from the PDF.\n",
    "    \"\"\"\n",
    "    images = convert_from_path(pdf_path)  # Convert PDF pages to images\n",
    "    text_content = []\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        # Save each image page temporarily for OCR processing\n",
    "        image_path = f\"temp_page_{i}.png\"\n",
    "        image.save(image_path, \"PNG\")\n",
    "        \n",
    "        # Perform OCR on the image to extract textual information\n",
    "        ocr_result = ocr.ocr(image_path, cls=True)\n",
    "\n",
    "        # Parse OCR result to collect text data from the image\n",
    "        extracted_text = \"\"\n",
    "        for line in ocr_result:\n",
    "            for word_info in line:\n",
    "                extracted_text += word_info[1][0] + \" \"\n",
    "            extracted_text += \"\\n\"  # Line break for readability\n",
    "        \n",
    "        if extracted_text:\n",
    "            # Send extracted text data to GPT-4 Vision for further processing\n",
    "            response = extract_with_gpt4_vision(image_path)\n",
    "            text_content.append(response)  # Append GPT-4 Vision response to text content\n",
    "\n",
    "        # Remove temporary image file to conserve storage\n",
    "        os.remove(image_path)\n",
    "\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "# Function to interact with GPT-4 Vision API using image data\n",
    "def extract_with_gpt4_vision(image_path):\n",
    "    \"\"\"\n",
    "    Sends a base64-encoded image to the GPT-4 Vision API for analysis and extracts a summarized response.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: Summarized text from GPT-4 Vision API response.\n",
    "    \"\"\"\n",
    "    # Convert image to base64 format for embedding in the API request\n",
    "    img_b64_str = image_to_base64(image_path)\n",
    "\n",
    "    print(f\"Calling GPT-4 Vision API with image: {image_path}\")  # Informative log message\n",
    "\n",
    "    # API call to OpenAI GPT-4 Vision for image analysis\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Model used for vision processing\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": (\n",
    "                            \"Please analyze the following image. \"\n",
    "                            \"The image contains instructions, diagrams, and other informative content related to telecommunication products and devices. \"\n",
    "                            \"Extract and summarize the relevant information, including instructions and any important details that might be useful.\"\n",
    "                        )\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{img_b64_str}\"\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract and return the content of the API response\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Main loop to process each PDF file in the input directory\n",
    "for file_name in os.listdir(pdf_dir):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_dir, file_name)  # Full path to the PDF file\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}.txt\")  # Output path for text file\n",
    "        \n",
    "        # Skip processing if the output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Skipping {file_name}, output file already exists: {output_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Extract text content from PDF and send to GPT-4 Vision\n",
    "            text_content = extract_and_send_to_gpt4(pdf_path)\n",
    "\n",
    "            # Write the processed text to the output file\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(text_content)\n",
    "\n",
    "            print(f\"Processed: {file_name}\")  # Informative message for successful processing\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log error message if processing fails\n",
    "            print(f\"Error processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Categorisation\n",
    "classifies and organizes text files into multiple categories based on content relevance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q9/c3sjffm125370mhwph6zm3ww0000gn/T/ipykernel_2143/2739517395.py:59: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response = model([HumanMessage(content=prompt)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied magentamobil-s-flex-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-s-flex-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied manuelle-konfiguration-magentazuhause-regio-zyxel-speedlink-5501.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied manuelle-konfiguration-magentazuhause-regio-zyxel-speedlink-5501.txt to /Users/taha/Desktop/rag/data/Hilfe bei Störungen\n",
      "Copied magentazuhause-xl.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentazuhause-xl.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied magentamobil-pluskarte-flex.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-pluskarte-flex.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied installationsanleitung-mein-magenta-app.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied installationsanleitung-mein-magenta-app.txt to /Users/taha/Desktop/rag/data/Apps & Dienste\n",
      "Copied magentamobil-special-m-eins.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-special-m-eins.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-s.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-s.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-prepaid-m.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-prepaid-m.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied esim-profil-apple-watch.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied esim-profil-apple-watch.txt to /Users/taha/Desktop/rag/data/Geräte & Zubehör\n",
      "Copied magentamobil-prepaid-l.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-prepaid-l.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-special-m-flex-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-special-m-flex-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentatv-downloads-fernbedienung.txt to /Users/taha/Desktop/rag/data/TV\n",
      "Copied magentatv-downloads-fernbedienung.txt to /Users/taha/Desktop/rag/data/Geräte & Zubehör\n",
      "Copied konfiguration-magentazuhause-regio-digitalsierungsbox-basic.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied konfiguration-magentazuhause-regio-digitalsierungsbox-basic.txt to /Users/taha/Desktop/rag/data/Hilfe bei Störungen\n",
      "Copied magentamobil-speedbox-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-speedbox-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied konfiguration-zyxel-speedlink-5501.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied konfiguration-zyxel-speedlink-5501.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied inbetriebnahme-router-mit-reset.txt to /Users/taha/Desktop/rag/data/Hilfe bei Störungen\n",
      "Copied inbetriebnahme-router-mit-reset.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied magentamobil-speedbox-flex-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-speedbox-flex-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-xl.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-xl.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-speedbox-flex.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-speedbox-flex.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-basic.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-basic.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-data-s.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-data-s.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-xl-flex.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-xl-flex.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied inbetriebnahme-frtzbox-mit-reset.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied inbetriebnahme-frtzbox-mit-reset.txt to /Users/taha/Desktop/rag/data/Hilfe bei Störungen\n",
      "Copied magentamobil-xl-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-xl-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-special-m-flex.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-special-m-flex.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-l-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-l-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied kurzbedienungsanleitung-media-reiceiver-401.txt to /Users/taha/Desktop/rag/data/TV\n",
      "Copied kurzbedienungsanleitung-media-reiceiver-401.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied magentamobil-xs.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-xs.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-m-flex-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-m-flex-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied inbetriebnahme-frtzbox-ohne-reset.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied inbetriebnahme-frtzbox-ohne-reset.txt to /Users/taha/Desktop/rag/data/Geräte & Zubehör\n",
      "Copied magentamobil-s-flex.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-s-flex.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-xl-premium.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-xl-premium.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied wechselmatrix.txt to /Users/taha/Desktop/rag/data/Apps & Dienste\n",
      "Copied wechselmatrix.txt to /Users/taha/Desktop/rag/data/Geräte & Zubehör\n",
      "Copied magentamobil-basic-flex.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-basic-flex.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied lte-sofort.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied lte-sofort.txt to /Users/taha/Desktop/rag/data/Geräte & Zubehör\n",
      "Copied unterstuetzte-kameras.txt to /Users/taha/Desktop/rag/data/Apps & Dienste\n",
      "Copied unterstuetzte-kameras.txt to /Users/taha/Desktop/rag/data/Geräte & Zubehör\n",
      "Copied magentamobil-prepaid-jahrestarif.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-prepaid-jahrestarif.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-data-l.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-data-l.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-m-flex.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-m-flex.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-l-flex.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-l-flex.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-data-m.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-data-m.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied manuelle-konfiguration-digitalisierungsbox-magentazuhause-regio.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied manuelle-konfiguration-digitalisierungsbox-magentazuhause-regio.txt to /Users/taha/Desktop/rag/data/Geräte & Zubehör\n",
      "Copied magentamobil-l-flex-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-l-flex-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-m-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-m-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied konfiguration-magentazuhause-regio-zyxel-speedlink-6501.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied konfiguration-magentazuhause-regio-zyxel-speedlink-6501.txt to /Users/taha/Desktop/rag/data/Hilfe bei Störungen\n",
      "Copied inbetriebnahme-router-ohne-reset.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied inbetriebnahme-router-ohne-reset.txt to /Users/taha/Desktop/rag/data/Hilfe bei Störungen\n",
      "Copied magentamobil-prepaid-max.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-prepaid-max.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied kurzbedienungsanleitung-magenta-tv-box.txt to /Users/taha/Desktop/rag/data/TV\n",
      "Copied kurzbedienungsanleitung-magenta-tv-box.txt to /Users/taha/Desktop/rag/data/MagentaEINS\n",
      "Copied magentamobil-xl-flex-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-xl-flex-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-m.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-m.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied MagentaTV_2.0.txt to /Users/taha/Desktop/rag/data/TV\n",
      "Copied MagentaTV_2.0.txt to /Users/taha/Desktop/rag/data/MagentaEINS\n",
      "Copied magentamobil-prepaid-xl.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-prepaid-xl.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-special-m-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-special-m-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-prepaid-s.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-prepaid-s.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied magentamobil-l.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-l.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied checkliste-neuer-hausanschluss.txt to /Users/taha/Desktop/rag/data/Internet & Telefonie\n",
      "Copied checkliste-neuer-hausanschluss.txt to /Users/taha/Desktop/rag/data/Geräte & Zubehör\n",
      "Copied magentamobil-s-young.txt to /Users/taha/Desktop/rag/data/Vertrag & Rechnung\n",
      "Copied magentamobil-s-young.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied esim-aktivierung-apple-watch.txt to /Users/taha/Desktop/rag/data/Mobilfunk\n",
      "Copied esim-aktivierung-apple-watch.txt to /Users/taha/Desktop/rag/data/Apps & Dienste\n"
     ]
    }
   ],
   "source": [
    "# It uses GPT-4o-mini to analyze text files in an input directory, identify the three most relevant categories for each file, \n",
    "# and then copies each file to the corresponding category folders within an output directory.\n",
    "# Dependencies: Ensure the environment contains required libraries and .env file with API key.\n",
    "\n",
    "# Required Libraries\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Example of how to run the function for processing a directory\n",
    "input_directory = \"data/text_outputs\"\n",
    "output_directory = \"/Users/taha/Desktop/rag/data\"\n",
    "\n",
    "load_dotenv()  # Load environment variables from a .env file\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the chat model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Define the mapping of category descriptions\n",
    "category_map = {\n",
    "    \"Vertrag & Rechnung\": \"vertrag_rechnung_ihre_daten_kundencenter_login-daten_rechnung_lieferstatus\",\n",
    "    \"Hilfe bei Störungen\": \"hilfe_stoerungen_stoerungen_selbst_beheben_melden_status_verfolgen\",\n",
    "    \"Mobilfunk\": \"mobilfunk_tarife_optionen_mobiles-internet_mailbox_esim_sim-karten\",\n",
    "    \"Internet & Telefonie\": \"internet_telefonie:_ausbau,_sicherheit,_einstellungen,_bauherren,_glasfaser_und_wlan\",\n",
    "    \"TV\": \"tv_magentatv_streaming-dienste_magentatv_jugendschutz_pins\",\n",
    "    \"MagentaEINS\": \"magentains_kombi-pakete_mit_magentains_vorteil_und_treuebonus\",\n",
    "    \"Apps & Dienste\": \"apps_dienste_e-mail_magenta_apps_voicemail_app_mobilityconnect\",\n",
    "    \"Geräte & Zubehör\": \"geraete_zubehoer_anleitungen_fuer_smartphones_tablets_telefone_router_receiver\"\n",
    "}\n",
    "\n",
    "# List of valid folder names\n",
    "folder_names = list(category_map.keys())\n",
    "\n",
    "def classify_with_gpt(content, model, category_map):\n",
    "    \"\"\"\n",
    "    Classifies the content into the three most relevant categories using GPT-4.\n",
    "\n",
    "    Parameters:\n",
    "    - content (str): The text content of the file to be classified.\n",
    "    - model (ChatOpenAI): The GPT-4o-mini model instance.\n",
    "    - category_map (dict): A dictionary with category descriptions.\n",
    "\n",
    "    Returns:\n",
    "    - tuple of str: The three most relevant category names determined by GPT-4.\n",
    "    \"\"\"\n",
    "    categories_with_descriptions = [f\"{name}: {desc}\" for name, desc in category_map.items()]\n",
    "    categories_text = \"\\n\".join(categories_with_descriptions)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Classify the following text into the three most relevant categories based on the descriptions:\\n\\n\"\n",
    "        f\"{categories_text}\\n\\n\"\n",
    "        f\"Text:\\n\\\"{content}\\\"\\n\\n\"\n",
    "        f\"Which three categories does this text fit into? Reply with only the three category names, separated by a comma.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = model([HumanMessage(content=prompt)])\n",
    "        category_response = response.content.strip()\n",
    "\n",
    "        # Parse and return the three categories\n",
    "        categories = [cat.strip() for cat in category_response.split(\",\")]\n",
    "        if len(categories) == 3 and all(cat in folder_names for cat in categories):\n",
    "            return categories[0], categories[1], categories[2]\n",
    "        else:\n",
    "            print(f\"Unrecognized categories returned: {category_response}\")\n",
    "            return folder_names[0], folder_names[1], folder_names[2]  # Default fallback\n",
    "    except Exception as e:\n",
    "        print(f\"Error during classification: {e}\")\n",
    "        return folder_names[0], folder_names[1], folder_names[2]  # Default fallback\n",
    "\n",
    "def read_in_chunks(file_path, chunk_size=8191):\n",
    "    \"\"\"\n",
    "    Reads a file in chunks to ensure that no single read exceeds the token limit.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the file to be read.\n",
    "    - chunk_size (int): The size of each chunk to read (default is 8191 characters).\n",
    "\n",
    "    Yields:\n",
    "    - str: A chunk of text from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield chunk\n",
    "\n",
    "def classify_file(file_path, model, category_map):\n",
    "    \"\"\"\n",
    "    Classifies a file's content by reading in chunks and finding the three most relevant categories.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the file to be classified.\n",
    "    - model (ChatOpenAI): The GPT-4o-mini model instance.\n",
    "    - category_map (dict): A dictionary with category descriptions.\n",
    "\n",
    "    Returns:\n",
    "    - tuple of str: The three most relevant categories for the file.\n",
    "    \"\"\"\n",
    "    categories_count = {cat: 0 for cat in folder_names}\n",
    "    \n",
    "    for chunk in read_in_chunks(file_path):\n",
    "        try:\n",
    "            first_category, second_category, third_category = classify_with_gpt(chunk, model, category_map)\n",
    "            categories_count[first_category] += 1\n",
    "            categories_count[second_category] += 1\n",
    "            categories_count[third_category] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "\n",
    "    # Sort categories by count and select the top three\n",
    "    sorted_categories = sorted(categories_count, key=categories_count.get, reverse=True)\n",
    "    return sorted_categories[0], sorted_categories[1], sorted_categories[2]\n",
    "\n",
    "def copy_to_three_categories(file_path, output_directory, model, category_map):\n",
    "    \"\"\"\n",
    "    Classifies a file's content and copies it to the three most relevant categories.\n",
    "    Also, prepends \"pdf_\" to the file name.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the file.\n",
    "    - output_directory (str): Path to the main output directory where files should be organized.\n",
    "    - model (ChatOpenAI): The GPT-4o-mini model instance.\n",
    "    - category_map (dict): A dictionary with category descriptions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Classify the file into three most relevant categories\n",
    "        first_category, second_category, third_category = classify_file(file_path, model, category_map)\n",
    "\n",
    "        filename = os.path.basename(file_path)\n",
    "        new_filename = \"pdf_\" + filename\n",
    "\n",
    "        # Copy to the first category folder\n",
    "        first_category_dir = os.path.join(output_directory, first_category)\n",
    "        os.makedirs(first_category_dir, exist_ok=True)\n",
    "        shutil.copy(file_path, os.path.join(first_category_dir, new_filename))\n",
    "        print(f\"Copied {filename} to {first_category_dir}\")\n",
    "\n",
    "        # Copy to the second category folder\n",
    "        second_category_dir = os.path.join(output_directory, second_category)\n",
    "        os.makedirs(second_category_dir, exist_ok=True)\n",
    "        shutil.copy(file_path, os.path.join(second_category_dir, new_filename))\n",
    "        print(f\"Copied {filename} to {second_category_dir}\")\n",
    "\n",
    "        # Copy to the third category folder\n",
    "        third_category_dir = os.path.join(output_directory, third_category)\n",
    "        os.makedirs(third_category_dir, exist_ok=True)\n",
    "        shutil.copy(file_path, os.path.join(third_category_dir, new_filename))\n",
    "        print(f\"Copied {filename} to {third_category_dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "def process_files_in_directory(input_directory, output_directory, category_map, model):\n",
    "    \"\"\"\n",
    "    Processes all .txt files in the input directory, classifies them, and copies them to three relevant categories.\n",
    "\n",
    "    Parameters:\n",
    "    - input_directory (str): Path to the directory containing .txt files.\n",
    "    - output_directory (str): Path to the main output directory where files should be organized.\n",
    "    - category_map (dict): A dictionary with category descriptions.\n",
    "    - model (ChatOpenAI): The GPT-4o-mini model instance.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            copy_to_three_categories(file_path, output_directory, model, category_map)\n",
    "\n",
    "process_files_in_directory(input_directory, output_directory, category_map, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
