{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Web Scraper Notebook\n",
    "\n",
    "\n",
    "This notebook provides a script for crawling a website to find and download PDF files. The script utilizes asynchronous HTTP requests for efficient web scraping and file downloading.\n",
    "\n",
    "The script performs the following tasks:\n",
    "1. **Crawls a specified website** to find all PDF links.\n",
    "2. **Downloads the PDF files** and saves them to a local directory (`data/pdf_files/`).\n",
    "3. **Logs the progress and errors** encountered during the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import logging\n",
    "\n",
    "# Directory to save downloaded PDF files\n",
    "DOWNLOAD_DIR = 'data/pdf_files'\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# Starting URL for crawling\n",
    "START_URL = 'https://www.telekom.de/hilfe'\n",
    "\n",
    "# Set to keep track of downloaded PDF filenames\n",
    "downloaded_files = set()\n",
    "\n",
    "# Configure logger settings for better traceability\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to find PDF links on a given URL\n",
    "async def find_pdfs(url, session):\n",
    "    \"\"\"\n",
    "    Fetches the content of a page and finds all PDF links.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the page to fetch.\n",
    "        session (aiohttp.ClientSession): The aiohttp session used for making HTTP requests.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of URLs pointing to PDF files.\n",
    "    \"\"\"\n",
    "    pdf_links = []\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            # Only process if the response is HTML\n",
    "            if 'text/html' in response.headers.get('Content-Type', '').lower():\n",
    "                soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "                # Extract all links ending with .pdf\n",
    "                pdf_links = [\n",
    "                    urljoin(url, a['href'])\n",
    "                    for a in soup.find_all('a', href=True)\n",
    "                    if a['href'].lower().endswith('.pdf')\n",
    "                ]\n",
    "            else:\n",
    "                logger.error(f'{url} is not an HTML page.')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to fetch {url}. Error: {str(e)}')\n",
    "    \n",
    "    return pdf_links\n",
    "\n",
    "# Function to download a PDF file\n",
    "async def download_pdf(url, session):\n",
    "    \"\"\"\n",
    "    Downloads a PDF file from the given URL and saves it to the local directory.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the PDF file to download.\n",
    "        session (aiohttp.ClientSession): The aiohttp session used for making HTTP requests.\n",
    "    \"\"\"\n",
    "    filename = url.split('/')[-1]\n",
    "    # Skip download if file has already been downloaded\n",
    "    if filename in downloaded_files:\n",
    "        logger.info(f'Already downloaded: {filename}')\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            # Only process if the response status is OK\n",
    "            if response.status == 200:\n",
    "                file_path = os.path.join(DOWNLOAD_DIR, filename)\n",
    "                # Save the PDF file to disk\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(await response.read())\n",
    "                downloaded_files.add(filename)\n",
    "                logger.info(f'Downloaded: {file_path}')\n",
    "            else:\n",
    "                logger.error(f'Failed to download {url}. Status code: {response.status}')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to download {url}. Error: {str(e)}')\n",
    "\n",
    "# Function to crawl the website and find PDF links\n",
    "async def crawl_site(start_url):\n",
    "    \"\"\"\n",
    "    Crawls the website starting from the given URL, finds PDF links, and downloads them.\n",
    "    \n",
    "    Args:\n",
    "        start_url (str): The starting URL for the crawl.\n",
    "    \"\"\"\n",
    "    urls_to_visit = {start_url}\n",
    "    visited_urls = set()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        while urls_to_visit:\n",
    "            url = urls_to_visit.pop()\n",
    "            # Skip URLs that have already been visited\n",
    "            if url in visited_urls:\n",
    "                continue\n",
    "            visited_urls.add(url)\n",
    "            logger.info(f'Crawling: {url}')\n",
    "\n",
    "            # Find and download PDF links on the current page\n",
    "            pdf_links = await find_pdfs(url, session)\n",
    "            for link in pdf_links:\n",
    "                await download_pdf(link, session)\n",
    "\n",
    "            try:\n",
    "                async with session.get(url) as response:\n",
    "                    # Only process if the response status is OK\n",
    "                    if response.status == 200:\n",
    "                        soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "                        # Add new URLs to visit that are within the starting URL and not visited yet\n",
    "                        urls_to_visit.update(\n",
    "                            urljoin(url, a['href'])\n",
    "                            for a in soup.find_all('a', href=True)\n",
    "                            if a['href'].startswith('/') and urljoin(url, a['href']) not in visited_urls\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                logger.error(f'Failed to crawl {url}. Error: {str(e)}')\n",
    "\n",
    "# Entry point for the script\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to start the web crawling process.\n",
    "    \"\"\"\n",
    "    await crawl_site(START_URL)\n",
    "\n",
    "# Function to run an asynchronous coroutine\n",
    "def run_async(coro):\n",
    "    \"\"\"\n",
    "    Runs an asynchronous coroutine using the current event loop.\n",
    "    \n",
    "    Args:\n",
    "        coro (coroutine): The coroutine to run.\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # If the event loop is already running, schedule the coroutine\n",
    "        asyncio.ensure_future(coro)\n",
    "    else:\n",
    "        # If the event loop is not running, run the coroutine until complete\n",
    "        loop.run_until_complete(coro)\n",
    "\n",
    "# Run the script\n",
    "if __name__ == '__main__':\n",
    "    run_async(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and filter PDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pypdf import PdfReader\n",
    "from pathlib import Path\n",
    "\n",
    "# Directories\n",
    "pdf_dir = \"data/pdf_files\"\n",
    "first_filter_dir = \"data/keyword_filter\"\n",
    "\n",
    "# Keywords to filter by\n",
    "filter_keywords = [\n",
    "    \"Datenschutzhinweis\", \"Datenschutzhinweise\", \"Datenschutzrichtlinie\", \"Datenschutz\", \"Data Privacy\", \"Data privacy\", \"Data privacy information\",\n",
    "    \"Ergänzende Bedingungen\", \"End-User License\", \"Firmware-Änderungen\", \"Firmwareänderungen\", \"Firmware\", \"Geschäftsbedingungen\",\n",
    "    \"Konformitätserklärung\", \"LEGAL NOTICE\", \"LIZENZTEXTE\", \"LICENSES\", \"LIZENZ\", \"LICENCE\" \"privacy\", \"Privacy\", \"RECHTLICHE HINWEISE\"\n",
    "]\n",
    "\n",
    "# Create the filtered directory if it doesn't exist\n",
    "Path(first_filter_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to check if the page has text, if it's an image, or other errors\n",
    "def check_first_page(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "\n",
    "        # Check if the PDF is encrypted\n",
    "        if reader.is_encrypted:\n",
    "            try:\n",
    "                # Try to decrypt with an empty password (sometimes PDFs don't need a password)\n",
    "                reader.decrypt(\"\")\n",
    "                print(f\"Decrypted PDF: {pdf_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not decrypt {pdf_path}: {e}\")\n",
    "                return False\n",
    "\n",
    "        first_page = reader.pages[0]\n",
    "\n",
    "        # Check if the page has any images (image-based PDF)\n",
    "        has_images = bool(first_page.images)\n",
    "\n",
    "        # Extract text from the first page\n",
    "        first_page_text = first_page.extract_text()\n",
    "\n",
    "        # If the page has images but no text, consider it image-only\n",
    "        if has_images and not first_page_text:\n",
    "            print(f\"Image-only page detected in {pdf_path}\")\n",
    "            return False\n",
    "\n",
    "        # If no extractable text, treat it as unreadable\n",
    "        if not first_page_text:\n",
    "            print(f\"Unreadable or image-only first page in {pdf_path}\")\n",
    "            return False\n",
    "\n",
    "        # Check for any of the keywords in the first page text\n",
    "        for keyword in filter_keywords:\n",
    "            if keyword in first_page_text:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle cases where the file cannot be read or is encrypted\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Iterate over the files in the pdf directory\n",
    "for file_name in os.listdir(pdf_dir):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_dir, file_name)\n",
    "        \n",
    "        # Check if the PDF should be included based on the first page content\n",
    "        if check_first_page(file_path):\n",
    "            # Destination path\n",
    "            destination_path = os.path.join(first_filter_dir, file_name)\n",
    "            \n",
    "            # Copy file if it doesn't already exist in the first_filter folder\n",
    "            if not os.path.exists(destination_path):\n",
    "                shutil.copy(file_path, destination_path)\n",
    "                print(f\"Copied: {file_name}\")\n",
    "            else:\n",
    "                print(f\"Skipped (already exists): {file_name}\")\n",
    "\n",
    "# Count and print the number of files in the filtered directory\n",
    "filtered_files = os.listdir(first_filter_dir)\n",
    "print(f\"Number of files in the filtered directory: {len(filtered_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading remaining complex PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ilk önce burada direkt PDF lerden metni alan klasik yontemleri kullaniyorum\n",
    "#### suraya refer ediyorum: https://pub.towardsai.net/advanced-rag-02-unveiling-pdf-parsing-b84ae866344e \n",
    "#### tezde karsilastirma yapmak icin bu kodu tutuyorum burda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/25 12:39:28] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/taha/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/taha/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/Users/taha/Desktop/rag/venv/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/Users/taha/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "Processed: combicard-smart-connect-m-ohne-endgeraet.pdf\n",
      "Processed: openscape-desk-phone-cp600-kurzbedienungsanleitung.pdf\n",
      "Processed: octo_f50_kurzbaeng0904.pdf\n",
      "Processed: kurzbedienungsanleitung_dect_500d.pdf\n",
      "Processed: b_308pa.pdf\n",
      "Processed: bedanl_e730-930.pdf\n",
      "Processed: samsung-note-20-ultra-android-12-tr.pdf\n",
      "Processed: samsung-note-20-ultra-android-12-ua.pdf\n",
      "Processed: schnittstellenbeschreibung-telekom-deutschland-gmbh-07-2011.pdf\n",
      "Processed: oct_f615_sip_an_f50_de_20110919_mangenta.pdf\n",
      "Processed: b_362pck.pdf\n",
      "Processed: bedienungsanleitung-mobilteil-elmeg-d130.pdf\n",
      "Processed: kbedanl_fax_500_gb_v_04.2007.pdf\n",
      "Processed: release-notes-firmware-w1001n.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1054a30e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/taha/Desktop/rag/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: benutzerhandbuch-erweiterte-konfiguration-digitalisierungsbox-premium-2.pdf\n",
      "Processed: kurzanl_tpx721_tk_11.01.pdf\n",
      "Processed: routing-modus-dt-version-zyxel-vmg1312-b30a.pdf\n",
      "Processed: data-sim.pdf\n",
      "Processed: bedienungsanleitung_alcatel_lucent_8232_06.2012.pdf\n",
      "Processed: einrichtung-companyflex-pbx-mode-digitalsierungsbox-basic.pdf\n",
      "Processed: bedienungsanleitung_eumex_402_stand_27022014.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import Table, Image as UnstructuredImage\n",
    "import layoutparser as lp\n",
    "from paddleocr import PaddleOCR\n",
    "from pdf2image import convert_from_path\n",
    "import openai\n",
    "from dotenv import load_dotenv  # Import the load_dotenv function\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define directories\n",
    "pdf_dir = \"data/keyword_filter\"\n",
    "output_dir = \"data/text_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "# Function to extract text from simpler PDFs using pdfplumber\n",
    "def extract_with_pdfplumber(pdf_path):\n",
    "    text_content = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                text_content.append((\"text\", text))\n",
    "    return text_content\n",
    "\n",
    "# Function to extract tables and text using Unstructured\n",
    "def extract_with_unstructured(pdf_path):\n",
    "    elements = partition_pdf(pdf_path)\n",
    "    text_content = []\n",
    "    \n",
    "    for element in elements:\n",
    "        if isinstance(element, Table):\n",
    "            text_content.append((\"table\", \"\\n\".join([str(row) for row in element.rows])))\n",
    "        elif isinstance(element, UnstructuredImage):\n",
    "            text_content.append((\"image\", element))\n",
    "        else:\n",
    "            text_content.append((\"text\", str(element)))\n",
    "    \n",
    "    return text_content\n",
    "\n",
    "# Function to extract text using Layout-Parser and PaddleOCR for complex PDFs\n",
    "def extract_with_layout_and_ocr(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    text_content = []\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        # Save page image for OCR\n",
    "        image_path = f\"temp_page_{i}.png\"\n",
    "        image.save(image_path, \"PNG\")\n",
    "        \n",
    "        # Perform OCR\n",
    "        ocr_result = ocr.ocr(image_path, cls=True)\n",
    "        for line in ocr_result:\n",
    "            text_content.append((\"ocr\", ' '.join([word_info[1][0] for word_info in line])))\n",
    "\n",
    "        # Clean up temp image\n",
    "        os.remove(image_path)\n",
    "\n",
    "    return text_content\n",
    "\n",
    "# Function to extract information using GPT-4 Vision (advanced)\n",
    "def extract_with_gpt4_vision(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        img_data = img_file.read()\n",
    "\n",
    "    # Call the OpenAI GPT API with the image data\n",
    "    response = openai.Image.create(\n",
    "        file=img_data,\n",
    "        model=\"gpt-4.0-turbo\"\n",
    "    )\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "# Main extraction loop\n",
    "for file_name in os.listdir(pdf_dir):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_dir, file_name)\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}.txt\")\n",
    "\n",
    "        try:\n",
    "            # Initialize text content list\n",
    "            text_content = []\n",
    "            \n",
    "            # Try rule-based extraction first\n",
    "            text_content += extract_with_pdfplumber(pdf_path)\n",
    "            if not any(content[0] == \"text\" for content in text_content):  # If no text is found, try Unstructured\n",
    "                text_content += extract_with_unstructured(pdf_path)\n",
    "            if not any(content[0] == \"text\" for content in text_content):  # If still no text, use layout parser and OCR\n",
    "                text_content += extract_with_layout_and_ocr(pdf_path)\n",
    "\n",
    "            # Check for images and apply GPT-4 Vision if needed\n",
    "            for i, item in enumerate(text_content):\n",
    "                if item[0] == \"image\":\n",
    "                    image_path = f\"temp_image_{i}.png\"\n",
    "                    item[1].to_image().save(image_path)  # Save the image for GPT-4 Vision\n",
    "                    vision_text = extract_with_gpt4_vision(image_path)\n",
    "                    text_content[i] = (\"vision\", vision_text)  # Replace image info with vision analysis\n",
    "                    os.remove(image_path)\n",
    "\n",
    "            # Write the extracted content to the output file\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                for content_type, content in text_content:\n",
    "                    output_file.write(f\"{content_type.upper()}: {content}\\n\\n\")  # Format for clarity\n",
    "\n",
    "            print(f\"Processed: {file_name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tamamen OpenAI GPT Vision kullanarak tek tek Pdf'lerin özet bilgisini aliyorum burada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/25 12:36:04] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/taha/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/taha/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/Users/taha/Desktop/rag/venv/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/Users/taha/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "Skipping combicard-smart-connect-m-ohne-endgeraet.pdf, output file already exists: data/text_outputs/combicard-smart-connect-m-ohne-endgeraet.txt\n",
      "Skipping openscape-desk-phone-cp600-kurzbedienungsanleitung.pdf, output file already exists: data/text_outputs/openscape-desk-phone-cp600-kurzbedienungsanleitung.txt\n",
      "Skipping octo_f50_kurzbaeng0904.pdf, output file already exists: data/text_outputs/octo_f50_kurzbaeng0904.txt\n",
      "[2024/09/25 12:36:36] ppocr DEBUG: dt_boxes num : 131, elapsed : 0.567673921585083\n",
      "[2024/09/25 12:36:36] ppocr DEBUG: cls num  : 131, elapsed : 0.6804831027984619\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pdf2image import convert_from_path\n",
    "from paddleocr import PaddleOCR\n",
    "import base64\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define directories\n",
    "pdf_dir = \"data/keyword_filter\"\n",
    "output_dir = \"data/text_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "# Function to convert an image to a base64 string\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "# Function to extract tables using OCR and send to GPT-4 Vision\n",
    "def extract_and_send_to_gpt4(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    text_content = []\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        # Save the image for processing\n",
    "        image_path = f\"temp_page_{i}.png\"\n",
    "        image.save(image_path, \"PNG\")\n",
    "        \n",
    "        # Use PaddleOCR to extract data from the image\n",
    "        ocr_result = ocr.ocr(image_path, cls=True)\n",
    "\n",
    "        # Collect extracted text and other data from OCR results\n",
    "        extracted_text = \"\"\n",
    "        for line in ocr_result:\n",
    "            for word_info in line:\n",
    "                extracted_text += word_info[1][0] + \" \"\n",
    "            extracted_text += \"\\n\"\n",
    "        \n",
    "        if extracted_text:\n",
    "            # Send the extracted data to GPT-4 Vision\n",
    "            response = extract_with_gpt4_vision(image_path)\n",
    "            text_content.append(response)\n",
    "\n",
    "        # Clean up the image file\n",
    "        os.remove(image_path)\n",
    "\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "# Function to extract information using GPT-4 Vision\n",
    "def extract_with_gpt4_vision(image_path):\n",
    "    # Convert the image to a base64 string\n",
    "    img_b64_str = image_to_base64(image_path)\n",
    "\n",
    "    print(f\"Calling GPT-4 Vision API with image: {image_path}\")  # Ekrana bilgi yazdır\n",
    "\n",
    "    # OpenAI API çağrısını uygun şekilde güncelleyelim\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Güncel model adını kullanıyoruz\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": (\n",
    "                            \"Please analyze the following image. \"\n",
    "                            \"The image contains instructions, diagrams, and other informative content related to telecommunication devices. \"\n",
    "                            \"Extract and summarize the relevant information, including instructions and any important details that might be useful.\"\n",
    "                        )\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{img_b64_str}\"\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Cevabı almak için doğru yöntemi kullanalım\n",
    "    return response.choices[0].message.content  # Bu kısım düzeltildi\n",
    "\n",
    "# Main extraction loop\n",
    "for file_name in os.listdir(pdf_dir):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_dir, file_name)\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}.txt\")\n",
    "        \n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Skipping {file_name}, output file already exists: {output_path}\")\n",
    "            continue  # Skip processing if output file exists\n",
    "\n",
    "        try:\n",
    "            # Extract information and send to GPT-4 Vision\n",
    "            text_content = extract_and_send_to_gpt4(pdf_path)\n",
    "\n",
    "            # Write the extracted content to the output file\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(text_content)\n",
    "\n",
    "            print(f\"Processed: {file_name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
