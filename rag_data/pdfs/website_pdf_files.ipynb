{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Web Scraper Notebook\n",
    "\n",
    "\n",
    "This notebook provides a script for crawling a website to find and download PDF files. The script utilizes asynchronous HTTP requests for efficient web scraping and file downloading.\n",
    "\n",
    "The script performs the following tasks:\n",
    "1. **Crawls a specified website** to find all PDF links.\n",
    "2. **Downloads the PDF files** and saves them to a local directory (`data/pdf_files/`).\n",
    "3. **Logs the progress and errors** encountered during the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import logging\n",
    "\n",
    "# Directory to save downloaded PDF files\n",
    "DOWNLOAD_DIR = 'data/pdf_files'\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# Starting URL for crawling\n",
    "START_URL = 'https://www.telekom.de/hilfe'\n",
    "\n",
    "# Set to keep track of downloaded PDF filenames\n",
    "downloaded_files = set()\n",
    "\n",
    "# Configure logger settings for better traceability\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to find PDF links on a given URL\n",
    "async def find_pdfs(url, session):\n",
    "    \"\"\"\n",
    "    Fetches the content of a page and finds all PDF links.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the page to fetch.\n",
    "        session (aiohttp.ClientSession): The aiohttp session used for making HTTP requests.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of URLs pointing to PDF files.\n",
    "    \"\"\"\n",
    "    pdf_links = []\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            # Only process if the response is HTML\n",
    "            if 'text/html' in response.headers.get('Content-Type', '').lower():\n",
    "                soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "                # Extract all links ending with .pdf\n",
    "                pdf_links = [\n",
    "                    urljoin(url, a['href'])\n",
    "                    for a in soup.find_all('a', href=True)\n",
    "                    if a['href'].lower().endswith('.pdf')\n",
    "                ]\n",
    "            else:\n",
    "                logger.error(f'{url} is not an HTML page.')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to fetch {url}. Error: {str(e)}')\n",
    "    \n",
    "    return pdf_links\n",
    "\n",
    "# Function to download a PDF file\n",
    "async def download_pdf(url, session):\n",
    "    \"\"\"\n",
    "    Downloads a PDF file from the given URL and saves it to the local directory.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the PDF file to download.\n",
    "        session (aiohttp.ClientSession): The aiohttp session used for making HTTP requests.\n",
    "    \"\"\"\n",
    "    filename = url.split('/')[-1]\n",
    "    # Skip download if file has already been downloaded\n",
    "    if filename in downloaded_files:\n",
    "        logger.info(f'Already downloaded: {filename}')\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            # Only process if the response status is OK\n",
    "            if response.status == 200:\n",
    "                file_path = os.path.join(DOWNLOAD_DIR, filename)\n",
    "                # Save the PDF file to disk\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(await response.read())\n",
    "                downloaded_files.add(filename)\n",
    "                logger.info(f'Downloaded: {file_path}')\n",
    "            else:\n",
    "                logger.error(f'Failed to download {url}. Status code: {response.status}')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to download {url}. Error: {str(e)}')\n",
    "\n",
    "# Function to crawl the website and find PDF links\n",
    "async def crawl_site(start_url):\n",
    "    \"\"\"\n",
    "    Crawls the website starting from the given URL, finds PDF links, and downloads them.\n",
    "    \n",
    "    Args:\n",
    "        start_url (str): The starting URL for the crawl.\n",
    "    \"\"\"\n",
    "    urls_to_visit = {start_url}\n",
    "    visited_urls = set()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        while urls_to_visit:\n",
    "            url = urls_to_visit.pop()\n",
    "            # Skip URLs that have already been visited\n",
    "            if url in visited_urls:\n",
    "                continue\n",
    "            visited_urls.add(url)\n",
    "            logger.info(f'Crawling: {url}')\n",
    "\n",
    "            # Find and download PDF links on the current page\n",
    "            pdf_links = await find_pdfs(url, session)\n",
    "            for link in pdf_links:\n",
    "                await download_pdf(link, session)\n",
    "\n",
    "            try:\n",
    "                async with session.get(url) as response:\n",
    "                    # Only process if the response status is OK\n",
    "                    if response.status == 200:\n",
    "                        soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "                        # Add new URLs to visit that are within the starting URL and not visited yet\n",
    "                        urls_to_visit.update(\n",
    "                            urljoin(url, a['href'])\n",
    "                            for a in soup.find_all('a', href=True)\n",
    "                            if a['href'].startswith('/') and urljoin(url, a['href']) not in visited_urls\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                logger.error(f'Failed to crawl {url}. Error: {str(e)}')\n",
    "\n",
    "# Entry point for the script\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to start the web crawling process.\n",
    "    \"\"\"\n",
    "    await crawl_site(START_URL)\n",
    "\n",
    "# Function to run an asynchronous coroutine\n",
    "def run_async(coro):\n",
    "    \"\"\"\n",
    "    Runs an asynchronous coroutine using the current event loop.\n",
    "    \n",
    "    Args:\n",
    "        coro (coroutine): The coroutine to run.\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # If the event loop is already running, schedule the coroutine\n",
    "        asyncio.ensure_future(coro)\n",
    "    else:\n",
    "        # If the event loop is not running, run the coroutine until complete\n",
    "        loop.run_until_complete(coro)\n",
    "\n",
    "# Run the script\n",
    "if __name__ == '__main__':\n",
    "    run_async(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
