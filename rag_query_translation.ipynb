{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG\n",
    "### Bütüm kodta tutarlilik kontrolü yap, mesela hersey cevabi \"answer\" olarak dönmeli. Front'a p sekilde vermeliyiz\n",
    "\n",
    "### Routing mantigi calismadi retriever'i sadece ilk seferde filtreliyor, her seferinde chroma ya gömüyü ve hepsini ariyor.\n",
    "### chain invoke etmeden retriever cagrildigi yerde filtreleme olabilir. Bunu dene!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Evime internet baglattirmak istiyorum, nelere dikkat etmeliyim?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the rag data\n",
    "data_directory = \"/Users/taha/Desktop/rag/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### acaba cakisan kütüphaneler var mi? güncel tut! HEPSINI BIR HUCRE YAP KONTROL ET!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import asyncio\n",
    "import gc\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, FewShotChatMessagePromptTemplate, PromptTemplate\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.schema import Document\n",
    "from operator import itemgetter\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load API Keys from environment variables\n",
    "load_dotenv()  # Load environment variables from a .env file\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")  # This key is loaded but not used in the code\n",
    "\n",
    "# Initialize the chat model and embedding model\n",
    "# ChatOpenAI is used to interact with the OpenAI GPT model, and OpenAIEmbeddings is used for generating embeddings for documents\n",
    "model = ChatOpenAI(model=\"gpt-4o\", api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model max token limit\n",
    "MAX_TOKEN_LENGTH = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summarizing chain with \"refine\" method to reduce token size\n",
    "summarize_chain = load_summarize_chain(model, chain_type=\"refine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_count(docs, question, prompt):\n",
    "    \"\"\"\n",
    "    If the total token count for the RAG chain exceeds the limit, summarize only the retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of documents to check for token limits and summarize if needed.\n",
    "        question (str): The original question to include in token count.\n",
    "        prompt (str): The prompt template to include in token count.\n",
    "        max_token_length (int): The maximum number of tokens allowed before summarization.\n",
    "\n",
    "    Returns:\n",
    "        list: Summarized documents or original documents based on token limit.\n",
    "    \"\"\"\n",
    "    # Calculate token counts for different components\n",
    "    prompt_tokens = num_tokens_from_string(prompt.format(context=\"dummy\", question=question), \"cl100k_base\")\n",
    "    question_tokens = num_tokens_from_string(question, \"cl100k_base\")\n",
    "    docs_tokens = sum([num_tokens_from_string(doc.page_content, \"cl100k_base\") for doc in docs])\n",
    "    \n",
    "    # Total token count including prompt, question, and documents\n",
    "    total_tokens = prompt_tokens + question_tokens + docs_tokens\n",
    "    #print(f\"Token count (prompt): {prompt_tokens}\")\n",
    "    #print(f\"Token count (question): {question_tokens}\")\n",
    "    #print(f\"Token count (retrieved documents): {docs_tokens}\")\n",
    "    #print(f\"Total token count (for RAG chain): {total_tokens}\")\n",
    "    \n",
    "   \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Routing\n",
    "\n",
    "#### Retriever'in belli klasörden datayi almasini istiyorum soruya göre\n",
    "#### ama bu olmuyor cünkü her defasinda vector database eski datanin üzerine yüklüyor.\n",
    "\n",
    "#### datacategory kismina daha etkili keyword'lar konabilir, illa alt cizgi olmak zorunda mi, daha nasil efektif hale getirebilirim?\n",
    "#### ona göre retriever klasör filtresi yapacak cünkü\n",
    "\n",
    "#### !! Others klasörüne refer etmiyor sadece 8 ana klasöre göre degerlendiriyor, bir logic kur!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user question to the most relevant datacategory.\"\"\"\n",
    "\n",
    "    datacategory: Literal[\"vertrag_rechnung_ihre_daten_kundencenter_login-daten_rechnung_lieferstatus\", \n",
    "                          \"hilfe_stoerungen_stoerungen_selbst_beheben_melden_status_verfolgen\",\n",
    "                          \"mobilfunk_tarife_optionen_mobiles-internet_mailbox_esim_sim-karten\",\n",
    "                          \"internet_telefonie:_ausbau,_sicherheit,_einstellungen,_bauherren,_glasfaser_und_wlan\",\n",
    "                          \"tv_magentatv_streaming-dienste_magentatv_jugendschutz_pins\",\n",
    "                          \"magentains_kombi-pakete_mit_magentains_vorteil_und_treuebonus\",\n",
    "                          \"apps_dienste_e-mail_magenta_apps_voicemail_app_mobilityconnect\",\n",
    "                          \"geraete_zubehoer_anleitungen_fuer_smartphones_tablets_telefone_router_receiver\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datacategory would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "structured_model = model.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing user questions to the appropriate data category.\n",
    "\n",
    "Based on the help category the question is referring to, route it to the relevant data category. \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_model\n",
    "\n",
    "\n",
    "category = router.invoke({\"question\": question})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    # Kategorileri ve ilgili alt dizinleri bir sözlükte tanımlayın\n",
    "    category_map = {\n",
    "        \"vertrag_rechnung_ihre_daten_kundencenter_login-daten_rechnung_lieferstatus\": \"Vertrag & Rechnung\",\n",
    "        \"hilfe_stoerungen_stoerungen_selbst_beheben_melden_status_verfolgen\": \"Hilfe bei Störungen\",\n",
    "        \"mobilfunk_tarife_optionen_mobiles-internet_mailbox_esim_sim-karten\": \"Mobilfunk\",\n",
    "        \"internet_telefonie:_ausbau,_sicherheit,_einstellungen,_bauherren,_glasfaser_und_wlan\": \"Internet & Telefonie\",\n",
    "        \"tv_magentatv_streaming-dienste_magentatv_jugendschutz_pins\": \"TV\",\n",
    "        \"magentains_kombi-pakete_mit_magentains_vorteil_und_treuebonus\": \"MagentaEINS\",\n",
    "        \"apps_dienste_e-mail_magenta_apps_voicemail_app_mobilityconnect\": \"Apps & Dienste\",\n",
    "        \"geraete_zubehoer_anleitungen_fuer_smartphones_tablets_telefone_router_receiver\": \"Geräte & Zubehör\"\n",
    "    }\n",
    "    \n",
    "    # Datacategory'yi küçült ve sözlükte ara, yoksa \"Others\" döner\n",
    "    return category_map.get(result.datacategory.lower(), \"Others\")\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"/Users/taha/Desktop/rag/data\"\n",
    "\n",
    "sub_directory = full_chain.invoke({\"question\": question})\n",
    "print(sub_directory)\n",
    "\n",
    "specific_directory = os.path.join(data_directory, sub_directory)\n",
    "print(specific_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever with filter\n",
    "#### ama sadece belli bir kategori filtresi ile calistirmak yeterli context saglamiyor, baska yerlerde de ilgili soru icin gerekli belge olabiliyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# İlk olarak, eski vectorstore ve retriever nesnelerini temizleyelim\n",
    "vectorstore = None\n",
    "retriever = None\n",
    "\n",
    "# Çöp toplama işlemi\n",
    "gc.collect()\n",
    "\n",
    "# Load documents from the specified directory\n",
    "loader = DirectoryLoader(data_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()  # Load all text documents matching the pattern\n",
    "print(f\"Loaded {len(docs)} documents.\")  # Debug print\n",
    "\n",
    "# Ensure all documents have metadata\n",
    "for doc in docs:\n",
    "    if 'full_path' not in doc.metadata:\n",
    "        doc.metadata['full_path'] = doc.metadata.get('source', 'unknown')\n",
    "\n",
    "# Manually filter documents based on metadata\n",
    "filtered_docs = [doc for doc in docs if doc.metadata.get('full_path', '').startswith(specific_directory)]\n",
    "print(f\"Filtered {len(filtered_docs)} documents.\")  # Debug print\n",
    "\n",
    "# Create a Chroma vector store from the filtered documents and embeddings\n",
    "if filtered_docs:\n",
    "    vectorstore = Chroma.from_documents(documents=filtered_docs, embedding=embedding)\n",
    "    print(\"Vectorstore created from filtered documents.\")\n",
    "\n",
    "    # Set up the retriever using the filtered vector store\n",
    "    retriever = vectorstore.as_retriever()\n",
    "else:\n",
    "    print(\"No documents found to create a vectorstore.\")\n",
    "    retriever = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RETRIEVER\n",
    "#### Komple data klsörünü yüklüyor, filtreleme yok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def initialize_vectorstore(directory):\n",
    "    \"\"\"\n",
    "    Initializes a vector store from the documents found in the specified directory.\n",
    "    This function performs the following steps:\n",
    "    1. Loads text documents from the given directory using a DirectoryLoader.\n",
    "    2. Creates embeddings for the loaded documents using a predefined embedding model.\n",
    "    3. Initializes a Chroma vector store with these embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): The path to the directory containing text files to be processed.\n",
    "    \n",
    "        \n",
    "    Returns:\n",
    "        vectorstore (Chroma): A Chroma vector store object containing the embeddings of the documents.\n",
    "        docs (List[Document]): A list of Document objects loaded from the specified directory.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Load documents from the specified directory using DirectoryLoader\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs = loader.load()  # Load all text documents matching the pattern\n",
    "    \n",
    "    # Create a Chroma vector store from the loaded documents and embeddings\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "    \n",
    "    return vectorstore, docs\n",
    "\n",
    "# Initialize the vector store and document list\n",
    "vectorstore, docs = initialize_vectorstore(data_directory)\n",
    "\n",
    "# Set up the retriever using the vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for generating an answer based on context and a question\n",
    "telekom_template = \"\"\"You are an assistant for question-answering tasks for telekom.de help, providing answers to Telekom customers or potential customers. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer or if the provided documents do not contain relevant information, simply say that unfortunately, you cannot assist with this question and please visit www.telekom.de/hilfe for further assistance. \n",
    "Use up to four sentences and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_telekom = ChatPromptTemplate.from_template(telekom_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - vec1 (np.ndarray): The first vector.\n",
    "    - vec2 (np.ndarray): The second vector.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The cosine similarity between vec1 and vec2.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2) if (norm_vec1 and norm_vec2) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous function to print generated queries\n",
    "async def print_generated_queries(question):\n",
    "    \"\"\"\n",
    "    Generates and prints multiple search queries related to the input question.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The input query for which related search queries are generated.\n",
    "    \"\"\"\n",
    "    queries = generate_queries.invoke({\"question\": question})\n",
    "    print(\"\\nGenerated Questions:\")\n",
    "    for q in queries:\n",
    "        print(f\"{q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for Generating Alternative Questions\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "# Create a prompt template for generating multiple perspectives of the user's question\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define a pipeline for generating alternative queries\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))  # Split the generated output into individual queries\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    \"\"\"\n",
    "    Returns a unique union of retrieved documents.\n",
    "\n",
    "    This function takes a list of lists of documents, flattens it, and removes duplicates\n",
    "    to ensure each document is unique.\n",
    "\n",
    "    Args:\n",
    "        documents (list of lists): A list where each element is a list of documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique documents.\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists of documents\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Remove duplicates by converting to a set and then back to a list\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Deserialize the documents back into their original form\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Define the retrieval chain, which includes generating queries, retrieving documents, and removing duplicates\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "# Retrieve multiple documents based on the input question\n",
    "multi_query_docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "\n",
    "def format_docs(docs, query_embedding):\n",
    "    \"\"\"\n",
    "    Formats the retrieved documents with their source and cosine similarity score.\n",
    "\n",
    "    This function takes a list of documents and formats them to include the source of each document\n",
    "    and its cosine similarity to the query embedding.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents retrieved from the database.\n",
    "        query_embedding (numpy array): The embedding of the user's query.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the source, similarity score, and content of each document.\n",
    "    \"\"\"\n",
    "    # Initialize a set to track unique sources\n",
    "    unique_sources = set()\n",
    "    formatted_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Retrieve the source of the document from its metadata\n",
    "        source = doc.metadata.get(\"source\")\n",
    "        # Check if the source is unique\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            # Compute the embedding of the document's content\n",
    "            document_embedding = embedding.embed_query(doc.page_content)\n",
    "            # Calculate cosine similarity between the query and document embeddings\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)\n",
    "            # Use a placeholder message if the document content is empty\n",
    "            content = doc.page_content.strip() or \"This document content is empty.\"\n",
    "            # Format the document's source, similarity score, and content\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "\n",
    "    # Join the formatted documents into a single string\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Define a retrieval and generation (RAG) chain for processing the question and context\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} \n",
    "    | prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async def retrieve_and_format_docs(question):\n",
    "    \"\"\"\n",
    "    Asynchronously retrieves and formats documents for the given question.\n",
    "\n",
    "    This function retrieves documents relevant to the user's question and formats them with their\n",
    "    source information and cosine similarity scores.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the answer and formatted documents.\n",
    "    \"\"\"\n",
    "    # Compute the embedding for the user's question\n",
    "    query_embedding = embedding.embed_query(question)\n",
    "    # Format the retrieved documents with their cosine similarity scores\n",
    "    formatted_docs = format_docs(multi_query_docs, query_embedding)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to retrieve an answer using the RAG chain asynchronously\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Fallback in case of TypeError, invoke the RAG chain synchronously\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    # Return the answer and the formatted documents\n",
    "    return answer, formatted_docs\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    The main asynchronous function to run the complete flow.\n",
    "\n",
    "    This function handles the process of generating alternative queries, retrieving and formatting\n",
    "    documents, and printing the final answer along with the source documents.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Retrieve and format documents, then get the answer\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "\n",
    "    get_token_count(multi_query_docs, question, prompt_telekom)\n",
    "    # Print the final answer\n",
    "    print(\"\\nAnswer:\", answer)\n",
    "     # Generate and print alternative queries\n",
    "    await print_generated_queries(question)\n",
    "    # Print the source documents used for the answer\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "# Execute the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for generating multiple search queries based on a single input query.\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create a chain for generating four related search queries\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Function for Reciprocal Rank Fusion (RRF)\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"\n",
    "    Applies Reciprocal Rank Fusion (RRF) to combine multiple lists of ranked documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - results (list[list]): A list of lists where each inner list contains ranked documents.\n",
    "    - k (int): An optional parameter for the RRF formula, default is 60.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of tuples where each tuple contains a document and its fused score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to store the fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Serialize the document to a string format to use as a key\n",
    "            doc_str = dumps(doc)\n",
    "            # Initialize the document's score if not already present\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the document's score using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort documents based on their fused scores in descending order\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples\n",
    "    return reranked_results\n",
    "\n",
    "# Create a retrieval chain that generates queries, retrieves documents, and applies RRF\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "fusion_docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "# Function to get embeddings for a document's content\n",
    "async def get_document_embeddings(doc):\n",
    "    \"\"\"\n",
    "    Retrieves the embeddings for a document's content asynchronously.\n",
    "    \n",
    "    Parameters:\n",
    "    - doc (Document): The document object whose content embeddings are to be retrieved.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: The embeddings of the document's content.\n",
    "    \"\"\"\n",
    "    return embedding.embed_query(doc.page_content)\n",
    "\n",
    "# Function to format fusion_docs as a readable string with similarity scores\n",
    "async def format_fusion_docs_with_similarity(fusion_docs):\n",
    "    \"\"\"\n",
    "    Formats the fusion documents with their scores and cosine similarity to the question.\n",
    "    \n",
    "    Parameters:\n",
    "    - fusion_docs (list[tuple]): A list of tuples containing documents and their scores.\n",
    "    \n",
    "    Returns:\n",
    "    - str: A formatted string containing each document's source, fusion score, cosine similarity, and content.\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    question_embedding = embedding.embed_query(question)\n",
    "    \n",
    "    for doc, score in fusion_docs:\n",
    "        doc_embedding = await get_document_embeddings(doc)\n",
    "        similarity = cosine_similarity(question_embedding, doc_embedding)\n",
    "        source = doc.metadata.get(\"source\", \"No source\")\n",
    "        content = doc.page_content\n",
    "        formatted_docs.append(f\"Source: {source}\\nFusion Score: {score:.4f}\\nCosine Similarity: {similarity:.4f}\\nContent: {content}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "\n",
    "# Create a chain that uses context and question to generate an answer\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")} \n",
    "    | prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Asynchronous function to retrieve and format documents, then get an answer\n",
    "async def retrieve_and_format_docs(question):\n",
    "    \"\"\"\n",
    "    Retrieves and formats documents, then obtains an answer to the question.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The query for which answers and document formats are required.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing the answer and the formatted documents.\n",
    "    \"\"\"\n",
    "    formatted_docs = await format_fusion_docs_with_similarity(fusion_docs)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to get the answer asynchronously\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Fallback to synchronous invocation if asynchronous fails\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "\n",
    "# Main function to run the sequence of operations\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the entire process: generating queries, retrieving and formatting documents, and getting answers.\n",
    "    \"\"\"\n",
    "    doc_list = [doc for doc, score in fusion_docs]\n",
    "    get_token_count(doc_list, question, prompt_telekom)\n",
    "    \n",
    "    answer, formatted_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"\\nAnswer:\", answer)\n",
    "    await print_generated_queries(question)\n",
    "    print(\"\\nSources:\")\n",
    "    print(formatted_docs)  # Print the formatted version of fusion_docs with similarity scores\n",
    "\n",
    "# Execute the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!Decomposition\n",
    "#### Calismadi olmadi maalesef, asnwer sadece 3. sorunun cevabini veriyor, stratch den baska kaynakalara bakip cözüm bulmak lazim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts and chains\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answered in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | model | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer recursion\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | model\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer_decomposition = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer_decomposition)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Back\n",
    "#### cosine similarity ve token sayisi eksik sadece calisiyor suan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Transform examples into example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate step-back queries\n",
    "generate_queries_step_back = prompt | model | StrOutputParser()\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Original Question: {question}\")\n",
    "print(f\"Step-Back Question: {step_back_question}\")\n",
    "\n",
    "# Response prompt template\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "def get_retrieved_content(retrieved_documents):\n",
    "    \"\"\"Format retrieved documents as a string with source information.\"\"\"\n",
    "    seen_sources = set()\n",
    "    content_list = []\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            content = (\n",
    "                f\"Source: {source}\\n\"\n",
    "                f\"Content:\\n{doc.page_content}\\n\"\n",
    "                \"------------------------------\\n\"\n",
    "            )\n",
    "            content_list.append(content)\n",
    "    return \"\\n\".join(content_list)\n",
    "\n",
    "def format_retrieved_context(query):\n",
    "    \"\"\"Retrieve and format context for the given query.\"\"\"\n",
    "    # Retrieve documents using the 'invoke' method\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    return get_retrieved_content(retrieved_docs)\n",
    "\n",
    "# Construct the chain to retrieve and generate the response\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": lambda x: format_retrieved_context(x[\"question\"]),\n",
    "        \"step_back_context\": lambda x: format_retrieved_context(x[\"step_back_question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "result = chain.invoke({\"question\": question, \"step_back_question\": step_back_question})\n",
    "\n",
    "# Display the final response\n",
    "print(\"\\nAnswer:\\n\", result)\n",
    "print(\"\\nNormal Context:\\n\", format_retrieved_context(question))\n",
    "print(\"\\nStep-Back Context:\\n\", format_retrieved_context(step_back_question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "# This list provides example pairs of input questions and their corresponding step-back questions for model training.\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create a prompt template for examples.\n",
    "# This template formats example messages for the model to learn from.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),  # Input from the user\n",
    "        (\"ai\", \"{output}\"),    # Model's response to the input\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a few-shot prompt template that includes example prompts.\n",
    "# This helps the model understand the context by providing example inputs and outputs.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Define the final prompt template.\n",
    "# This includes system instructions and integrates the few-shot prompt.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),  # Input question from the user\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate step-back queries using the defined prompt.\n",
    "# This involves processing the original question to generate a more general query.\n",
    "generate_queries_step_back = prompt | model | StrOutputParser()\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "\n",
    "# Response prompt template\n",
    "# This template is used to generate the final response based on the retrieved context and the original question.\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "def get_retrieved_content(retrieved_documents):\n",
    "    \"\"\"\n",
    "    Format retrieved documents as a string with source information.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_documents (list): List of documents retrieved based on the query.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string containing source and content of retrieved documents.\n",
    "    \"\"\"\n",
    "    seen_sources = set()  # Track unique sources\n",
    "    content_list = []      # List to accumulate formatted content\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown')  # Get source of the document\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            content = (\n",
    "                f\"Source: {source}\\n\"\n",
    "                f\"Content:\\n{doc.page_content}\\n\"\n",
    "                \"------------------------------\\n\"\n",
    "            )\n",
    "            content_list.append(content)\n",
    "    return \"\\n\".join(content_list)\n",
    "\n",
    "def format_retrieved_context(query):\n",
    "    \"\"\"\n",
    "    Retrieve and format context for the given query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query for which context needs to be retrieved.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string containing context relevant to the query.\n",
    "    \"\"\"\n",
    "    # Retrieve documents using the 'invoke' method\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    return get_retrieved_content(retrieved_docs)\n",
    "\n",
    "# Construct the chain to retrieve and generate the response.\n",
    "# This chain combines context retrieval and response generation.\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": lambda x: format_retrieved_context(x[\"question\"]),\n",
    "        \"step_back_context\": lambda x: format_retrieved_context(x[\"step_back_question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain to get the final response.\n",
    "result = chain.invoke({\"question\": question, \"step_back_question\": step_back_question})\n",
    "\n",
    "# Display the final response along with normal and step-back contexts.\n",
    "print(\"Answer:\", result)\n",
    "print(f\"\\n\\nOriginal Question: {question}\")\n",
    "print(f\"\\nStep-Back Question: {step_back_question}\")\n",
    "print(\"\\nNormal Context:\\n\", format_retrieved_context(question))\n",
    "print(\"\\nStep-Back Context:\\n\", format_retrieved_context(step_back_question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE document generation\n",
    "template = \"\"\"You are creating professional and customer-focused web page content and texts for a major telecommunications provider like Telekom.de. \n",
    "Your content is very brief, very clear, and informative. Please write a text for the following question\n",
    "Question: {question}\n",
    "text:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run HyDE generation\n",
    "try:\n",
    "    hyde_output = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "    print(f\"HyDE hypothetical answer:\\n{hyde_output.strip()}\\n\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating documents for retrieval: {e}\")\n",
    "    raise\n",
    "\n",
    "# Retrieve documents\n",
    "try:\n",
    "    retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "    retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Print retrieved documents, deduplicated\n",
    "    seen_sources = set()\n",
    "    print(\"Retrieved sources:\")\n",
    "    for doc in retrieved_docs:\n",
    "        source = doc.metadata.get('source', 'Unknown Source')\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            print(f\"\\nDocument Source: {source}\")\n",
    "            print(f\"Document Content:\\n{doc.page_content.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "try:\n",
    "    final_answer = final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "    print(f\"\\nFinal RAG Answer:\\n{final_answer.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating final RAG answer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE Document Generation\n",
    "# This section is responsible for creating professional and customer-focused content\n",
    "# for a major telecommunications provider based on a given question.\n",
    "\n",
    "# Define a template for generating content.\n",
    "# The template specifies that the content should be brief, clear, and informative.\n",
    "template = \"\"\"You are creating professional and customer-focused web page content and texts for a major telecommunications provider like Telekom.de. \n",
    "Your content is very brief, very clear, and informative. Please write a text for the following question:\n",
    "Question: {question}\n",
    "text:\"\"\"\n",
    "\n",
    "# Create a prompt template using the defined template.\n",
    "# This template will be used to generate content for a given question.\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define a chain to generate documents for retrieval.\n",
    "# This chain uses the prompt template, a language model, and an output parser.\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run HyDE document generation to produce content for the given question.\n",
    "# The try-except block handles potential errors during document generation.\n",
    "try:\n",
    "    hyde_output = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "    print(f\"HyDE hypothetical context:\\n{hyde_output.strip()}\\n\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating documents for retrieval: {e}\")\n",
    "    raise\n",
    "\n",
    "# Retrieve Documents\n",
    "# This section retrieves documents based on the generated content and prints them.\n",
    "\n",
    "# Define a chain to retrieve documents using the generated content.\n",
    "# The chain combines the document generation process with a retriever.\n",
    "try:\n",
    "    retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "    retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Print retrieved documents and deduplicate them based on source information.\n",
    "    seen_sources = set()\n",
    "    print(\"Retrieved sources:\")\n",
    "    for doc in retrieved_docs:\n",
    "        source = doc.metadata.get('source', 'Unknown Source')  # Get the source of the document\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            print(f\"\\nSource file: {source}\")\n",
    "            print(f\"Document Content:\\n{doc.page_content.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define a chain to generate the final answer using the RAG process.\n",
    "# The chain combines the prompt template, a language model, and an output parser.\n",
    "final_rag_chain = (\n",
    "    prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Generate the final answer using the RAG process.\n",
    "# The try-except block handles potential errors during the final answer generation.\n",
    "try:\n",
    "    final_answer = final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "    print(f\"\\nFinal Answer:\\n{final_answer.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating final RAG answer: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
