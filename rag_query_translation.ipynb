{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG\n",
    "### Routing mantigi calismadi retriever'i sadece ilk seferde filtreliyor, her seferinde chroma ya gömüyü ve hepsini ariyor.\n",
    "### chain invoke etmeden retriever cagrildigi yerde filtreleme olabilir. Bunu dene!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Telekom'da hangi tarifeler mevcut?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the rag data\n",
    "data_directory = \"/Users/taha/Desktop/rag/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, FewShotChatMessagePromptTemplate, PromptTemplate\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# Load API Keys from environment variables\n",
    "load_dotenv()  # Load environment variables from a .env file\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")  # This key is loaded but not used in the code\n",
    "\n",
    "# Initialize the chat model and embedding model\n",
    "# ChatOpenAI is used to interact with the OpenAI GPT model, and OpenAIEmbeddings is used for generating embeddings for documents\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user question to the most relevant datacategory.\"\"\"\n",
    "\n",
    "    datacategory: Literal[\"vertrag_rechnung_ihre_daten_kundencenter_login-daten_rechnung_lieferstatus\", \n",
    "                          \"hilfe_stoerungen_stoerungen_selbst_beheben_melden_status_verfolgen\",\n",
    "                          \"mobilfunk_tarife_optionen_mobiles-internet_mailbox_esim_sim-karten\",\n",
    "                          \"internet_telefonie:_ausbau,_sicherheit,_einstellungen,_bauherren,_glasfaser_und_wlan\",\n",
    "                          \"tv_magentatv_streaming-dienste_magentatv_jugendschutz_pins\",\n",
    "                          \"magentains_kombi-pakete_mit_magentains_vorteil_und_treuebonus\",\n",
    "                          \"apps_dienste_e-mail_magenta_apps_voicemail_app_mobilityconnect\",\n",
    "                          \"geraete_zubehoer_anleitungen_fuer_smartphones_tablets_telefone_router_receiver\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datacategory would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "structured_model = model.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data category.\n",
    "\n",
    "Based on help category the question is referring to, route it to the relevant data category.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_model\n",
    "\n",
    "\n",
    "category = router.invoke({\"question\": question})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    # Kategorileri ve ilgili alt dizinleri bir sözlükte tanımlayın\n",
    "    category_map = {\n",
    "        \"vertrag_rechnung_ihre_daten_kundencenter_login-daten_rechnung_lieferstatus\": \"Vertrag & Rechnung\",\n",
    "        \"hilfe_stoerungen_stoerungen_selbst_beheben_melden_status_verfolgen\": \"Hilfe bei Störungen\",\n",
    "        \"mobilfunk_tarife_optionen_mobiles-internet_mailbox_esim_sim-karten\": \"Mobilfunk\",\n",
    "        \"internet_telefonie:_ausbau,_sicherheit,_einstellungen,_bauherren,_glasfaser_und_wlan\": \"Internet & Telefonie\",\n",
    "        \"tv_magentatv_streaming-dienste_magentatv_jugendschutz_pins\": \"TV\",\n",
    "        \"magentains_kombi-pakete_mit_magentains_vorteil_und_treuebonus\": \"MagentaEINS\",\n",
    "        \"apps_dienste_e-mail_magenta_apps_voicemail_app_mobilityconnect\": \"Apps & Dienste\",\n",
    "        \"geraete_zubehoer_anleitungen_fuer_smartphones_tablets_telefone_router_receiver\": \"Geräte & Zubehör\"\n",
    "    }\n",
    "    \n",
    "    # Datacategory'yi küçült ve sözlükte ara, yoksa \"Others\" döner\n",
    "    return category_map.get(result.datacategory.lower(), \"Others\")\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobilfunk\n",
      "/Users/taha/Desktop/rag/data/Mobilfunk\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"/Users/taha/Desktop/rag/data\"\n",
    "\n",
    "sub_directory = full_chain.invoke({\"question\": question})\n",
    "print(sub_directory)\n",
    "\n",
    "choosen_data_directory = os.path.join(data_directory, sub_directory)\n",
    "print(choosen_data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize the retriever\\ndef initialize_vectorstore(main_directory, additional_directory):\\n    \"\"\"\\n    Initializes a vector store from the documents found in the specified directories.\\n    \\n    This function performs the following steps:\\n    1. Loads text documents from the given main directory and an additional directory using DirectoryLoader.\\n    2. Creates embeddings for the loaded documents using a predefined embedding model.\\n    3. Initializes a Chroma vector store with these embeddings.\\n    \\n    Parameters:\\n        main_directory (str): The path to the main directory containing text files to be processed.\\n        additional_directory (str): The path to the additional directory containing extra text files to be included.\\n        \\n    Returns:\\n        vectorstore (Chroma): A Chroma vector store object containing the embeddings of the documents.\\n        docs (List[Document]): A list of Document objects loaded from the specified directories.\\n        \\n    \"\"\"\\n    \\n    # Load documents from the main directory using DirectoryLoader\\n    loader_main = DirectoryLoader(main_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\\n    docs_main = loader_main.load()  # Load all text documents from the main directory\\n\\n    # Load documents from the additional directory using DirectoryLoader\\n    loader_additional = DirectoryLoader(additional_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\\n    docs_additional = loader_additional.load()  # Load all text documents from the additional directory\\n\\n    # Combine the documents from both directories\\n    all_docs = docs_main + docs_additional\\n    \\n    # Create a Chroma vector store from the loaded documents and embeddings\\n    vectorstore = Chroma.from_documents(documents=all_docs, embedding=embedding)\\n    \\n    return vectorstore, all_docs\\n\\nadditional_directory = \"data/Others\"  # Additional directory path\\n\\n# Initialize the vector store and document list with both directories\\nvectorstore, docs = initialize_vectorstore(data_directory, additional_directory)\\n\\n# Set up the retriever using the vector store\\nretriever = vectorstore.as_retriever()\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def initialize_vectorstore(directory):\n",
    "    \"\"\"\n",
    "    Initializes a vector store from the documents found in the specified directory.\n",
    "    \n",
    "    This function performs the following steps:\n",
    "    1. Loads text documents from the given directory using a DirectoryLoader.\n",
    "    2. Creates embeddings for the loaded documents using a predefined embedding model.\n",
    "    3. Initializes a Chroma vector store with these embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): The path to the directory containing text files to be processed.\n",
    "        \n",
    "    Returns:\n",
    "        vectorstore (Chroma): A Chroma vector store object containing the embeddings of the documents.\n",
    "        docs (List[Document]): A list of Document objects loaded from the specified directory.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Load documents from the specified directory using DirectoryLoader\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs = loader.load()  # Load all text documents matching the pattern\n",
    "    \n",
    "    # Create a Chroma vector store from the loaded documents and embeddings\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "    \n",
    "    return vectorstore, docs\n",
    "\n",
    "# Initialize the vector store and document list\n",
    "vectorstore, docs = initialize_vectorstore(choosen_data_directory)\n",
    "\n",
    "# Set up the retriever using the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "'''\n",
    "# Initialize the retriever\n",
    "def initialize_vectorstore(main_directory, additional_directory):\n",
    "    \"\"\"\n",
    "    Initializes a vector store from the documents found in the specified directories.\n",
    "    \n",
    "    This function performs the following steps:\n",
    "    1. Loads text documents from the given main directory and an additional directory using DirectoryLoader.\n",
    "    2. Creates embeddings for the loaded documents using a predefined embedding model.\n",
    "    3. Initializes a Chroma vector store with these embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        main_directory (str): The path to the main directory containing text files to be processed.\n",
    "        additional_directory (str): The path to the additional directory containing extra text files to be included.\n",
    "        \n",
    "    Returns:\n",
    "        vectorstore (Chroma): A Chroma vector store object containing the embeddings of the documents.\n",
    "        docs (List[Document]): A list of Document objects loaded from the specified directories.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Load documents from the main directory using DirectoryLoader\n",
    "    loader_main = DirectoryLoader(main_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs_main = loader_main.load()  # Load all text documents from the main directory\n",
    "\n",
    "    # Load documents from the additional directory using DirectoryLoader\n",
    "    loader_additional = DirectoryLoader(additional_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs_additional = loader_additional.load()  # Load all text documents from the additional directory\n",
    "\n",
    "    # Combine the documents from both directories\n",
    "    all_docs = docs_main + docs_additional\n",
    "    \n",
    "    # Create a Chroma vector store from the loaded documents and embeddings\n",
    "    vectorstore = Chroma.from_documents(documents=all_docs, embedding=embedding)\n",
    "    \n",
    "    return vectorstore, all_docs\n",
    "\n",
    "additional_directory = \"data/Others\"  # Additional directory path\n",
    "\n",
    "# Initialize the vector store and document list with both directories\n",
    "vectorstore, docs = initialize_vectorstore(data_directory, additional_directory)\n",
    "\n",
    "# Set up the retriever using the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for generating an answer based on context and a question\n",
    "telekom_template = \"\"\"You are an assistant for question-answering tasks for telekom.de help, providing answers to Telekom customers or potential customers. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer or if the provided documents do not contain relevant information, simply say that unfortunately, you cannot assist with this question and please visit telekom.de/hilfe for further assistance. \n",
    "Use up to four sentences and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_telekom = ChatPromptTemplate.from_template(telekom_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - vec1 (np.ndarray): The first vector.\n",
    "    - vec2 (np.ndarray): The second vector.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The cosine similarity between vec1 and vec2.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2) if (norm_vec1 and norm_vec2) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous function to print generated queries\n",
    "async def print_generated_queries(question):\n",
    "    \"\"\"\n",
    "    Generates and prints multiple search queries related to the input question.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The input query for which related search queries are generated.\n",
    "    \"\"\"\n",
    "    queries = generate_queries.invoke({\"question\": question})\n",
    "    print(\"\\nGenerated Questions:\")\n",
    "    for q in queries:\n",
    "        print(f\"{q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Telekom, çeşitli tarifeler sunmaktadır. Mobilfunk-Tarifler ve Festnetz-Tarifler gibi çeşitli tarifeler bulunmaktadır. Ayrıca, belirli şirketlerin çalışanlarına ve Telekom çalışanlarının arkadaşlarına özel indirimler sunulur. Ek olarak, MagentaTV veya Disney+ gibi hizmetler için özel tarifeler de bulunmaktadır. Daha fazla bilgi için lütfen telekom.de/hilfe sayfasını ziyaret edin.\n",
      "\n",
      "Generated Questions:\n",
      "1. What are the available tariffs at Telekom?\n",
      "2. Can you list the tariff options offered by Telekom?\n",
      "3. Which tariff plans does Telekom currently offer?\n",
      "4. What kind of tariffs can I find at Telekom?\n",
      "5. Could you provide information on the existing Telekom tariffs?\n",
      "\n",
      "Sources:\n",
      "Source document: /Users/taha/Desktop/rag/data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vorteile_andere_firmen.txt\n",
      "\n",
      "Cosine Similarity: 0.7955\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vorteile/andere-firmen\n",
      "Telekom > Hilfe & Service > Vertrag & Rechnung > Telekom > Vorteile > Angebote > für > andere > Firmen\n",
      "\n",
      "Question: Welche Angebote bekomme ich als Mitarbeiter ausgewählter Unternehmen?\n",
      "Answer: Für Mitarbeitende bestimmter Unternehmen bietet die Telekom einen monatlichen Rabatt auf die Mobilfunk-Tarife und attraktive Preise für Smartphones & Tablets an. Für weitere Informationen registrieren Sie sich bitte mit Ihrer geschäftlichen E-Mail-Adresse auf unsererWebseite für die Vorteilsangebote.\n",
      "Hinweis: Wenn Sie wissen möchten, ob Ihre Firma dazu gehört, erkundigen Sie sich in Ihrem Unternehmen, in der Personalabteilung oder beim Betriebsrat.\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_tarife_optionen_smartphone_tarife_erkennen.txt\n",
      "\n",
      "Cosine Similarity: 0.8035\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/tarife-optionen/smartphone-tarife/erkennen\n",
      "Telekom > Hilfe & Service > Mobilfunk > Tarife & Optionen > Smartphone-Tarife > Aktueller > Tarif\n",
      "\n",
      "Question: Wie erkenne ich, ob ich einen aktuellen Mobilfunk-Tarif habe?\n",
      "Answer: Sie haben einen aktuellen Mobilfunk-Tarif, wenn beim Tarifnamen keine Generation dahintersteht.\n",
      "Beispiel für einen alten Tarif: MagentaMobil M mit Top-Handy (5. Generation).\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vorteile_for_friends_tarife.txt\n",
      "\n",
      "Cosine Similarity: 0.8034\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vorteile/for-friends-tarife\n",
      "Telekom > Hilfe & Service > Vertrag & Rechnung > Telekom > Vorteile > Telekom > for > Friends > Tarife\n",
      "\n",
      "Question: Was sind for Friends Tarife?\n",
      "Answer: Mit Telekom for Friends erhalten Freunde von Mitarbeitern der Telekom einen Sparvorteil auf den Grundpreis für Mobilfunk- und Festnetz-Tarife.\n",
      "Wie Sie dieTelekom for Friends Tarife erhaltenkönnen, erfahren Sie auf unserer Hilfe-Seite.\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vertrag_aenderung_zubuchoptionen.txt\n",
      "\n",
      "Cosine Similarity: 0.7979\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vertrag/aenderung/zubuchoptionen\n",
      "Telekom > Hilfe & Service > Vertrag & Rechnung > Vertrag > Änderung > Zubuchoptionen\n",
      "\n",
      "Question: Welche Zubuchoptionen gibt es für meinen Festnetz- und Mobilfunk-Vertrag?\n",
      "Answer: Zu Ihrem bestehenden Vertrag sind folgende Erweiterungen möglich:\n",
      "• Extras für Ihren Festnetz-Vertragz. B. Optionen für das Telefonieren, Fernseh-Optionen wie MagentaTV oder Telekom Sicherheitspakete\n",
      "• Extras für Ihren Mobilfunk-Vertragz. B. Optionen für Gespräche im und ins Ausland, Roaming-Datenpakete, Entertainment und mehr Sicherheit\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vertrag_aenderung_festnetz_tarif.txt\n",
      "\n",
      "Cosine Similarity: 0.7980\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vertrag/aenderung/festnetz-tarif\n",
      "Telekom > Hilfe & Service > Vertrag & Rechnung > Vertrag > Änderung > Festnetz-Tarif\n",
      "\n",
      "Question: Wie kann ich meinen Festnetz-Tarif ändern?\n",
      "Answer: So können Sie Ihren Festnetz-Vertrag im Kundencenter und der MeinMagenta App ändern:\n",
      "Hinweis: In einen höherwertigen Tarif können Sie bereits vor Ablauf der Mindestlaufzeit wechseln. In einen günstigeren oder gleichwertigen Tarif (z. B. aus einem Tarif mit MagentaTV in einen Tarif ohne Fernsehen) können Sie erst zum Ende der Vertragslaufzeit wechseln.\n",
      "• KundencenterWählen Sie im Bereich \"Verträge\" Ihren Festnetz-Vertrag aus. Klicken Sie nun auf den farbigen Button \"Tarif ändern\". Wählen Sie Ihren Wunschtarif und folgen Sie den weiteren Schritten.\n",
      "• MeinMagenta AppWählen Sie auf der Startseite Ihren Vertrag aus und klicken Sie dann auf \"Einstellungen ändern\". Klicken Sie dann auf \"Festnetz Vertrag verlängern\" und loggen sich dann ggf. mit Ihrem Telekom Login ein. Wählen Sie Ihren Wunschtarif und folgen Sie den weiteren Schritten.\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/TV/https_www_telekom_de_hilfe_tv_inhalte_disneyplus_buchen.txt\n",
      "\n",
      "Cosine Similarity: 0.7963\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/tv/inhalte/disneyplus/buchen\n",
      "Telekom > Hilfe & Service > TV > Inhalte > Disney+ > Buchung\n",
      "\n",
      "Question: In welchen Tarifen kann ich Disney+ buchen?\n",
      "Answer: Sie könnenDisney+in folgenden Tarifen buchen:\n",
      "Hinweis:Bei MagentaTV SmartStream 2.0 und MagentaTV MegaStream 2.0  ist die Option bereits inklusive.\n",
      "• MagentaZuhause (ausgenommen MagentaZuhause XS/via Funk/Surf)\n",
      "• MagentaMobil (ab der 3. Generation)\n",
      "• MagentaTV Tarife\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_tarife_optionen_ausland_roaming_preise_ausland.txt\n",
      "\n",
      "Cosine Similarity: 0.7952\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/tarife-optionen/ausland-roaming/preise-ausland\n",
      "Telekom > Hilfe & Service > Mobilfunk > Tarife & Optionen > Ausland & Roaming > Preisinformation\n",
      "\n",
      "Question: Welche Kosten habe ich, wenn ich im Ausland telefoniere oder surfe?\n",
      "Answer: In unserer untenstehenden \"Preisinformationen Ausland\" können Sie ermitteln, welche Kosten Ihnen beim Telefonieren und Surfen im Ausland entstehen.\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_telefonieren_sms_mms_wlan_call_kosten_option.txt\n",
      "\n",
      "Cosine Similarity: 0.7909\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/telefonieren-sms-mms/wlan-call/kosten-option\n",
      "Telekom > Hilfe & Service > Mobilfunk > Telefonieren, > SMS & MMS > WLAN > Call > Kosten\n",
      "\n",
      "Question: Fällt für Option VoLTE und WLAN Call ein monatlicher Grundpreis an?\n",
      "Answer: Nein, die Mobilfunk Tarif-Option \"VoLTE und WLAN Call\" ist kostenlos. Gebühren fallen nur an, wenn Sie über WLAN Call telefonieren oder Nachrichten senden und Ihr Mobilfunk-Tarif das nicht abdeckt.\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_tarife_optionen_ausland_roaming_optionen_pruefen.txt\n",
      "\n",
      "Cosine Similarity: 0.7999\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/tarife-optionen/ausland-roaming/optionen-pruefen\n",
      "Telekom > Hilfe & Service > Mobilfunk > Tarife & Optionen > Ausland & Roaming > Auslandsoption > prüfen\n",
      "\n",
      "Question: Wo kann ich sehen, welche Auslandsoptionen ich gebucht habe?\n",
      "Answer: Sie haben die Möglichkeit, im Kundencenter zu prüfen, welche Auslandsoption Sie aktiviert haben. Gehen Sie bitte wie folgt vor:\n",
      "Klicken Sie imKundencenterauf \"Zum Vertrag\" bzw. \"Vertragsdetails\" des Mobilfunk-Vertrags und scrollen Sie bis \"Meine Zubuchoptionen\". In der Übersicht sehen Sie, ob und welche Option bei Ihrem Vertrag inklusive oder gebucht ist.\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vorteile_sozialtarif.txt\n",
      "\n",
      "Cosine Similarity: 0.7926\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vorteile/sozialtarif\n",
      "Telekom > Hilfe & Service > Vertrag & Rechnung > Telekom > Vorteile > Sozialtarif\n",
      "\n",
      "Question: Was ist der Sozialtarif für den Festnetz-Anschluss?\n",
      "Answer: Mit dem Sozialtarif 1 oder 2 erhalten berechtigte Personen und Menschen mit Beeinträchtigungen, die von der Rundfunkbeitragspflicht befreit sind, bzw. einen Anspruch auf Ermäßigung oder BAföG haben, eine Vergünstigung auf City- und Deutschlandverbindungen, Anrufe zur nationalen Teilnehmerrufnummer 032 (Internet-Telefonie) und Auslandsverbindungen.\n",
      "Benötigte NachweiseFür den Sozialtarif 1 wird entweder ein aktueller Bescheid über die Befreiung oder Ermäßigung auf den Rundfunkbeitrag, eine Kopie des Schwerbehindertenausweises mit Angabe des Grades der Beeinträchtigung sowie des Merkzeichens \"RF\", ein Feststellungsbescheid des Versorgungsamtes oder ein aktueller BAföG-Bescheid eingereicht.\n",
      "Für den Sozialtarif 2 wird eine Kopie des Schwerbehindertenausweises mit mindestens 90 Grad der Behinderung sowie Merkzeichen \"BI\", \"GI\" oder ein Feststellungsbescheid des Versorgungsamtes als Nachweis benötigt.\n",
      "Sozialtarif beantragen oder verlängernWenn Sie den Sozialtarif erstmalig in Auftrag geben möchten, laden Sie bitte zunächst denAuftrag für Sozialtarifherunter und senden Sie uns diesen mit Ihren Nachweisenonlinezu.\n",
      "Zur Verlängerung Ihres Sozialtarifs senden Sie uns einfach online Ihre gültigen Nachweise über diesesKontaktformular.\n",
      "Hinweis: Der Sozialtarif ist bei Vorlage des BAföG-Bescheids für maximal 1 Jahr gültig. Bei allen anderen Voraussetzungen für 3 Jahre, sofern die vorgelegte Bescheinigung nicht kürzer befristet ist. Die gesamte bzw. anteilige Vergünstigung wird nicht auf den Folgemonat übertragen und nicht rückwirkend gewährt.\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/TV/https_www_telekom_de_hilfe_tv_inhalte_wow_magenta_tv_tarife.txt\n",
      "\n",
      "Cosine Similarity: 0.7964\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/tv/inhalte/wow/magenta-tv-tarife\n",
      "Telekom > Hilfe & Service > TV > Inhalte > WOW > Buchen\n",
      "\n",
      "Question: Zu welchen MagentaTV Tarifen kann ich WOW hinzubuchen?\n",
      "Answer: DieWOWOptionen können zu allen MagentaTV Tarifen (ausgenommen MagentaTV Flex) gebucht werden.\n"
     ]
    }
   ],
   "source": [
    "# Template for Generating Alternative Questions\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "# Create a prompt template for generating multiple perspectives of the user's question\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define a pipeline for generating alternative queries\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))  # Split the generated output into individual queries\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    \"\"\"\n",
    "    Returns a unique union of retrieved documents.\n",
    "\n",
    "    This function takes a list of lists of documents, flattens it, and removes duplicates\n",
    "    to ensure each document is unique.\n",
    "\n",
    "    Args:\n",
    "        documents (list of lists): A list where each element is a list of documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique documents.\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists of documents\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Remove duplicates by converting to a set and then back to a list\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Deserialize the documents back into their original form\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Define the retrieval chain, which includes generating queries, retrieving documents, and removing duplicates\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "# Retrieve multiple documents based on the input question\n",
    "multi_query_docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "\n",
    "def format_docs(docs, query_embedding):\n",
    "    \"\"\"\n",
    "    Formats the retrieved documents with their source and cosine similarity score.\n",
    "\n",
    "    This function takes a list of documents and formats them to include the source of each document\n",
    "    and its cosine similarity to the query embedding.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents retrieved from the database.\n",
    "        query_embedding (numpy array): The embedding of the user's query.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the source, similarity score, and content of each document.\n",
    "    \"\"\"\n",
    "    # Initialize a set to track unique sources\n",
    "    unique_sources = set()\n",
    "    formatted_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Retrieve the source of the document from its metadata\n",
    "        source = doc.metadata.get(\"source\")\n",
    "        # Check if the source is unique\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            # Compute the embedding of the document's content\n",
    "            document_embedding = embedding.embed_query(doc.page_content)\n",
    "            # Calculate cosine similarity between the query and document embeddings\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)\n",
    "            # Use a placeholder message if the document content is empty\n",
    "            content = doc.page_content.strip() or \"This document content is empty.\"\n",
    "            # Format the document's source, similarity score, and content\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "\n",
    "    # Join the formatted documents into a single string\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Define a retrieval and generation (RAG) chain for processing the question and context\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} \n",
    "    | prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async def retrieve_and_format_docs(question):\n",
    "    \"\"\"\n",
    "    Asynchronously retrieves and formats documents for the given question.\n",
    "\n",
    "    This function retrieves documents relevant to the user's question and formats them with their\n",
    "    source information and cosine similarity scores.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the answer and formatted documents.\n",
    "    \"\"\"\n",
    "    # Compute the embedding for the user's question\n",
    "    query_embedding = embedding.embed_query(question)\n",
    "    # Format the retrieved documents with their cosine similarity scores\n",
    "    formatted_docs = format_docs(multi_query_docs, query_embedding)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to retrieve an answer using the RAG chain asynchronously\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Fallback in case of TypeError, invoke the RAG chain synchronously\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    # Return the answer and the formatted documents\n",
    "    return answer, formatted_docs\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    The main asynchronous function to run the complete flow.\n",
    "\n",
    "    This function handles the process of generating alternative queries, retrieving and formatting\n",
    "    documents, and printing the final answer along with the source documents.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Retrieve and format documents, then get the answer\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "    # Print the final answer\n",
    "    print(\"Answer:\", answer)\n",
    "     # Generate and print alternative queries\n",
    "    await print_generated_queries(question)\n",
    "    # Print the source documents used for the answer\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "# Execute the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for generating multiple search queries based on a single input query.\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create a chain for generating four related search queries\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Function for Reciprocal Rank Fusion (RRF)\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"\n",
    "    Applies Reciprocal Rank Fusion (RRF) to combine multiple lists of ranked documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - results (list[list]): A list of lists where each inner list contains ranked documents.\n",
    "    - k (int): An optional parameter for the RRF formula, default is 60.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of tuples where each tuple contains a document and its fused score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to store the fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Serialize the document to a string format to use as a key\n",
    "            doc_str = dumps(doc)\n",
    "            # Initialize the document's score if not already present\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the document's score using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort documents based on their fused scores in descending order\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples\n",
    "    return reranked_results\n",
    "\n",
    "# Create a retrieval chain that generates queries, retrieves documents, and applies RRF\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "fusion_docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "# Function to get embeddings for a document's content\n",
    "async def get_document_embeddings(doc):\n",
    "    \"\"\"\n",
    "    Retrieves the embeddings for a document's content asynchronously.\n",
    "    \n",
    "    Parameters:\n",
    "    - doc (Document): The document object whose content embeddings are to be retrieved.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: The embeddings of the document's content.\n",
    "    \"\"\"\n",
    "    return embedding.embed_query(doc.page_content)\n",
    "\n",
    "# Function to format fusion_docs as a readable string with similarity scores\n",
    "async def format_fusion_docs_with_similarity(fusion_docs):\n",
    "    \"\"\"\n",
    "    Formats the fusion documents with their scores and cosine similarity to the question.\n",
    "    \n",
    "    Parameters:\n",
    "    - fusion_docs (list[tuple]): A list of tuples containing documents and their scores.\n",
    "    \n",
    "    Returns:\n",
    "    - str: A formatted string containing each document's source, fusion score, cosine similarity, and content.\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    question_embedding = embedding.embed_query(question)\n",
    "    \n",
    "    for doc, score in fusion_docs:\n",
    "        doc_embedding = await get_document_embeddings(doc)\n",
    "        similarity = cosine_similarity(question_embedding, doc_embedding)\n",
    "        source = doc.metadata.get(\"source\", \"No source\")\n",
    "        content = doc.page_content\n",
    "        formatted_docs.append(f\"Source: {source}\\nFusion Score: {score:.4f}\\nCosine Similarity: {similarity:.4f}\\nContent: {content}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "\n",
    "# Create a chain that uses context and question to generate an answer\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")} \n",
    "    | prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Asynchronous function to retrieve and format documents, then get an answer\n",
    "async def retrieve_and_format_docs(question):\n",
    "    \"\"\"\n",
    "    Retrieves and formats documents, then obtains an answer to the question.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The query for which answers and document formats are required.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing the answer and the formatted documents.\n",
    "    \"\"\"\n",
    "    formatted_docs = await format_fusion_docs_with_similarity(fusion_docs)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to get the answer asynchronously\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Fallback to synchronous invocation if asynchronous fails\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "\n",
    "# Main function to run the sequence of operations\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the entire process: generating queries, retrieving and formatting documents, and getting answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    answer, formatted_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    await print_generated_queries(question)\n",
    "    print(\"\\nSources:\")\n",
    "    print(formatted_docs)  # Print the formatted version of fusion_docs with similarity scores\n",
    "\n",
    "# Execute the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!Decomposition\n",
    "#### Calismadi olmadi maalesef, asnwer sadece 3. sorunun cevabini veriyor, stratch den baska kaynakalara bakip cözüm bulmak lazim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts and chains\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answered in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | model | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer recursion\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | model\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer_decomposition = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer_decomposition)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Back\n",
    "#### cosine similarity eksik sadece calisiyor suan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Transform examples into example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate step-back queries\n",
    "generate_queries_step_back = prompt | model | StrOutputParser()\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Original Question: {question}\")\n",
    "print(f\"Step-Back Question: {step_back_question}\")\n",
    "\n",
    "# Response prompt template\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "def get_retrieved_content(retrieved_documents):\n",
    "    \"\"\"Format retrieved documents as a string with source information.\"\"\"\n",
    "    seen_sources = set()\n",
    "    content_list = []\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            content = (\n",
    "                f\"Source: {source}\\n\"\n",
    "                f\"Content:\\n{doc.page_content}\\n\"\n",
    "                \"------------------------------\\n\"\n",
    "            )\n",
    "            content_list.append(content)\n",
    "    return \"\\n\".join(content_list)\n",
    "\n",
    "def format_retrieved_context(query):\n",
    "    \"\"\"Retrieve and format context for the given query.\"\"\"\n",
    "    # Retrieve documents using the 'invoke' method\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    return get_retrieved_content(retrieved_docs)\n",
    "\n",
    "# Construct the chain to retrieve and generate the response\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": lambda x: format_retrieved_context(x[\"question\"]),\n",
    "        \"step_back_context\": lambda x: format_retrieved_context(x[\"step_back_question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "result = chain.invoke({\"question\": question, \"step_back_question\": step_back_question})\n",
    "\n",
    "# Display the final response\n",
    "print(\"\\nNormal Context:\\n\", format_retrieved_context(question))\n",
    "print(\"\\nStep-Back Context:\\n\", format_retrieved_context(step_back_question))\n",
    "print(\"\\nFinal Answer:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "# This list provides example pairs of input questions and their corresponding step-back questions for model training.\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create a prompt template for examples.\n",
    "# This template formats example messages for the model to learn from.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),  # Input from the user\n",
    "        (\"ai\", \"{output}\"),    # Model's response to the input\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a few-shot prompt template that includes example prompts.\n",
    "# This helps the model understand the context by providing example inputs and outputs.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Define the final prompt template.\n",
    "# This includes system instructions and integrates the few-shot prompt.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),  # Input question from the user\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate step-back queries using the defined prompt.\n",
    "# This involves processing the original question to generate a more general query.\n",
    "generate_queries_step_back = prompt | model | StrOutputParser()\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "\n",
    "# Response prompt template\n",
    "# This template is used to generate the final response based on the retrieved context and the original question.\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "def get_retrieved_content(retrieved_documents):\n",
    "    \"\"\"\n",
    "    Format retrieved documents as a string with source information.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_documents (list): List of documents retrieved based on the query.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string containing source and content of retrieved documents.\n",
    "    \"\"\"\n",
    "    seen_sources = set()  # Track unique sources\n",
    "    content_list = []      # List to accumulate formatted content\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown')  # Get source of the document\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            content = (\n",
    "                f\"Source: {source}\\n\"\n",
    "                f\"Content:\\n{doc.page_content}\\n\"\n",
    "                \"------------------------------\\n\"\n",
    "            )\n",
    "            content_list.append(content)\n",
    "    return \"\\n\".join(content_list)\n",
    "\n",
    "def format_retrieved_context(query):\n",
    "    \"\"\"\n",
    "    Retrieve and format context for the given query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query for which context needs to be retrieved.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string containing context relevant to the query.\n",
    "    \"\"\"\n",
    "    # Retrieve documents using the 'invoke' method\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    return get_retrieved_content(retrieved_docs)\n",
    "\n",
    "# Construct the chain to retrieve and generate the response.\n",
    "# This chain combines context retrieval and response generation.\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": lambda x: format_retrieved_context(x[\"question\"]),\n",
    "        \"step_back_context\": lambda x: format_retrieved_context(x[\"step_back_question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain to get the final response.\n",
    "result = chain.invoke({\"question\": question, \"step_back_question\": step_back_question})\n",
    "\n",
    "# Display the final response along with normal and step-back contexts.\n",
    "print(\"Answer:\", result)\n",
    "print(f\"\\n\\nOriginal Question: {question}\")\n",
    "print(f\"\\nStep-Back Question: {step_back_question}\")\n",
    "print(\"\\nNormal Context:\\n\", format_retrieved_context(question))\n",
    "print(\"\\nStep-Back Context:\\n\", format_retrieved_context(step_back_question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE document generation\n",
    "template = \"\"\"You are creating professional and customer-focused web page content and texts for a major telecommunications provider like Telekom.de. \n",
    "Your content is very brief, very clear, and informative. Please write a text for the following question\n",
    "Question: {question}\n",
    "text:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run HyDE generation\n",
    "try:\n",
    "    hyde_output = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "    print(f\"HyDE hypothetical answer:\\n{hyde_output.strip()}\\n\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating documents for retrieval: {e}\")\n",
    "    raise\n",
    "\n",
    "# Retrieve documents\n",
    "try:\n",
    "    retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "    retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Print retrieved documents, deduplicated\n",
    "    seen_sources = set()\n",
    "    print(\"Retrieved sources:\")\n",
    "    for doc in retrieved_docs:\n",
    "        source = doc.metadata.get('source', 'Unknown Source')\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            print(f\"\\nDocument Source: {source}\")\n",
    "            print(f\"Document Content:\\n{doc.page_content.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "try:\n",
    "    final_answer = final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "    print(f\"\\nFinal RAG Answer:\\n{final_answer.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating final RAG answer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE Document Generation\n",
    "# This section is responsible for creating professional and customer-focused content\n",
    "# for a major telecommunications provider based on a given question.\n",
    "\n",
    "# Define a template for generating content.\n",
    "# The template specifies that the content should be brief, clear, and informative.\n",
    "template = \"\"\"You are creating professional and customer-focused web page content and texts for a major telecommunications provider like Telekom.de. \n",
    "Your content is very brief, very clear, and informative. Please write a text for the following question:\n",
    "Question: {question}\n",
    "text:\"\"\"\n",
    "\n",
    "# Create a prompt template using the defined template.\n",
    "# This template will be used to generate content for a given question.\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define a chain to generate documents for retrieval.\n",
    "# This chain uses the prompt template, a language model, and an output parser.\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run HyDE document generation to produce content for the given question.\n",
    "# The try-except block handles potential errors during document generation.\n",
    "try:\n",
    "    hyde_output = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "    print(f\"HyDE hypothetical context:\\n{hyde_output.strip()}\\n\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating documents for retrieval: {e}\")\n",
    "    raise\n",
    "\n",
    "# Retrieve Documents\n",
    "# This section retrieves documents based on the generated content and prints them.\n",
    "\n",
    "# Define a chain to retrieve documents using the generated content.\n",
    "# The chain combines the document generation process with a retriever.\n",
    "try:\n",
    "    retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "    retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Print retrieved documents and deduplicate them based on source information.\n",
    "    seen_sources = set()\n",
    "    print(\"Retrieved sources:\")\n",
    "    for doc in retrieved_docs:\n",
    "        source = doc.metadata.get('source', 'Unknown Source')  # Get the source of the document\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            print(f\"\\nSource file: {source}\")\n",
    "            print(f\"Document Content:\\n{doc.page_content.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define a chain to generate the final answer using the RAG process.\n",
    "# The chain combines the prompt template, a language model, and an output parser.\n",
    "final_rag_chain = (\n",
    "    prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Generate the final answer using the RAG process.\n",
    "# The try-except block handles potential errors during the final answer generation.\n",
    "try:\n",
    "    final_answer = final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "    print(f\"\\nFinal Answer:\\n{final_answer.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating final RAG answer: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
