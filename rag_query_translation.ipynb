{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG\n",
    "### Routing mantigi calismadi retriever'i sadece ilk seferde filtreliyor, her seferinde chroma ya gömüyü ve hepsini ariyor.\n",
    "### chain invoke etmeden retriever cagrildigi yerde filtreleme olabilir. Bunu dene!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Magenta zuhause nedir, hangi amacla bu hizmeti sunuyor telekom?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the rag data\n",
    "data_directory = \"/Users/taha/Desktop/rag/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, FewShotChatMessagePromptTemplate, PromptTemplate\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "\n",
    "#from chromadb import Chroma\n",
    "#from chromadb.documents import Document\n",
    "#from chromadb.loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# Load API Keys from environment variables\n",
    "load_dotenv()  # Load environment variables from a .env file\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")  # This key is loaded but not used in the code\n",
    "\n",
    "# Initialize the chat model and embedding model\n",
    "# ChatOpenAI is used to interact with the OpenAI GPT model, and OpenAIEmbeddings is used for generating embeddings for documents\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Routing\n",
    "\n",
    "#### Routing ile alakali hic emin olamadim nerde nasil kullanacagim. Normalde retriever'in belli klasörden datayi almasini istiyorum soruya göre\n",
    "#### ama bu olmuyor cünkü her defasinda vector database eski datanin üzerine yüklüyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user question to the most relevant datacategory.\"\"\"\n",
    "\n",
    "    datacategory: Literal[\"vertrag_rechnung_ihre_daten_kundencenter_login-daten_rechnung_lieferstatus\", \n",
    "                          \"hilfe_stoerungen_stoerungen_selbst_beheben_melden_status_verfolgen\",\n",
    "                          \"mobilfunk_tarife_optionen_mobiles-internet_mailbox_esim_sim-karten\",\n",
    "                          \"internet_telefonie:_ausbau,_sicherheit,_einstellungen,_bauherren,_glasfaser_und_wlan\",\n",
    "                          \"tv_magentatv_streaming-dienste_magentatv_jugendschutz_pins\",\n",
    "                          \"magentains_kombi-pakete_mit_magentains_vorteil_und_treuebonus\",\n",
    "                          \"apps_dienste_e-mail_magenta_apps_voicemail_app_mobilityconnect\",\n",
    "                          \"geraete_zubehoer_anleitungen_fuer_smartphones_tablets_telefone_router_receiver\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datacategory would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "structured_model = model.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing user questions to the appropriate data category.\n",
    "\n",
    "Based on the help category the question is referring to, route it to the relevant data category. \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_model\n",
    "\n",
    "\n",
    "category = router.invoke({\"question\": question})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    # Kategorileri ve ilgili alt dizinleri bir sözlükte tanımlayın\n",
    "    category_map = {\n",
    "        \"vertrag_rechnung_ihre_daten_kundencenter_login-daten_rechnung_lieferstatus\": \"Vertrag & Rechnung\",\n",
    "        \"hilfe_stoerungen_stoerungen_selbst_beheben_melden_status_verfolgen\": \"Hilfe bei Störungen\",\n",
    "        \"mobilfunk_tarife_optionen_mobiles-internet_mailbox_esim_sim-karten\": \"Mobilfunk\",\n",
    "        \"internet_telefonie:_ausbau,_sicherheit,_einstellungen,_bauherren,_glasfaser_und_wlan\": \"Internet & Telefonie\",\n",
    "        \"tv_magentatv_streaming-dienste_magentatv_jugendschutz_pins\": \"TV\",\n",
    "        \"magentains_kombi-pakete_mit_magentains_vorteil_und_treuebonus\": \"MagentaEINS\",\n",
    "        \"apps_dienste_e-mail_magenta_apps_voicemail_app_mobilityconnect\": \"Apps & Dienste\",\n",
    "        \"geraete_zubehoer_anleitungen_fuer_smartphones_tablets_telefone_router_receiver\": \"Geräte & Zubehör\"\n",
    "    }\n",
    "    \n",
    "    # Datacategory'yi küçült ve sözlükte ara, yoksa \"Others\" döner\n",
    "    return category_map.get(result.datacategory.lower(), \"Others\")\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MagentaEINS\n",
      "/Users/taha/Desktop/rag/data/MagentaEINS\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"/Users/taha/Desktop/rag/data\"\n",
    "\n",
    "sub_directory = full_chain.invoke({\"question\": question})\n",
    "print(sub_directory)\n",
    "\n",
    "specific_directory = os.path.join(data_directory, sub_directory)\n",
    "print(specific_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deneme icin burada yeni bir yapi kuruyorum, metadata olarak klasör isimlerini ekliyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while initializing the vector store: Expected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['args', 'kwargs'])\n",
      "Please see https://docs.trychroma.com/guides/embeddings for details of the EmbeddingFunction interface.\n",
      "Please note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/deployment/migration#migration-to-0.4.16---november-7,-2023 \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['args', 'kwargs'])\nPlease see https://docs.trychroma.com/guides/embeddings for details of the EmbeddingFunction interface.\nPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/deployment/migration#migration-to-0.4.16---november-7,-2023 \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Initialize the vector store and document list\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m collection, docs \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_retriever_with_filter\u001b[39m(collection, filter_directory: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    Sets up a retriever that filters documents based on the specified directory metadata.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m        results: The results of the query, filtered by directory metadata.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 27\u001b[0m, in \u001b[0;36minitialize_vectorstore\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create a Chroma collection from the loaded documents and embeddings\u001b[39;00m\n\u001b[1;32m     26\u001b[0m client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mPersistentClient(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Replace with your Chroma client path\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtelekom_rag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Add documents to the collection\u001b[39;00m\n\u001b[1;32m     30\u001b[0m collection\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m     31\u001b[0m     documents\u001b[38;5;241m=\u001b[39m[doc\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs],  \u001b[38;5;66;03m# Assuming docs have a `text` attribute\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39m[doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs],\n\u001b[1;32m     33\u001b[0m     ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docs))]  \u001b[38;5;66;03m# Generate unique ids for documents\u001b[39;00m\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/rag/venv/lib/python3.12/site-packages/chromadb/api/client.py:125\u001b[0m, in \u001b[0;36mClient.create_collection\u001b[0;34m(self, name, configuration, metadata, embedding_function, data_loader, get_or_create)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_collection\u001b[39m(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     get_or_create: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[1;32m    117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[1;32m    118\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    119\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfiguration,\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/rag/venv/lib/python3.12/site-packages/chromadb/api/models/CollectionCommon.py:92\u001b[0m, in \u001b[0;36mCollectionCommon.__init__\u001b[0;34m(self, client, model, embedding_function, data_loader)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Check to make sure the embedding function has the right signature, as defined by the EmbeddingFunction protocol\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mvalidate_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;241m=\u001b[39m data_loader\n",
      "File \u001b[0;32m~/Desktop/rag/venv/lib/python3.12/site-packages/chromadb/api/types.py:231\u001b[0m, in \u001b[0;36mvalidate_embedding_function\u001b[0;34m(embedding_function)\u001b[0m\n\u001b[1;32m    228\u001b[0m protocol_signature \u001b[38;5;241m=\u001b[39m signature(EmbeddingFunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_signature \u001b[38;5;241m==\u001b[39m protocol_signature:\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected EmbeddingFunction.__call__ to have the following signature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotocol_signature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_signature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see https://docs.trychroma.com/guides/embeddings for details of the EmbeddingFunction interface.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/deployment/migration#migration-to-0.4.16---november-7,-2023 \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['args', 'kwargs'])\nPlease see https://docs.trychroma.com/guides/embeddings for details of the EmbeddingFunction interface.\nPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/deployment/migration#migration-to-0.4.16---november-7,-2023 \n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import chromadb  # Ensure you're importing the necessary module\n",
    "\n",
    "def initialize_vectorstore(directory: str):\n",
    "    \"\"\"\n",
    "    Initializes a vector store from the documents found in the specified directory,\n",
    "    including directory metadata for filtering.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): The path to the directory containing text files to be processed.\n",
    "\n",
    "    Returns:\n",
    "        collection (Collection): A Chroma collection object containing the embeddings of the documents.\n",
    "        docs (List[Document]): A list of Document objects loaded from the specified directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load documents from the specified directory using DirectoryLoader\n",
    "        loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "        docs = loader.load()  # Load all text documents matching the pattern\n",
    "\n",
    "        # Add directory path as metadata to each document\n",
    "        for doc in docs:\n",
    "            doc.metadata['directory'] = directory\n",
    "\n",
    "        # Create a Chroma collection from the loaded documents and embeddings\n",
    "        client = chromadb.PersistentClient(path=\"vector_data\")  # Replace with your Chroma client path\n",
    "        collection = client.create_collection(name=\"telekom_rag\", embedding_function=embedding)\n",
    "\n",
    "        # Add documents to the collection\n",
    "        collection.add(\n",
    "            documents=[doc.text for doc in docs],  # Assuming docs have a `text` attribute\n",
    "            metadatas=[doc.metadata for doc in docs],\n",
    "            ids=[str(i) for i in range(len(docs))]  # Generate unique ids for documents\n",
    "        )\n",
    "        \n",
    "        return collection, docs\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while initializing the vector store: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize the vector store and document list\n",
    "\n",
    "collection, docs = initialize_vectorstore(data_directory)\n",
    "\n",
    "def get_retriever_with_filter(collection, filter_directory: str):\n",
    "    \"\"\"\n",
    "    Sets up a retriever that filters documents based on the specified directory metadata.\n",
    "\n",
    "    Parameters:\n",
    "        collection (Collection): The Chroma collection object containing the embeddings of the documents.\n",
    "        specific_directory (str): The directory path to filter documents by.\n",
    "\n",
    "    Returns:\n",
    "        results: The results of the query, filtered by directory metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define a retriever with a filter based on the directory metadata\n",
    "        results = collection.query(\n",
    "            where={\"directory\": {\"$eq\": filter_directory}}  # Use the $eq operator to match exact value\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while setting up the retriever: {e}\")\n",
    "        raise\n",
    "\n",
    "retriever = get_retriever_with_filter(collection, specific_directory)\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef initialize_vectorstore(directory):\\n    \"\"\"\\n    Initializes a vector store from the documents found in the specified directory.\\n    \\n    This function performs the following steps:\\n    1. Loads text documents from the given directory using a DirectoryLoader.\\n    2. Creates embeddings for the loaded documents using a predefined embedding model.\\n    3. Initializes a Chroma vector store with these embeddings.\\n    \\n    Parameters:\\n        directory (str): The path to the directory containing text files to be processed.\\n        \\n    Returns:\\n        vectorstore (Chroma): A Chroma vector store object containing the embeddings of the documents.\\n        docs (List[Document]): A list of Document objects loaded from the specified directory.\\n        \\n    \"\"\"\\n    \\n    # Load documents from the specified directory using DirectoryLoader\\n    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\\n    docs = loader.load()  # Load all text documents matching the pattern\\n    \\n    # Create a Chroma vector store from the loaded documents and embeddings\\n    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\\n    \\n    return vectorstore, docs\\n\\n# Initialize the vector store and document list\\nvectorstore, docs = initialize_vectorstore(data_directory)\\n\\n# Set up the retriever using the vector store\\nretriever = vectorstore.as_retriever()\\n\\n\\n\\n# Initialize the retriever\\ndef initialize_vectorstore(main_directory, additional_directory):\\n    \"\"\"\\n    Initializes a vector store from the documents found in the specified directories.\\n    \\n    This function performs the following steps:\\n    1. Loads text documents from the given main directory and an additional directory using DirectoryLoader.\\n    2. Creates embeddings for the loaded documents using a predefined embedding model.\\n    3. Initializes a Chroma vector store with these embeddings.\\n    \\n    Parameters:\\n        main_directory (str): The path to the main directory containing text files to be processed.\\n        additional_directory (str): The path to the additional directory containing extra text files to be included.\\n        \\n    Returns:\\n        vectorstore (Chroma): A Chroma vector store object containing the embeddings of the documents.\\n        docs (List[Document]): A list of Document objects loaded from the specified directories.\\n        \\n    \"\"\"\\n    \\n    # Load documents from the main directory using DirectoryLoader\\n    loader_main = DirectoryLoader(main_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\\n    docs_main = loader_main.load()  # Load all text documents from the main directory\\n\\n    # Load documents from the additional directory using DirectoryLoader\\n    loader_additional = DirectoryLoader(additional_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\\n    docs_additional = loader_additional.load()  # Load all text documents from the additional directory\\n\\n    # Combine the documents from both directories\\n    all_docs = docs_main + docs_additional\\n    \\n    # Create a Chroma vector store from the loaded documents and embeddings\\n    vectorstore = Chroma.from_documents(documents=all_docs, embedding=embedding)\\n    \\n    return vectorstore, all_docs\\n\\nadditional_directory = \"data/Others\"  # Additional directory path\\n\\n# Initialize the vector store and document list with both directories\\nvectorstore, docs = initialize_vectorstore(data_directory, additional_directory)\\n\\n# Set up the retriever using the vector store\\nretriever = vectorstore.as_retriever()\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def initialize_vectorstore(directory):\n",
    "    \"\"\"\n",
    "    Initializes a vector store from the documents found in the specified directory.\n",
    "    \n",
    "    This function performs the following steps:\n",
    "    1. Loads text documents from the given directory using a DirectoryLoader.\n",
    "    2. Creates embeddings for the loaded documents using a predefined embedding model.\n",
    "    3. Initializes a Chroma vector store with these embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): The path to the directory containing text files to be processed.\n",
    "        \n",
    "    Returns:\n",
    "        vectorstore (Chroma): A Chroma vector store object containing the embeddings of the documents.\n",
    "        docs (List[Document]): A list of Document objects loaded from the specified directory.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Load documents from the specified directory using DirectoryLoader\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs = loader.load()  # Load all text documents matching the pattern\n",
    "    \n",
    "    # Create a Chroma vector store from the loaded documents and embeddings\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "    \n",
    "    return vectorstore, docs\n",
    "\n",
    "# Initialize the vector store and document list\n",
    "vectorstore, docs = initialize_vectorstore(data_directory)\n",
    "\n",
    "# Set up the retriever using the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the retriever\n",
    "def initialize_vectorstore(main_directory, additional_directory):\n",
    "    \"\"\"\n",
    "    Initializes a vector store from the documents found in the specified directories.\n",
    "    \n",
    "    This function performs the following steps:\n",
    "    1. Loads text documents from the given main directory and an additional directory using DirectoryLoader.\n",
    "    2. Creates embeddings for the loaded documents using a predefined embedding model.\n",
    "    3. Initializes a Chroma vector store with these embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        main_directory (str): The path to the main directory containing text files to be processed.\n",
    "        additional_directory (str): The path to the additional directory containing extra text files to be included.\n",
    "        \n",
    "    Returns:\n",
    "        vectorstore (Chroma): A Chroma vector store object containing the embeddings of the documents.\n",
    "        docs (List[Document]): A list of Document objects loaded from the specified directories.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Load documents from the main directory using DirectoryLoader\n",
    "    loader_main = DirectoryLoader(main_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs_main = loader_main.load()  # Load all text documents from the main directory\n",
    "\n",
    "    # Load documents from the additional directory using DirectoryLoader\n",
    "    loader_additional = DirectoryLoader(additional_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs_additional = loader_additional.load()  # Load all text documents from the additional directory\n",
    "\n",
    "    # Combine the documents from both directories\n",
    "    all_docs = docs_main + docs_additional\n",
    "    \n",
    "    # Create a Chroma vector store from the loaded documents and embeddings\n",
    "    vectorstore = Chroma.from_documents(documents=all_docs, embedding=embedding)\n",
    "    \n",
    "    return vectorstore, all_docs\n",
    "\n",
    "additional_directory = \"data/Others\"  # Additional directory path\n",
    "\n",
    "# Initialize the vector store and document list with both directories\n",
    "vectorstore, docs = initialize_vectorstore(data_directory, additional_directory)\n",
    "\n",
    "# Set up the retriever using the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for generating an answer based on context and a question\n",
    "telekom_template = \"\"\"You are an assistant for question-answering tasks for telekom.de help, providing answers to Telekom customers or potential customers. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer or if the provided documents do not contain relevant information, simply say that unfortunately, you cannot assist with this question and please visit telekom.de/hilfe for further assistance. \n",
    "Use up to four sentences and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_telekom = ChatPromptTemplate.from_template(telekom_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - vec1 (np.ndarray): The first vector.\n",
    "    - vec2 (np.ndarray): The second vector.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The cosine similarity between vec1 and vec2.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2) if (norm_vec1 and norm_vec2) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous function to print generated queries\n",
    "async def print_generated_queries(question):\n",
    "    \"\"\"\n",
    "    Generates and prints multiple search queries related to the input question.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The input query for which related search queries are generated.\n",
    "    \"\"\"\n",
    "    queries = generate_queries.invoke({\"question\": question})\n",
    "    print(\"\\nGenerated Questions:\")\n",
    "    for q in queries:\n",
    "        print(f\"{q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Magenta Zuhause, Telekom'un sunduğu bir hizmettir. Bu hizmet, evinizdeki internet bağlantısını ve TV hizmetlerini kapsar. Bu hizmeti satın aldığınızda, Telekom size kurulumda yardımcı olur ve ev ağınızdaki teknik sorunları modern analiz teknolojileri ile önceden tespit edebilir. Bu hizmet aynı zamanda Magenta Zuhause TV'yi de içerir.\n",
      "\n",
      "Generated Questions:\n",
      "1. Nedir Magenta zuhause ve telekom hangi amaçla bu hizmeti sunuyor?\n",
      "2. Telekom'un sunduğu Magenta zuhause hizmeti ne işe yarar?\n",
      "3. Magenta zuhause hizmeti nedir ve telekom bu hizmeti hangi amaçla sunuyor?\n",
      "4. Hangi amaçla telekom Magenta zuhause hizmetini sunmaktadır?\n",
      "5. Magenta zuhause'nin amacı nedir ve telekom bu hizmeti neden sunmaktadır?\n",
      "\n",
      "Sources:\n",
      "Source document: /Users/taha/Desktop/rag/data/Others/https_www_telekom_de_magenta_tv_online_tv.txt\n",
      "\n",
      "Cosine Similarity: 0.8476\n",
      "\n",
      "Source URL: https://www.telekom.de/magenta-tv/online-tv\n",
      "\n",
      "Question: Sie haben noch kein MagentaTV?\n",
      "Answer: Ein großartiges Fernsehvergnügen zu Hause oder unterwegs – das genießen Sie mit der Telekom und Online-TV. Nutzen Sie dafür entweder unserenTV-Receiver, dieMagentaTV Oneoder dieMagentaTV App. Profitieren Sie von zahlreichen komfortablen Funktionen, die Ihr Online-TV-Erlebnis besonders machen:\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/Others/https_www_telekom_de_zubuchoptionen_magenta_tv.txt\n",
      "\n",
      "Cosine Similarity: 0.8525\n",
      "\n",
      "Source URL: https://www.telekom.de/zubuchoptionen/magenta-tv\n",
      "\n",
      "Question: Welchen Telekom Vertrag haben Sie?\n",
      "Answer: MagentaTV mit Telekom Internet\n",
      "Telekom Internet Tarif\n",
      "MagentaTV ohne Telekom Internet\n",
      "\n",
      "Source document: /Users/taha/Desktop/rag/data/Hilfe bei Störungen/youtube_Kennst Du...unseren proaktiven Heimnetzservice？ I Telekom.txt\n",
      "\n",
      "Cosine Similarity: 0.8591\n",
      "\n",
      "Question:\n",
      "Können Sie mehr Informationen über den zusätzlichen Service geben, den ich erhalte, wenn ich Magenta zu Hause oder Magenta zu Hause TV kaufe?\n",
      "\n",
      "Answer:\n",
      "Hallo, ich bin Florian von der Telekom. In diesem Format präsentieren wir unseren Service. Ab sofort bieten wir mehr Service beim Kauf von Magenta Zuhause oder Magenta Zuhause TV. Wir helfen bei der Einrichtung und können mit moderner Analysetechnik technische Probleme in Ihrem Heimnetzwerk erkennen, bevor sie auftreten. Bei Auffälligkeiten melden sich unsere Experten sofort. Innerhalb von 30 Tagen nach dem Kauf können Sie sich auch bei technischen Problemen mit Ihren Geräten an uns wenden - diese müssen nicht von der Telekom stammen. Ob Ihr Kochautomat keine Rezepte mehr herunterlädt oder Ihre Kameras kein Bild mehr anzeigen - unsere Experten sind nur einen Anruf entfernt, um Ihnen zu helfen. Dieser Service ist jetzt automatisch beim Kauf von Magenta Zuhause oder Magenta Zuhause TV enthalten.\n"
     ]
    }
   ],
   "source": [
    "# Template for Generating Alternative Questions\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "# Create a prompt template for generating multiple perspectives of the user's question\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define a pipeline for generating alternative queries\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))  # Split the generated output into individual queries\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    \"\"\"\n",
    "    Returns a unique union of retrieved documents.\n",
    "\n",
    "    This function takes a list of lists of documents, flattens it, and removes duplicates\n",
    "    to ensure each document is unique.\n",
    "\n",
    "    Args:\n",
    "        documents (list of lists): A list where each element is a list of documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique documents.\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists of documents\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Remove duplicates by converting to a set and then back to a list\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Deserialize the documents back into their original form\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Define the retrieval chain, which includes generating queries, retrieving documents, and removing duplicates\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "# Retrieve multiple documents based on the input question\n",
    "multi_query_docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "\n",
    "def format_docs(docs, query_embedding):\n",
    "    \"\"\"\n",
    "    Formats the retrieved documents with their source and cosine similarity score.\n",
    "\n",
    "    This function takes a list of documents and formats them to include the source of each document\n",
    "    and its cosine similarity to the query embedding.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents retrieved from the database.\n",
    "        query_embedding (numpy array): The embedding of the user's query.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the source, similarity score, and content of each document.\n",
    "    \"\"\"\n",
    "    # Initialize a set to track unique sources\n",
    "    unique_sources = set()\n",
    "    formatted_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Retrieve the source of the document from its metadata\n",
    "        source = doc.metadata.get(\"source\")\n",
    "        # Check if the source is unique\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            # Compute the embedding of the document's content\n",
    "            document_embedding = embedding.embed_query(doc.page_content)\n",
    "            # Calculate cosine similarity between the query and document embeddings\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)\n",
    "            # Use a placeholder message if the document content is empty\n",
    "            content = doc.page_content.strip() or \"This document content is empty.\"\n",
    "            # Format the document's source, similarity score, and content\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "\n",
    "    # Join the formatted documents into a single string\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Define a retrieval and generation (RAG) chain for processing the question and context\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} \n",
    "    | prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async def retrieve_and_format_docs(question):\n",
    "    \"\"\"\n",
    "    Asynchronously retrieves and formats documents for the given question.\n",
    "\n",
    "    This function retrieves documents relevant to the user's question and formats them with their\n",
    "    source information and cosine similarity scores.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the answer and formatted documents.\n",
    "    \"\"\"\n",
    "    # Compute the embedding for the user's question\n",
    "    query_embedding = embedding.embed_query(question)\n",
    "    # Format the retrieved documents with their cosine similarity scores\n",
    "    formatted_docs = format_docs(multi_query_docs, query_embedding)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to retrieve an answer using the RAG chain asynchronously\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Fallback in case of TypeError, invoke the RAG chain synchronously\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    # Return the answer and the formatted documents\n",
    "    return answer, formatted_docs\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    The main asynchronous function to run the complete flow.\n",
    "\n",
    "    This function handles the process of generating alternative queries, retrieving and formatting\n",
    "    documents, and printing the final answer along with the source documents.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Retrieve and format documents, then get the answer\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "    # Print the final answer\n",
    "    print(\"Answer:\", answer)\n",
    "     # Generate and print alternative queries\n",
    "    await print_generated_queries(question)\n",
    "    # Print the source documents used for the answer\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "# Execute the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for generating multiple search queries based on a single input query.\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create a chain for generating four related search queries\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Function for Reciprocal Rank Fusion (RRF)\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"\n",
    "    Applies Reciprocal Rank Fusion (RRF) to combine multiple lists of ranked documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - results (list[list]): A list of lists where each inner list contains ranked documents.\n",
    "    - k (int): An optional parameter for the RRF formula, default is 60.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of tuples where each tuple contains a document and its fused score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to store the fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Serialize the document to a string format to use as a key\n",
    "            doc_str = dumps(doc)\n",
    "            # Initialize the document's score if not already present\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the document's score using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort documents based on their fused scores in descending order\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples\n",
    "    return reranked_results\n",
    "\n",
    "# Create a retrieval chain that generates queries, retrieves documents, and applies RRF\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "fusion_docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "# Function to get embeddings for a document's content\n",
    "async def get_document_embeddings(doc):\n",
    "    \"\"\"\n",
    "    Retrieves the embeddings for a document's content asynchronously.\n",
    "    \n",
    "    Parameters:\n",
    "    - doc (Document): The document object whose content embeddings are to be retrieved.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: The embeddings of the document's content.\n",
    "    \"\"\"\n",
    "    return embedding.embed_query(doc.page_content)\n",
    "\n",
    "# Function to format fusion_docs as a readable string with similarity scores\n",
    "async def format_fusion_docs_with_similarity(fusion_docs):\n",
    "    \"\"\"\n",
    "    Formats the fusion documents with their scores and cosine similarity to the question.\n",
    "    \n",
    "    Parameters:\n",
    "    - fusion_docs (list[tuple]): A list of tuples containing documents and their scores.\n",
    "    \n",
    "    Returns:\n",
    "    - str: A formatted string containing each document's source, fusion score, cosine similarity, and content.\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    question_embedding = embedding.embed_query(question)\n",
    "    \n",
    "    for doc, score in fusion_docs:\n",
    "        doc_embedding = await get_document_embeddings(doc)\n",
    "        similarity = cosine_similarity(question_embedding, doc_embedding)\n",
    "        source = doc.metadata.get(\"source\", \"No source\")\n",
    "        content = doc.page_content\n",
    "        formatted_docs.append(f\"Source: {source}\\nFusion Score: {score:.4f}\\nCosine Similarity: {similarity:.4f}\\nContent: {content}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "\n",
    "# Create a chain that uses context and question to generate an answer\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")} \n",
    "    | prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Asynchronous function to retrieve and format documents, then get an answer\n",
    "async def retrieve_and_format_docs(question):\n",
    "    \"\"\"\n",
    "    Retrieves and formats documents, then obtains an answer to the question.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The query for which answers and document formats are required.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing the answer and the formatted documents.\n",
    "    \"\"\"\n",
    "    formatted_docs = await format_fusion_docs_with_similarity(fusion_docs)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to get the answer asynchronously\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Fallback to synchronous invocation if asynchronous fails\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "\n",
    "# Main function to run the sequence of operations\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the entire process: generating queries, retrieving and formatting documents, and getting answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    answer, formatted_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    await print_generated_queries(question)\n",
    "    print(\"\\nSources:\")\n",
    "    print(formatted_docs)  # Print the formatted version of fusion_docs with similarity scores\n",
    "\n",
    "# Execute the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!Decomposition\n",
    "#### Calismadi olmadi maalesef, asnwer sadece 3. sorunun cevabini veriyor, stratch den baska kaynakalara bakip cözüm bulmak lazim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts and chains\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answered in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | model | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer recursion\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | model\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer_decomposition = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer_decomposition)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Back\n",
    "#### cosine similarity eksik sadece calisiyor suan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: Telekom sprachbox nedir, islevi nedir?\n",
      "Step-Back Question: What is Telekom Sprachbox and what does it do?\n",
      "\n",
      "Normal Context:\n",
      " Source: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_mailbox_mobilbox_aktivieren.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/mailbox/mobilbox/aktivieren\n",
      "Telekom > Hilfe & Service > Mobilfunk > Mailbox > Mobilbox > Aktivieren\n",
      "\n",
      "Question: Wie kann ich meine Mobilbox aktivieren?\n",
      "Answer: Um Ihre Mobilbox zu aktivieren, haben Sie zwei Möglichkeiten:\n",
      "Die Tastaturbelegung für die Aktivierung Ihrer Mobilbox funktioniert im In- und Ausland.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Source: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_mailbox_mobilbox_abrufen.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/mailbox/mobilbox/abrufen\n",
      "Telekom > Hilfe & Service > Mobilfunk > Mailbox > Mobilbox > Abrufen\n",
      "\n",
      "Question: Wie kann ich meine Telekom Mobilbox abrufen?\n",
      "Answer: Die Handy-Vorwahl (z. B. 0171) Ihrer Mobilbox.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Source: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_mailbox_mobilbox_pro_deaktivieren.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/mailbox/mobilbox-pro/deaktivieren\n",
      "Telekom > Hilfe & Service > Mobilfunk > Mailbox > Mobilbox > Pro > Deaktivieren\n",
      "\n",
      "Question: Kann ich die Mobilbox Pro wieder deaktivieren und auf die normale Mobilbox umsteigen?\n",
      "Answer: Rufen Sie bitte die kostenfreie Mobilfunk-Hotline 0800 33 02202 an, um sich die klassische Mobilbox aktivieren zu lassen.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Source: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_mailbox_mobilbox_deaktivieren.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/mailbox/mobilbox/deaktivieren\n",
      "Telekom > Hilfe & Service > Mobilfunk > Mailbox > Mobilbox > Deaktivieren\n",
      "\n",
      "Question: Wie kann ich meine Mobilbox deaktivieren?\n",
      "Answer: Um Ihre Mobilbox zu deaktivieren, haben Sie zwei Möglichkeiten:\n",
      "Die Tastaturbelegung für die Aktivierung Ihrer Mobilbox funktioniert im In- und Ausland.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Step-Back Context:\n",
      " Source: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_mailbox_mobilbox_aktivieren.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/mailbox/mobilbox/aktivieren\n",
      "Telekom > Hilfe & Service > Mobilfunk > Mailbox > Mobilbox > Aktivieren\n",
      "\n",
      "Question: Wie kann ich meine Mobilbox aktivieren?\n",
      "Answer: Um Ihre Mobilbox zu aktivieren, haben Sie zwei Möglichkeiten:\n",
      "Die Tastaturbelegung für die Aktivierung Ihrer Mobilbox funktioniert im In- und Ausland.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Source: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_mailbox_mobilbox_pro_nutzung_smartphones.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/mailbox/mobilbox-pro/nutzung-smartphones\n",
      "Telekom > Hilfe & Service > Mobilfunk > Mailbox > Mobilbox > Pro > Smartphones\n",
      "\n",
      "Question: Mit welchen Smartphones kann ich die Mobilbox Pro nutzen?\n",
      "Answer: Auf allen Smartphones, die aktuell von der Telekom vertrieben werden (Ausnahme: Apple-Geräte) ist Mobilbox Pro bereits für Sie eingerichtet.\n",
      "Auf Android Geräten finden Sie Ihre Mobilbox Pro Nachrichten in der Voicemail App. Ist Voicemail auf Ihrem Android Smartphone noch nicht vorinstalliert, können Sie dieApp aus dem Google Play Store herunterladen.\n",
      "Aktivieren Sie den Service, indem Sie eine kostenlose SMS mit dem Text \"MBP\" an die Kurzwahl 3011 senden.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Source: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_mailbox_mobilbox_begruessung_wiederherstellen.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/mailbox/mobilbox/begruessung-wiederherstellen\n",
      "Telekom > Hilfe & Service > Mobilfunk > Mailbox > Mobilbox > Begrüßungsansage > wiederherstellen\n",
      "\n",
      "Question: Wie kann ich bei meiner Mobilbox die Standard-Begrüßung mit Rufnummer wiederherstellen?\n",
      "Answer: Für die Wiederherstellung der Standard-Begrüßung rufen Sie Ihre Mobilbox über die3311auf, warten den Begrüßungstext ab bzw. wechseln über die Eingabe der Zahl \"3\" ins Hauptmenü. Folgen Sie anschließend den Anweisungen.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Source: /Users/taha/Desktop/rag/data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_mailbox_mobilbox_abrufen.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/hilfe/mobilfunk/mailbox/mobilbox/abrufen\n",
      "Telekom > Hilfe & Service > Mobilfunk > Mailbox > Mobilbox > Abrufen\n",
      "\n",
      "Question: Wie kann ich meine Telekom Mobilbox abrufen?\n",
      "Answer: Die Handy-Vorwahl (z. B. 0171) Ihrer Mobilbox.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Final Answer:\n",
      " Telekom sprachbox, aynı zamanda Mobilbox olarak da bilinir, Telekom tarafından sağlanan bir mobil hizmettir. Bu hizmet, birisi sizi aradığında ve siz telefonu açamadığınızda, arayan kişinin sesli bir mesaj bırakmasını sağlar. Bu, arayan kişinin size önemli bir mesajı olduğunda, ancak sizinle doğrudan iletişim kuramadığında çok kullanışlıdır.\n",
      "\n",
      "Mobilbox'ı aktive etmek için iki seçeneğiniz vardır ve bu, hem yurt içi hem de yurt dışında çalışır. Mobilbox'ınızı deaktive etmek de aynı şekilde iki seçenekle mümkündür. Telekom Mobilbox'ınızı, Mobilbox'ınıza ait cep telefonu ön ekini (ör. 0171) kullanarak alabilirsiniz.\n",
      "\n",
      "Telekom, Mobilbox Pro adında bir Mobilbox versiyonunu da sunar. Bu servis, Telekom tarafından satılan tüm akıllı telefonlarda (Apple cihazları hariç) önceden kurulmuştur. Android cihazlarda, Mobilbox Pro mesajlarınız Voicemail uygulamasında bulunabilir. Voicemail uygulaması Android akıllı telefonunuzda önceden yüklenmemişse, uygulamayı Google Play Store'dan indirebilirsiniz. Bu hizmeti etkinleştirmek için \"MBP\" metniyle ücretsiz bir SMS'i 3011 kısa numarasına göndermeniz gerekir.\n",
      "\n",
      "Mobilbox'ınızın varsayılan karşılama mesajını geri yüklemek isterseniz, Mobilbox'ınızı 3311 üzerinden aramanız ve ardından yönergeleri takip etmeniz gerekmektedir.\n"
     ]
    }
   ],
   "source": [
    "# Few Shot Examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Transform examples into example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate step-back queries\n",
    "generate_queries_step_back = prompt | model | StrOutputParser()\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Original Question: {question}\")\n",
    "print(f\"Step-Back Question: {step_back_question}\")\n",
    "\n",
    "# Response prompt template\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "def get_retrieved_content(retrieved_documents):\n",
    "    \"\"\"Format retrieved documents as a string with source information.\"\"\"\n",
    "    seen_sources = set()\n",
    "    content_list = []\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            content = (\n",
    "                f\"Source: {source}\\n\"\n",
    "                f\"Content:\\n{doc.page_content}\\n\"\n",
    "                \"------------------------------\\n\"\n",
    "            )\n",
    "            content_list.append(content)\n",
    "    return \"\\n\".join(content_list)\n",
    "\n",
    "def format_retrieved_context(query):\n",
    "    \"\"\"Retrieve and format context for the given query.\"\"\"\n",
    "    # Retrieve documents using the 'invoke' method\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    return get_retrieved_content(retrieved_docs)\n",
    "\n",
    "# Construct the chain to retrieve and generate the response\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": lambda x: format_retrieved_context(x[\"question\"]),\n",
    "        \"step_back_context\": lambda x: format_retrieved_context(x[\"step_back_question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "result = chain.invoke({\"question\": question, \"step_back_question\": step_back_question})\n",
    "\n",
    "# Display the final response\n",
    "print(\"\\nNormal Context:\\n\", format_retrieved_context(question))\n",
    "print(\"\\nStep-Back Context:\\n\", format_retrieved_context(step_back_question))\n",
    "print(\"\\nFinal Answer:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "# This list provides example pairs of input questions and their corresponding step-back questions for model training.\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create a prompt template for examples.\n",
    "# This template formats example messages for the model to learn from.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),  # Input from the user\n",
    "        (\"ai\", \"{output}\"),    # Model's response to the input\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a few-shot prompt template that includes example prompts.\n",
    "# This helps the model understand the context by providing example inputs and outputs.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Define the final prompt template.\n",
    "# This includes system instructions and integrates the few-shot prompt.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),  # Input question from the user\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate step-back queries using the defined prompt.\n",
    "# This involves processing the original question to generate a more general query.\n",
    "generate_queries_step_back = prompt | model | StrOutputParser()\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "\n",
    "# Response prompt template\n",
    "# This template is used to generate the final response based on the retrieved context and the original question.\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "def get_retrieved_content(retrieved_documents):\n",
    "    \"\"\"\n",
    "    Format retrieved documents as a string with source information.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_documents (list): List of documents retrieved based on the query.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string containing source and content of retrieved documents.\n",
    "    \"\"\"\n",
    "    seen_sources = set()  # Track unique sources\n",
    "    content_list = []      # List to accumulate formatted content\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown')  # Get source of the document\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            content = (\n",
    "                f\"Source: {source}\\n\"\n",
    "                f\"Content:\\n{doc.page_content}\\n\"\n",
    "                \"------------------------------\\n\"\n",
    "            )\n",
    "            content_list.append(content)\n",
    "    return \"\\n\".join(content_list)\n",
    "\n",
    "def format_retrieved_context(query):\n",
    "    \"\"\"\n",
    "    Retrieve and format context for the given query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query for which context needs to be retrieved.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string containing context relevant to the query.\n",
    "    \"\"\"\n",
    "    # Retrieve documents using the 'invoke' method\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    return get_retrieved_content(retrieved_docs)\n",
    "\n",
    "# Construct the chain to retrieve and generate the response.\n",
    "# This chain combines context retrieval and response generation.\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": lambda x: format_retrieved_context(x[\"question\"]),\n",
    "        \"step_back_context\": lambda x: format_retrieved_context(x[\"step_back_question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain to get the final response.\n",
    "result = chain.invoke({\"question\": question, \"step_back_question\": step_back_question})\n",
    "\n",
    "# Display the final response along with normal and step-back contexts.\n",
    "print(\"Answer:\", result)\n",
    "print(f\"\\n\\nOriginal Question: {question}\")\n",
    "print(f\"\\nStep-Back Question: {step_back_question}\")\n",
    "print(\"\\nNormal Context:\\n\", format_retrieved_context(question))\n",
    "print(\"\\nStep-Back Context:\\n\", format_retrieved_context(step_back_question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE document generation\n",
    "template = \"\"\"You are creating professional and customer-focused web page content and texts for a major telecommunications provider like Telekom.de. \n",
    "Your content is very brief, very clear, and informative. Please write a text for the following question\n",
    "Question: {question}\n",
    "text:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run HyDE generation\n",
    "try:\n",
    "    hyde_output = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "    print(f\"HyDE hypothetical answer:\\n{hyde_output.strip()}\\n\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating documents for retrieval: {e}\")\n",
    "    raise\n",
    "\n",
    "# Retrieve documents\n",
    "try:\n",
    "    retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "    retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Print retrieved documents, deduplicated\n",
    "    seen_sources = set()\n",
    "    print(\"Retrieved sources:\")\n",
    "    for doc in retrieved_docs:\n",
    "        source = doc.metadata.get('source', 'Unknown Source')\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            print(f\"\\nDocument Source: {source}\")\n",
    "            print(f\"Document Content:\\n{doc.page_content.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "try:\n",
    "    final_answer = final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "    print(f\"\\nFinal RAG Answer:\\n{final_answer.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating final RAG answer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE Document Generation\n",
    "# This section is responsible for creating professional and customer-focused content\n",
    "# for a major telecommunications provider based on a given question.\n",
    "\n",
    "# Define a template for generating content.\n",
    "# The template specifies that the content should be brief, clear, and informative.\n",
    "template = \"\"\"You are creating professional and customer-focused web page content and texts for a major telecommunications provider like Telekom.de. \n",
    "Your content is very brief, very clear, and informative. Please write a text for the following question:\n",
    "Question: {question}\n",
    "text:\"\"\"\n",
    "\n",
    "# Create a prompt template using the defined template.\n",
    "# This template will be used to generate content for a given question.\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define a chain to generate documents for retrieval.\n",
    "# This chain uses the prompt template, a language model, and an output parser.\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run HyDE document generation to produce content for the given question.\n",
    "# The try-except block handles potential errors during document generation.\n",
    "try:\n",
    "    hyde_output = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "    print(f\"HyDE hypothetical context:\\n{hyde_output.strip()}\\n\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating documents for retrieval: {e}\")\n",
    "    raise\n",
    "\n",
    "# Retrieve Documents\n",
    "# This section retrieves documents based on the generated content and prints them.\n",
    "\n",
    "# Define a chain to retrieve documents using the generated content.\n",
    "# The chain combines the document generation process with a retriever.\n",
    "try:\n",
    "    retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "    retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Print retrieved documents and deduplicate them based on source information.\n",
    "    seen_sources = set()\n",
    "    print(\"Retrieved sources:\")\n",
    "    for doc in retrieved_docs:\n",
    "        source = doc.metadata.get('source', 'Unknown Source')  # Get the source of the document\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            print(f\"\\nSource file: {source}\")\n",
    "            print(f\"Document Content:\\n{doc.page_content.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define a chain to generate the final answer using the RAG process.\n",
    "# The chain combines the prompt template, a language model, and an output parser.\n",
    "final_rag_chain = (\n",
    "    prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Generate the final answer using the RAG process.\n",
    "# The try-except block handles potential errors during the final answer generation.\n",
    "try:\n",
    "    final_answer = final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "    print(f\"\\nFinal Answer:\\n{final_answer.strip()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating final RAG answer: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
