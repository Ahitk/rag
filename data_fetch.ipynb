{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT'nin optimize ettigi path ve file yazan kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape yaptiktan sonra output'larin oldugu path'i en son output_folder seklindeydi, bunu ortak sabit bir yere almak gerek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed. The extracted sections from all text files have been written to data.txt.\n",
      "Total number of .txt files in the folder: 2595\n",
      "Number of .txt files processed and written to data.txt: 1676\n",
      "Unprocessed files and reasons have been written to data.txt.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def extract_and_write_data(directory_path, output_file):\n",
    "    pattern = r\"\\.\\.\\.Telekom.*?\\s{2}\"\n",
    "    total_files = 0\n",
    "    processed_files = 0\n",
    "    unprocessed_files = []\n",
    "\n",
    "    def process_file(filename):\n",
    "        nonlocal processed_files\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as source:\n",
    "                content = source.read()\n",
    "                match = re.search(pattern, content, re.DOTALL)\n",
    "                if match:\n",
    "                    processed_files += 1\n",
    "                    matched_text = match.group(0)\n",
    "                    cleaned_text = matched_text.strip(\".\").strip()\n",
    "                    sections = re.split(r'\\.\\.\\.\\.+', cleaned_text)\n",
    "                    sections = [section.strip() for section in sections if section.strip()]\n",
    "                    if sections and sections[-1].endswith(\"...\"):\n",
    "                        sections[-1] = sections[-1].rstrip(\".\")\n",
    "                    return filename, sections\n",
    "                else:\n",
    "                    return filename, None\n",
    "        except Exception as e:\n",
    "            return filename, str(e)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                total_files += 1\n",
    "                result = process_file(filename)\n",
    "                if isinstance(result[1], str):\n",
    "                    unprocessed_files.append(f\"{result[0]}: Error - {result[1]}\")\n",
    "                elif result[1] is None:\n",
    "                    unprocessed_files.append(f\"{result[0]}: No matching content found.\")\n",
    "                else:\n",
    "                    output.write(f\"File: {result[0]}\\n\")\n",
    "                    for section in result[1]:\n",
    "                        output.write(f\"{section}\\n\")\n",
    "                    output.write(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "        output.write(f\"\\nTotal number of .txt files in the folder: {total_files}\\n\")\n",
    "        output.write(f\"Number of .txt files processed and written to data.txt: {processed_files}\\n\")\n",
    "        if unprocessed_files:\n",
    "            output.write(\"\\nUnprocessed files:\\n\")\n",
    "            for entry in unprocessed_files:\n",
    "                output.write(f\"{entry}\\n\")\n",
    "\n",
    "    print(\"Process completed. The extracted sections from all text files have been written to data.txt.\")\n",
    "    print(f\"Total number of .txt files in the folder: {total_files}\")\n",
    "    print(f\"Number of .txt files processed and written to data.txt: {processed_files}\")\n",
    "    print(f\"Unprocessed files and reasons have been written to data.txt.\")\n",
    "\n",
    "# KullanÄ±m\n",
    "directory_path = \"/Users/taha/Desktop/scrapeV2/output_folder\"\n",
    "output_file = \"data.txt\"\n",
    "extract_and_write_data(directory_path, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.txt ve data.csv - navigation soru ve cevap - ayiklanmadi ama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed. The extracted sections from all text files have been written to data.txt and data.csv.\n",
      "Total number of .txt files in the folder: 2595\n",
      "Number of .txt files processed and written to data.txt: 1676\n",
      "Unprocessed files and reasons have been written to data.txt.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def extract_and_write_data(directory_path, output_file_txt, output_file_csv):\n",
    "    # Regex pattern: Start with \"...Telekom\" and end with two spaces\n",
    "    pattern = r\"\\.\\.\\.Telekom.*?\\s{2}\"\n",
    "    \n",
    "    # Initialize counters and lists\n",
    "    total_files = 0\n",
    "    processed_files = 0\n",
    "    unprocessed_files = []\n",
    "    processed_files_data = []\n",
    "\n",
    "    def process_file(filename):\n",
    "        nonlocal processed_files\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as source:\n",
    "                content = source.read()\n",
    "                match = re.search(pattern, content, re.DOTALL)\n",
    "                if match:\n",
    "                    processed_files += 1\n",
    "                    matched_text = match.group(0)\n",
    "                    start_index = match.end()\n",
    "\n",
    "                    # Clean and format the matched text\n",
    "                    # Remove leading and trailing dots and split by dots\n",
    "                    cleaned_text = re.sub(r'\\.\\.\\.+', '\\n', matched_text.strip(\".\").strip())\n",
    "                    \n",
    "                    # Extract text after the pattern\n",
    "                    post_pattern_text = content[start_index:]\n",
    "                    \n",
    "                    # Regex pattern to find paragraphs separated by multiple newlines\n",
    "                    paragraph_pattern = r'([^\\n]+(?:\\n[^\\n]+)*)(?:\\n{2,})'\n",
    "                    \n",
    "                    paragraphs = re.findall(paragraph_pattern, post_pattern_text)\n",
    "                    \n",
    "                    if len(paragraphs) >= 2:\n",
    "                        # Extract the first and second paragraphs\n",
    "                        question = paragraphs[0].strip()\n",
    "                        answer = paragraphs[1].strip()\n",
    "                        \n",
    "                        # Check for excessive newlines after the second paragraph\n",
    "                        if len(paragraphs) > 2:\n",
    "                            # Find the number of newlines after the second paragraph\n",
    "                            remaining_text = post_pattern_text[post_pattern_text.find(paragraphs[1]) + len(paragraphs[1]):]\n",
    "                            newlines_after_second = re.match(r'\\n{5,}', remaining_text)\n",
    "                            if newlines_after_second:\n",
    "                                processed_files_data.append((filename, cleaned_text, question, answer))\n",
    "                        else:\n",
    "                            processed_files_data.append((filename, cleaned_text, question, answer))\n",
    "                else:\n",
    "                    unprocessed_files.append(f\"{filename}: No matching content found.\")\n",
    "        except Exception as e:\n",
    "            unprocessed_files.append(f\"{filename}: Error - {str(e)}\")\n",
    "\n",
    "    # Process all files\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            total_files += 1\n",
    "            process_file(filename)\n",
    "\n",
    "    # Sort processed and unprocessed files alphabetically\n",
    "    processed_files_data.sort(key=lambda x: x[0])\n",
    "    unprocessed_files.sort()\n",
    "\n",
    "    # Write to both TXT and CSV files\n",
    "    with open(output_file_txt, \"w\", encoding=\"utf-8\") as output_txt, \\\n",
    "         open(output_file_csv, \"w\", newline='', encoding=\"utf-8\") as output_csv:\n",
    "        \n",
    "        csv_writer = csv.writer(output_csv)\n",
    "        csv_writer.writerow([\"File\", \"Navigation\", \"Question\", \"Answer\"])\n",
    "        \n",
    "        # Write processed files to TXT and CSV\n",
    "        for filename, navigation, question, answer in processed_files_data:\n",
    "            # Write to TXT file\n",
    "            output_txt.write(f\"File: {filename}\\n\")\n",
    "            output_txt.write(f\"Navigation:\\n{navigation}\\n\\n\")\n",
    "            output_txt.write(f\"Question:\\n{question}\\n\\n\")\n",
    "            output_txt.write(f\"Answer:\\n{answer}\\n\")\n",
    "            output_txt.write(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "            \n",
    "            # Write to CSV file\n",
    "            csv_writer.writerow([filename, navigation.replace('\\n', ' '), question.replace('\\n', ' '), answer.replace('\\n', ' ')])\n",
    "\n",
    "        # Write summary information to TXT file\n",
    "        output_txt.write(f\"\\nTotal number of .txt files in the folder: {total_files}\\n\")\n",
    "        output_txt.write(f\"Number of .txt files processed and written to data.txt: {processed_files}\\n\")\n",
    "        \n",
    "        # Write unprocessed files in alphabetical order\n",
    "        if unprocessed_files:\n",
    "            output_txt.write(\"\\nUnprocessed files:\\n\")\n",
    "            for entry in unprocessed_files:\n",
    "                output_txt.write(f\"{entry}\\n\")\n",
    "\n",
    "    print(\"Process completed. The extracted sections from all text files have been written to data.txt and data.csv.\")\n",
    "    print(f\"Total number of .txt files in the folder: {total_files}\")\n",
    "    print(f\"Number of .txt files processed and written to data.txt: {processed_files}\")\n",
    "    print(f\"Unprocessed files and reasons have been written to data.txt.\")\n",
    "\n",
    "# KullanÄ±m\n",
    "directory_path = \"/Users/taha/Desktop/scrape_telekom_website/output_folder\"\n",
    "output_file_txt = \"data.txt\"\n",
    "output_file_csv = \"data.csv\"\n",
    "extract_and_write_data(directory_path, output_file_txt, output_file_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 farkli dosyaya aliyorum, qa.txt..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed. The extracted sections from all text files have been written to data.txt and data.csv.\n",
      "Total number of .txt files in the folder: 2595\n",
      "Number of .txt files processed and written to data.txt: 1676\n",
      "Unprocessed files and reasons have been written to data.txt.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def extract_and_write_data(directory_path, output_file_txt, output_file_csv):\n",
    "    # Regex pattern: Start with \"...Telekom\" and end with two spaces\n",
    "    pattern = r\"\\.\\.\\.Telekom.*?\\s{2}\"\n",
    "    \n",
    "    # Initialize counters and lists\n",
    "    total_files = 0\n",
    "    processed_files = 0\n",
    "    unprocessed_files = []\n",
    "    processed_files_data = []\n",
    "\n",
    "    def process_file(filename):\n",
    "        nonlocal processed_files\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as source:\n",
    "                content = source.read()\n",
    "                match = re.search(pattern, content, re.DOTALL)\n",
    "                if match:\n",
    "                    processed_files += 1\n",
    "                    matched_text = match.group(0)\n",
    "                    start_index = match.end()\n",
    "\n",
    "                    # Clean and format the matched text\n",
    "                    # Remove leading and trailing dots and split by dots\n",
    "                    cleaned_text = re.sub(r'\\.\\.\\.+', '\\n', matched_text.strip(\".\").strip())\n",
    "                    \n",
    "                    # Extract text after the pattern\n",
    "                    post_pattern_text = content[start_index:]\n",
    "                    \n",
    "                    # Regex pattern to find paragraphs separated by multiple newlines\n",
    "                    paragraph_pattern = r'([^\\n]+(?:\\n[^\\n]+)*)(?:\\n{2,})'\n",
    "                    \n",
    "                    paragraphs = re.findall(paragraph_pattern, post_pattern_text)\n",
    "                    \n",
    "                    if len(paragraphs) >= 2:\n",
    "                        # Extract the first and second paragraphs\n",
    "                        question = paragraphs[0].strip()\n",
    "                        answer = paragraphs[1].strip()\n",
    "                        \n",
    "                        # Check for excessive newlines after the second paragraph\n",
    "                        if len(paragraphs) > 2:\n",
    "                            # Find the number of newlines after the second paragraph\n",
    "                            remaining_text = post_pattern_text[post_pattern_text.find(paragraphs[1]) + len(paragraphs[1]):]\n",
    "                            newlines_after_second = re.match(r'\\n{5,}', remaining_text)\n",
    "                            if newlines_after_second:\n",
    "                                processed_files_data.append((filename, cleaned_text, question, answer))\n",
    "                        else:\n",
    "                            processed_files_data.append((filename, cleaned_text, question, answer))\n",
    "                else:\n",
    "                    unprocessed_files.append(f\"{filename}: No matching content found.\")\n",
    "        except Exception as e:\n",
    "            unprocessed_files.append(f\"{filename}: Error - {str(e)}\")\n",
    "\n",
    "    # Process all files\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            total_files += 1\n",
    "            process_file(filename)\n",
    "\n",
    "    # Sort processed and unprocessed files alphabetically\n",
    "    processed_files_data.sort(key=lambda x: x[0])\n",
    "    unprocessed_files.sort()\n",
    "\n",
    "    # Write to both TXT and CSV files\n",
    "    with open(output_file_txt, \"w\", encoding=\"utf-8\") as output_txt, \\\n",
    "         open(output_file_csv, \"w\", newline='', encoding=\"utf-8\") as output_csv:\n",
    "        \n",
    "        csv_writer = csv.writer(output_csv)\n",
    "        csv_writer.writerow([\"File\", \"Navigation\", \"Question\", \"Answer\"])\n",
    "        \n",
    "        # Write processed files to TXT and CSV\n",
    "        for filename, navigation, question, answer in processed_files_data:\n",
    "            # Write to TXT file\n",
    "            output_txt.write(f\"File: {filename}\\n\")\n",
    "            output_txt.write(f\"Navigation:\\n{navigation}\\n\\n\")\n",
    "            output_txt.write(f\"Question: {question}\\n\\n\")\n",
    "            output_txt.write(f\"Answer: {answer}\\n\")\n",
    "            output_txt.write(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "            \n",
    "            # Write to CSV file\n",
    "            csv_writer.writerow([filename, navigation.replace('\\n', ' '), question.replace('\\n', ' '), answer.replace('\\n', ' ')])\n",
    "\n",
    "        # Write summary information to TXT file\n",
    "        output_txt.write(f\"\\nTotal number of .txt files in the folder: {total_files}\\n\")\n",
    "        output_txt.write(f\"Number of .txt files processed and written to data.txt: {processed_files}\\n\")\n",
    "        \n",
    "        # Write unprocessed files in alphabetical order\n",
    "        if unprocessed_files:\n",
    "            output_txt.write(\"\\nUnprocessed files:\\n\")\n",
    "            for entry in unprocessed_files:\n",
    "                output_txt.write(f\"{entry}\\n\")\n",
    "\n",
    "    print(\"Process completed. The extracted sections from all text files have been written to data.txt and data.csv.\")\n",
    "    print(f\"Total number of .txt files in the folder: {total_files}\")\n",
    "    print(f\"Number of .txt files processed and written to data.txt: {processed_files}\")\n",
    "    print(f\"Unprocessed files and reasons have been written to data.txt.\")\n",
    "\n",
    "# KullanÄ±m\n",
    "directory_path = \"/Users/taha/Desktop/scrape_telekom_website/output_folder\"\n",
    "output_file_txt = \"data.txt\"\n",
    "output_file_csv = \"data.csv\"\n",
    "extract_and_write_data(directory_path, output_file_txt, output_file_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 farkli txt dosyasina al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks processed as questions: 1036\n",
      "Filtering completed. Data has been written to qa.txt, uncategorised.txt, and unidentified.txt.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def filter_data_files(data_txt_file, qa_file, uncategorised_file, unidentified_file):\n",
    "    # Regular expressions for finding sections\n",
    "    file_pattern = re.compile(r\"File:\\s*(.*?)\\n\")\n",
    "    navigation_pattern = re.compile(r\"Navigation:\\n(.*?)\\n\\n\", re.DOTALL)\n",
    "    question_pattern = re.compile(r\"Question:\\s*(.*?)\\n\\n\", re.DOTALL)\n",
    "    answer_pattern = re.compile(r\"Answer:\\s*(.*?)\\n\", re.DOTALL)\n",
    "    \n",
    "    # Lists to store results\n",
    "    qa_data = []\n",
    "    uncategorised_data = []\n",
    "    unidentified_data = []\n",
    "\n",
    "    # Read and process the data.txt file\n",
    "    with open(data_txt_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        \n",
    "        # Split content into blocks based on separator lines\n",
    "        blocks = content.split(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "        for block in blocks:\n",
    "            # Match different sections in the block\n",
    "            file_match = file_pattern.search(block)\n",
    "            navigation_match = navigation_pattern.search(block)\n",
    "            question_match = question_pattern.search(block)\n",
    "            answer_match = answer_pattern.search(block)\n",
    "            \n",
    "            if file_match and navigation_match and question_match and answer_match:\n",
    "                filename = file_match.group(1).strip()\n",
    "                navigation = navigation_match.group(1).strip()\n",
    "                question = question_match.group(1).strip()\n",
    "                answer = answer_match.group(1).strip()\n",
    "                \n",
    "                # Check if the question ends with a question mark\n",
    "                if question.endswith('?'):\n",
    "                    qa_data.append(f\"File: {filename}\\nNavigation:\\n{navigation}\\n\\nQuestion: {question}\\n\\nAnswer: {answer}\\n\\n\")\n",
    "                else:\n",
    "                    uncategorised_data.append(f\"File: {filename}\\nNavigation:\\n{navigation}\\n\\nQuestion: {question}\\n\\nAnswer: {answer}\\n\\n\")\n",
    "            else:\n",
    "                # If any part of the block is missing, consider it unidentified\n",
    "                unidentified_data.append(block.strip())\n",
    "    \n",
    "    # Write filtered data to respective files\n",
    "    with open(qa_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(qa_data)\n",
    "\n",
    "    with open(uncategorised_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(uncategorised_data)\n",
    "    \n",
    "    with open(unidentified_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(unidentified_data)\n",
    "\n",
    "    # Print the number of blocks processed as questions\n",
    "    print(f\"Number of blocks processed as questions: {len(qa_data)}\")\n",
    "\n",
    "    print(\"Filtering completed. Data has been written to qa.txt, uncategorised.txt, and unidentified.txt.\")\n",
    "\n",
    "# KullanÄ±m\n",
    "data_txt_file = \"data.txt\"\n",
    "qa_file = \"qa.txt\"\n",
    "uncategorised_file = \"uncategorised.txt\"\n",
    "unidentified_file = \"unidentified.txt\"\n",
    "filter_data_files(data_txt_file, qa_file, uncategorised_file, unidentified_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
