{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RAG - Q&A System\n",
    "\n",
    "This code implements a document retrieval and question-answering system using OpenAI's GPT-4 model. Key steps include:\n",
    "\n",
    "- **Environment Setup:** Loads API keys from `.env`.\n",
    "- **Model Initialization:** Sets up GPT-4 for generating responses.\n",
    "- **Document Indexing:** Loads text documents, embeds them, and stores in a Chroma vector store for retrieval.\n",
    "- **Retrieval & Formatting:** Retrieves relevant documents based on a question, calculates cosine similarity, and formats the output.\n",
    "- **Async Execution:** Uses async functions to retrieve documents and generate concise answers efficiently.\n",
    "\n",
    "Run the `main()` function to see the system in action with a sample question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: MagentaTV is a high-quality streaming service offered by Telekom. It provides a vast and unique selection of series, films, shows, documentaries, and content for children, including many originals and exclusives only available on MagentaTV+. It can be enjoyed at home or on the go using a TV receiver, the MagentaTV One, or the MagentaTV App, all offering numerous comfortable features to enhance your online TV experience.\n",
      "\n",
      "Sources:\n",
      "Source document: rag_data/website/organized_data/Others/https_www_telekom_de_unterhaltung_serien_und_filme.txt\n",
      "\n",
      "Cosine Similarity: 0.8683\n",
      "\n",
      "Source URL: https://www.telekom.de/unterhaltung/serien-und-filme\n",
      "\n",
      "Question: Was ist MagentaTV+?\n",
      "Answer: MagentaTV+ ist ein hochwertiges Streaming-Angebot, das bei MagentaTV immer enthalten ist. Hier finden Sie eine riesige und einzigartige Auswahl an Serien, Filmen, Shows, Dokumentationen und Kinderinhalten. Dazu gehören viele Originals und Exklusives, die es nur bei MagentaTV+ gibt. Dazu zählen etwa:\n",
      "Darüber hinaus bietet MagentaTV+ zahlreiche internationale Premium-Serien & -Filme von AXN+, Pa\n",
      "\n",
      "Source document: rag_data/website/organized_data/Others/https_www_telekom_de_magenta_tv_online_tv.txt\n",
      "\n",
      "Cosine Similarity: 0.8595\n",
      "\n",
      "Source URL: https://www.telekom.de/magenta-tv/online-tv\n",
      "\n",
      "Question: Sie haben noch kein MagentaTV?\n",
      "Answer: Ein großartiges Fernsehvergnügen zu Hause oder unterwegs – das genießen Sie mit der Telekom und Online-TV. Nutzen Sie dafür entweder unserenTV-Receiver, dieMagentaTV Oneoder dieMagentaTV App. Profitieren Sie von zahlreichen komfortablen Funktionen, die Ihr Online-TV-Erlebnis besonders machen:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "import asyncio\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Model Initialization\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Base directory for loading documents\n",
    "base_directory = \"rag_data/website/organized_data\"\n",
    "\n",
    "# Load all .txt files from the specified directory using DirectoryLoader\n",
    "loader = DirectoryLoader(base_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "# Load documents\n",
    "docs = loader.load()\n",
    "\n",
    "# Embedding\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "\n",
    "# Define the retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt template for question-answering\n",
    "custom_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a Telekom-Hilfe assistant for question-answering tasks, providing answers to Telekom customers or potential customers.\n",
    "        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Cosine Similarity calculation function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Function to format documents with a limit on the number of documents included\n",
    "def format_docs(docs, query_embedding, max_docs=5):\n",
    "    \"\"\"\n",
    "    Formats the documents for inclusion in the context, limiting the total number of documents.\n",
    "    Args:\n",
    "        docs: List of retrieved documents.\n",
    "        query_embedding: Embedding vector for the query.\n",
    "        max_docs: Maximum number of documents to include.\n",
    "    Returns:\n",
    "        A string containing formatted documents.\n",
    "    \"\"\"\n",
    "    unique_sources = set()  # To keep track of unique sources\n",
    "    formatted_docs = []\n",
    "\n",
    "    '''The format_docs function limits the number of documents to include in the context (max_docs=5).\n",
    "    Each document’s content is truncated to 500 characters to help stay within the model’s token limit.\n",
    "    These changes should help avoid exceeding the token limit of the GPT-4 model and prevent the BadRequestError. \n",
    "    Adjust max_docs and content length if necessary based on further testing.'''\n",
    "\n",
    "    for doc in docs[:max_docs]:  # Limit the number of documents to `max_docs`\n",
    "        source = doc.metadata.get(\"source\")  # Get the source from metadata\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            document_embedding = embedding.embed_query(doc.page_content)  # Compute embedding\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)  # Cosine similarity\n",
    "            content = doc.page_content.strip()[:500]  # Trim content to reduce size\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Define the processing chain\n",
    "rag_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | custom_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Function to retrieve and format documents and generate an answer\n",
    "async def retrieve_and_format_docs(question):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents and formats them to answer a given question.\n",
    "    Args:\n",
    "        question: The input question as a string.\n",
    "    Returns:\n",
    "        The generated answer and formatted documents.\n",
    "    \"\"\"\n",
    "    # Get the query embedding\n",
    "    query_embedding = embedding.embed_query(question)\n",
    "    retrieved_docs = await retriever.ainvoke(question)  # Retrieve relevant documents\n",
    "\n",
    "    # Format documents with a limit to avoid exceeding token limit\n",
    "    formatted_docs = format_docs(retrieved_docs, query_embedding, max_docs=5)\n",
    "    \n",
    "    # Generate the answer using the formatted context\n",
    "    try:\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "# Main function to run the retrieval and answer generation process\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the retrieval and answer generation process.\n",
    "    \"\"\"\n",
    "    question = \"What is Magenta TV?\"\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "# Execute the main function using the current event loop\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
