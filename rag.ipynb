{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() #.env dosyasini yüklüyor, API key'leri yüklüyor.\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\") #UniKassel hesabi gpt-4. Temperature eklenebilir, duruma göre belki evaluation asamasinda faydali olabilir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobilfunk Dieser Warenkorb enthält {{ITEMS_COUNT}} Artikel. Zum Warenkorb Festnetz & TV Dieser Warenkorb enthält {{ITEMS_COUNT}} Artikel. Zum Warenkorb Magenta-WK Dieser Warenkorb enthält {{ITEMS_COUNT}} Artikel. Zum Warenkorb MeinMagenta App entdecken Zur App und Vorteile sichern! Kundencenter Im Kundencenter können Sie Verträge verwalten und auf Rechnungen, Verbrauchsanzeigen, Einstellungen und mehr zugreifen. Hilfe zum LoginNoch nicht registriert? Jetzt registrieren Login-Einstellungen bearbeiten   Hallo {{USER_NAME}} Sie möchten Ihren persönlichen Bereich verlassen?   So können Sie Ihren Mobilfunk-Vertrag mit einem Telekom Login verknüpfen: Melden Sie sich mit Ihrem Telekom Login im Kundencenter an und gehen Sie in der Kachel \"Verträge\" auf \"Verträge verwalten\". Klicken Sie dort auf \"Vertrag hinzufügen\". Nun wählen Sie zwischen \"Privater Vertrag\" oder \"Geschäftlicher Vertrag\". In der nächsten Ansicht sehen Sie die bereits verknüpften Verträge inklusive Nutzerrollen. Klicken Sie im Reiter \"Vertragsinhaber\" oder \"Nutzer\" auf \"Mobilfunk-Vertrag hinzufügen\" und tragen Sie anschließend Ihre Mobilfunk-Nummer ein. Folgen Sie den weiteren Anweisungen. Hinweis: Sie benötigen dabei den Zugriff auf die Mobilfunk-Nummer, da der Freischalt-Code per SMS verschickt wird. Newsletter Ich möchte per E-Mail Newsletter über Angebote, Produktneuheiten und exklusive Aktionen der Telekom Deutschland GmbH informiert werden. Die Einwilligung zur Newsletterzusendung kann ich jederzeit durch einen Klick auf den Abmeldelink am Ende jeder E-Mail widerrufen werden. Darüber hinaus gelten der Datenschutzhinweis Newsletterversand sowie der Allgemeine Datenschutzhinweis der Telekom Deutschland GmbH. MeinMagenta App Alle Infos und bester Service für Mobilfunk & Festnetz Social Media \n"
     ]
    }
   ],
   "source": [
    "#ChatGPT kodu bu: deneme icin bir telekom sayfasi getirmeye calistim ama istedgiim sayfanin icerigi görüntülenmiyor.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.telekom.de/hilfe/vertrag-rechnung/telekom-login/mobilfunk-verknuepfen'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Web sayfasındaki tüm paragrafları al\n",
    "paragraphs = soup.find_all('p')\n",
    "text_data = [p.get_text() for p in paragraphs]\n",
    "\n",
    "# Tüm metni birleştir\n",
    "full_text = ' '.join(text_data)\n",
    "print(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# creating or overwriting a file \"subtitles.txt\" with \\n# the info inside the context manager\\nwith open(\"subtitles.txt\", \"w\") as f:\\n   \\n        # iterating through each element of list srt\\n    for i in transcript:\\n        # writing each element of transcript on a new line\\n        f.write(\"{}\\n\".format(i))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deneme icin buraya bir tane Telekom Hilfe kanalindan bir video alyiorum, \n",
    "# komple youtube kanalini nasil transkript ettirip alabilirim buna bakmaliyim, \n",
    "# bir dünya video var cünkü.\n",
    "# BURADA PROBLEM ALMANCA KARAKTERLERIN DÜZGÜN GÖSTERILMEMESI; ß,ü,ö,ä...\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=BrsocJb-fAo&t=1525s&ab_channel=Underfitted\"\n",
    "\n",
    "transcript = YouTubeTranscriptApi.get_transcript(\"X3D008558Ek?si=YUbILctuzqEOHzOR\", languages=['de', 'en'])\n",
    "\n",
    "'''\n",
    "# creating or overwriting a file \"subtitles.txt\" with \n",
    "# the info inside the context manager\n",
    "with open(\"subtitles.txt\", \"w\") as f:\n",
    "   \n",
    "        # iterating through each element of list srt\n",
    "    for i in transcript:\n",
    "        # writing each element of transcript on a new line\n",
    "        f.write(\"{}\\n\".format(i))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmessages = [\\n    SystemMessage(content=\"Translate the following from Turkish into English\"),\\n    HumanMessage(content=\"Bu benim ilk denemem!\"),\\n]\\n\\nresult = model.invoke(messages)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Burada OpenAI API request yapisi var, human, system vb...\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "'''\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from Turkish into English\"),\n",
    "    HumanMessage(content=\"Bu benim ilk denemem!\"),\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video1: Overview - RAG From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taha/Desktop/rag/venv/lib/python3.12/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Task Decomposition is a process where complex tasks are broken down into smaller, simpler steps. This technique, often used in models to enhance performance on complicated tasks, allows for more efficient use of computation time during testing. It transforms a big task into multiple manageable tasks, providing insight into the model's thinking process.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents, Youtube\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM, yukarida zaten model diye gpt-4 tanimladik, temperature eklenebilir\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} #question: RunnablePassthrough kullanarak kullanıcıdan gelen soruyu işler.\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser() \n",
    "    # StrOutputParser: Modelden gelen yanıtı ayrıştırır, karmasik olarak degilde direkt string olarak verir.\n",
    "    # parser = StrOutputParser() chain = model | parser, bu sekilde de kullanilabilir.\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"My favorite pet is a cat.\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)\n",
    "# sorunun embedding olarak ne kadar uzun oldugunu yazdiriyor, 1536..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000016EF5782210>\n"
     ]
    }
   ],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video3: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"What is Task Decomposition?\")\n",
    "len(docs)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task Decomposition is a process where hard tasks are broken down into smaller and simpler steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts are used to enhance model performance on complex tasks. CoT instructs the model to \"think step by step\" to utilize more test-time computation to decompose tasks, while Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. Task decomposition can be done by an LLM with simple prompting, by using task-specific instructions, or with human inputs.', response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 331, 'total_tokens': 441}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8fa1913f-a4aa-46a0-90c9-a99604e60b41-0', usage_metadata={'input_tokens': 331, 'output_tokens': 110, 'total_tokens': 441})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a process where hard tasks are broken down into smaller and simpler steps. This technique, often used to enhance model performance on complex tasks, transforms big tasks into multiple manageable tasks. It can be done by using simple prompting, task-specific instructions, or with human inputs.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_hub_rag\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
