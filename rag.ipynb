{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaynak dosya adi, kaynak url, cos similarity ve kaynak soru cevaplarla veriyor ciktiyi.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "import asyncio\n",
    "\n",
    "# .env dosyasını yükleyerek API anahtarlarını getir\n",
    "load_dotenv()\n",
    "\n",
    "# API Anahtarları\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Model Tanımlaması\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Ana veri dizini: Tüm kategoriler altındaki dosyaları yükleme\n",
    "base_directory = \"rag_data/website/organized_data\"\n",
    "\n",
    "# DirectoryLoader ile belirtilen dizindeki tüm alt klasörlerden txt dosyalarını yükle\n",
    "loader = DirectoryLoader(base_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "# Belgeleri yükle\n",
    "docs = loader.load()\n",
    "\n",
    "# Embedding işlemi\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "\n",
    "# Retriever tanımlaması\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt: ChatPromptTemplate kullanılarak prompt hazırlanır\n",
    "custom_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a Telekom-Hilfe assistant for question-answering tasks, providing answers to Telekom customers or potential customers.\n",
    "        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Cosine Similarity hesaplama fonksiyonu\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Belgeleri formatlama fonksiyonu\n",
    "def format_docs(docs, query_embedding):\n",
    "    unique_sources = set()  # Benzersiz kaynakları saklamak için bir set\n",
    "    formatted_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get(\"source\")  # Kaynağı metadata'dan al\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            document_embedding = embedding.embed_query(doc.page_content)  # Embedding hesapla\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)  # Cosine similarity hesapla\n",
    "            content = doc.page_content.strip() or \"Bu belge içeriği boş.\"  # Belge içeriği\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Chain tanımlaması\n",
    "rag_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | custom_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Sorgu için kullanılan belgeleri ve cevabı döndüren fonksiyon\n",
    "async def retrieve_and_format_docs(question):\n",
    "    # Belgeleri sorgu için al\n",
    "    query_embedding = embedding.embed_query(question)  # Sorgu için embedding al\n",
    "    retrieved_docs = await retriever.ainvoke(question)\n",
    "    \n",
    "    # Belgelerin içeriğini formatla\n",
    "    formatted_docs = format_docs(retrieved_docs, query_embedding)\n",
    "    \n",
    "    # Cevabı al\n",
    "    try:\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Eğer TypeError alınırsa, invoke çağrısının senkron olduğu anlamına gelebilir\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "# Jupyter Notebook gibi bir ortamda, mevcut olay döngüsünü kullanarak asenkron işlevleri çağırma\n",
    "async def main():\n",
    "    question = \"Magenta TV nedir?\"\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "# Mevcut olay döngüsünü kullanarak asenkron işlevleri çalıştırma\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "\n",
    "question = \"Vertrag'imi kündigen yapmak istiyorum, ne yapmaliyim?\"\n",
    "\n",
    "# .env dosyasını yükleyerek API anahtarlarını getir\n",
    "load_dotenv()\n",
    "\n",
    "# API Anahtarları\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Model Tanımlaması\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Ana veri dizini: Tüm kategoriler altındaki dosyaları yükleme\n",
    "base_directory = \"rag_data/website/organized_data\"\n",
    "\n",
    "# DirectoryLoader ile belirtilen dizindeki tüm alt klasörlerden txt dosyalarını yükle\n",
    "loader = DirectoryLoader(base_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "# Belgeleri yükle\n",
    "docs = loader.load()\n",
    "\n",
    "# Embedding işlemi\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "\n",
    "# Retriever tanımlaması\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "####### Multi Query ########\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "   \n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)\n",
    "print(docs)\n",
    "\n",
    "\n",
    "#################################\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt: ChatPromptTemplate kullanılarak prompt hazırlanır\n",
    "custom_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a Telekom-Hilfe assistant for question-answering tasks, providing answers to Telekom customers or potential customers.\n",
    "        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Cosine Similarity hesaplama fonksiyonu\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Belgeleri formatlama fonksiyonu\n",
    "def format_docs(docs, query_embedding):\n",
    "    unique_sources = set()  # Benzersiz kaynakları saklamak için bir set\n",
    "    formatted_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get(\"source\")  # Kaynağı metadata'dan al\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            document_embedding = embedding.embed_query(doc.page_content)  # Embedding hesapla\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)  # Cosine similarity hesapla\n",
    "            content = doc.page_content.strip() or \"Bu belge içeriği boş.\"  # Belge içeriği\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Chain tanımlaması\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    |\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | custom_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Sorgu için kullanılan belgeleri ve cevabı döndüren fonksiyon\n",
    "async def retrieve_and_format_docs(question):\n",
    "    # Belgeleri sorgu için al\n",
    "    query_embedding = embedding.embed_query(question)  # Sorgu için embedding al\n",
    "    retrieved_docs = await retriever.ainvoke(question)\n",
    "    \n",
    "    # Belgelerin içeriğini formatla\n",
    "    formatted_docs = format_docs(retrieved_docs, query_embedding)\n",
    "    \n",
    "    # Cevabı al\n",
    "    try:\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Eğer TypeError alınırsa, invoke çağrısının senkron olduğu anlamına gelebilir\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "# Jupyter Notebook gibi bir ortamda, mevcut olay döngüsünü kullanarak asenkron işlevleri çağırma\n",
    "async def main():\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "# Mevcut olay döngüsünü kullanarak asenkron işlevleri çalıştırma\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alttaki son kod daha iyi calisiyorsa üsttekini sil!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "\n",
    "# Soru Tanımlaması\n",
    "question = \"Vertrag'imi kündigen yapmak istiyorum, ne yapmaliyim?\"\n",
    "\n",
    "# API Anahtarlarını Yükle\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Modeli ve Embedding'i Başlat\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Belgeleri Yükle ve Embedding Oluştur\n",
    "def initialize_vectorstore(directory):\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs = loader.load()\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "    return vectorstore, docs\n",
    "\n",
    "vectorstore, docs = initialize_vectorstore(\"rag_data/website/organized_data\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Alternatif Sorular İçin Şablon\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "multi_query_docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "# Prompt Tanımlaması\n",
    "custom_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a Telekom-Hilfe assistant for question-answering tasks, providing answers to Telekom customers or potential customers.\n",
    "        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Cosine Similarity Hesaplama Fonksiyonu\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Belgeleri Formatlama\n",
    "def format_docs(docs, query_embedding):\n",
    "    unique_sources = set()\n",
    "    formatted_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get(\"source\")\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            document_embedding = embedding.embed_query(doc.page_content)\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)\n",
    "            content = doc.page_content.strip() or \"Bu belge içeriği boş.\"\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Chain Tanımlaması\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} \n",
    "    | custom_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async def retrieve_and_format_docs(question):\n",
    "    query_embedding = embedding.embed_query(question)\n",
    "    #retrieved_docs = await retriever.ainvoke(question)\n",
    "    formatted_docs = format_docs(multi_query_docs, query_embedding)\n",
    "    \n",
    "    try:\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "# Asenkron Sorular Fonksiyonu\n",
    "async def print_generated_queries(question):\n",
    "    queries = generate_queries.invoke({\"question\": question})  # Burada await kullanmaya gerek yok\n",
    "    print(\"Generated Questions:\")\n",
    "    for i, q in enumerate(queries):\n",
    "        print(f\"{i+1}: {q}\")\n",
    "\n",
    "# Ana Fonksiyon\n",
    "async def main():\n",
    "    await print_generated_queries(question)\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iyi calisiyor gibi ama 4 soru generate ederken bazen dil sorunu olabiliyor türkce girince, ing ve alm olm\n",
    "### Problem: cevapta mevcut data'da bulamayince mevcut belgelerde bulunamadi gibi dönüyor, bunu prompt a ekle oyle coz, salak salak konusmasin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "\n",
    "# Configure logging to suppress INFO logs from HTTP requests\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "logging.getLogger('urllib3').setLevel(logging.WARNING)\n",
    "\n",
    "# Soru Tanımlaması\n",
    "question = \"esim nedir?\"\n",
    "\n",
    "# API Anahtarlarını Yükle\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Modeli ve Embedding'i Başlat\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Belgeleri Yükle ve Embedding Oluştur\n",
    "def initialize_vectorstore(directory):\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs = loader.load()\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "    return vectorstore, docs\n",
    "\n",
    "vectorstore, docs = initialize_vectorstore(\"rag_data/website/organized_data\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "############ RAG-Fusion #############\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Generate four queries\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Function for reciprocal rank fusion\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal rank fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "# Chain for the retrieval process\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "fusion_docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "# Cosine Similarity Calculation Function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2) if (norm_vec1 and norm_vec2) else 0.0\n",
    "\n",
    "# Function to get embeddings for a document's content\n",
    "async def get_document_embeddings(doc):\n",
    "    return embedding.embed_query(doc.page_content)\n",
    "\n",
    "# Function to format fusion_docs as a readable string with similarity scores\n",
    "async def format_fusion_docs_with_similarity(fusion_docs):\n",
    "    formatted_docs = []\n",
    "    question_embedding = embedding.embed_query(question)\n",
    "    \n",
    "    for doc, score in fusion_docs:\n",
    "        doc_embedding = await get_document_embeddings(doc)\n",
    "        similarity = cosine_similarity(question_embedding, doc_embedding)\n",
    "        source = doc.metadata.get(\"source\", \"No source\")\n",
    "        content = doc.page_content\n",
    "        formatted_docs.append(f\"Source: {source}\\nFusion Score: {score:.4f}\\nCosine Similarity: {similarity:.4f}\\nContent: {content}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "####################\n",
    "\n",
    "# Prompt Definition\n",
    "telekom_template = \"\"\"You are a Telekom-Hilfe assistant for question-answering tasks, providing answers to Telekom customers or potential customers. \n",
    "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_telekom = ChatPromptTemplate.from_template(telekom_template)\n",
    "\n",
    "# Chain Definition\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")} \n",
    "    | prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async def retrieve_and_format_docs(question):\n",
    "    formatted_docs = await format_fusion_docs_with_similarity(fusion_docs)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to get the answer asynchronously\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Fallback to synchronous invocation if asynchronous fails\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "# Asynchronous function to print generated queries\n",
    "async def print_generated_queries(question):\n",
    "    queries = generate_queries.invoke({\"question\": question})\n",
    "    print(\"Generated Questions:\")\n",
    "    for i, q in enumerate(queries):\n",
    "        print(f\"{i+1}: {q}\")\n",
    "\n",
    "# Main Function\n",
    "async def main():\n",
    "    await print_generated_queries(question)\n",
    "    answer, formatted_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"\\nSources:\")\n",
    "    print(formatted_docs)  # Print the formatted version of fusion_docs with similarity scores\n",
    "\n",
    "# Run the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!Decomposition\n",
    "## Calismadi olmadi maalesef, asnwer sadece 3. sorunun cevabini veriyor, stratch den baska kaynakalara bakip cözüm bulmak lazim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 19:03:30,511 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 19:03:33,289 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 19:03:36,762 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 19:03:37,067 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 19:03:41,522 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 19:03:41,878 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 19:03:50,274 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 19:03:50,654 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 19:03:59,931 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "\n",
    "question = \"Was ist Mobilfunk?\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "logging.getLogger('urllib3').setLevel(logging.WARNING)\n",
    "\n",
    "# Load API keys\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Initialize model and embeddings\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def initialize_vectorstore(directory):\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs = loader.load()\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "    return vectorstore, docs\n",
    "\n",
    "vectorstore, docs = initialize_vectorstore(\"rag_data/website/organized_data\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define prompts and chains\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answered in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | model | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "\n",
    "# Answer recursion\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | model\n",
    "    | StrOutputParser())\n",
    "\n",
    "    \n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. \"Definition of Mobilfunk\"',\n",
       " '2. \"History of Mobilfunk\"',\n",
       " '3. \"How does Mobilfunk work?\"']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mobilfunk, or mobile communication, works by transmitting signals between mobile devices and network towers. It supports various types of communication such as voice calls, text messaging, video calls, and data transfer. The communication process happens through a mobile network which is divided into cells, each covered by a base station. When a mobile device makes a call or sends a message, the information is sent to the nearest base station, which then transmits the information to the recipient's nearest base station and finally to their device. \\n\\nFor data transfer, when a user browses the internet or uses an app, the request is sent to the base station, then to the mobile network's servers, which retrieve the data from the internet and send it back to the device through the base station. \\n\\nThe ability to move seamlessly between base stations while maintaining the call or data session is what gives Mobilfunk its mobility. However, the effectiveness of Mobilfunk can be influenced by factors such as the strength of the signal in certain areas, the battery life of the mobile device, and the user's mobile tariff.\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Back\n",
    "### cosine similarity eksik sadece calisiyor suan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: Glasfaser baglantisina sahibim, bilmeme gereken en önemli noktalar nelerdir?\n",
      "Step-Back Question: Glasfaser bağlantısı hakkında genel bilgiye sahip olmam gerekiyor mu?\n",
      "\n",
      "Normal Context:\n",
      " Source: rag_data/website/organized_data/Others/https_www_telekom_de_netz_glasfaser_neubauprojekte.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/netz/glasfaser/neubauprojekte\n",
      "\n",
      "Question: Warum Telekom Glasfaser für Ihr privates Eigentum?\n",
      "Answer: Mit einem Telekom Glasfaser-Anschluss sind Sie nicht an uns gebunden und können auch Produkte von anderen Anbietern nutzen.\n",
      "Ein Glasfaser-Anschluss ist der neue Standard für die digitale Versorgung und steigert schon heute den Wert Ihrer Immobilie.\n",
      "Profitieren Sie von der Erfahrung und Zuverlässigkeit der Telekom als Partner. Wir stehen Ihnen jederzeit zur Seite und sorgen für einen reibungslosen Ablauf.\n",
      "\n",
      "Question: Worüber möchten Sie sich informieren?\n",
      "Answer: Wer baut, muss rechtzeitig planen. In allen Fragen zum modernen Hausanschluss berät und begleitet Sie unser geschultes Personal gerne – von der Planung bis zum Anschluss mit Glasfaser von der Telekom in Ihrem Neubau.\n",
      "Sie erteilen uns den Auftrag für Ihren Hausanschluss und übermitteln uns Ihre Neubaudaten über das Eigentümerportal.\n",
      "Das beauftragte Partner­unternehmen vereinbart einen Termin mit Ihnen zum Ausbau Ihres Glasfaser-Anschlusses. Dazu gehören Tiefbau, Hauszuführung und Montage.\n",
      "Unmittelbar nach der Installation in den Wohnräumen können Sie Ihre gebuchten Tarife oder Produkte aktivieren und nutzen.\n",
      "Egal, ob Sie Ihre Immobilie vermieten oder selbst bewohnen, Glasfaser ist in jedem Fall ein echter Gewinn. In wenigen Schritten verlegen wir Glasfaser bis in Ihre Immobilie. Registrieren Sie Ihre Immobilie schon jetzt, um frühzeitig bei der Ausbauplanung miteinbezogen und informiert zu werden, sobald der Ausbau startet.Tipp:Bei gleichzeitiger Tarifbuchung verlegen wir in unseren Ausbaugebieten die Glasfaser von der Straße bis in den Hausanschlussraum kostenlos.\n",
      "Noch Fragen?Dann informieren Sie sich auch unter unserer Glasfaser-Hotline unter0800 33 04174\n",
      "\n",
      "Question: Wie spart mir die Telekom Zeit und Kosten beim Glasfaser-Ausbau?\n",
      "Answer: Um Zeit und Kosten zu minimieren, koordinieren wir uns mit lokalen Unternehmen. So stellen wir sicher, dass Sie Ihren Glasfaser-Anschluss möglichst schnell nutzen können.\n",
      "\n",
      "Question: Für wen bietet die Telekom Glasfaser beim Bau eines neuen Hauses an?\n",
      "Answer: Wir unterstützen sowohl Privatpersonen als auch gewerbliche Bauvorhaben beim Anschluss ihres Neubaus an das Glasfaser-Netz.\n",
      "\n",
      "Question: Ist ein Glasfaser-Anschluss für jeden Neubau in Deutschland möglich?\n",
      "Answer: Die Verfügbarkeit vonGlasfaser-Anschlüssenhängt von verschiedenen Faktoren ab, darunter die regionale Infrastruktur, die Ausbaupläne der Telekommunikationsanbieter und mögliche baurechtliche Vorgaben. In vielen Regionen ist die Glasfaser-Versorgung bereits weit fortgeschritten, während in anderen Gebieten der Ausbau noch aussteht. Denken Sie außerdem daran, dass Sie für das Surfen mit Highspeed-Geschwindigkeit einenGlasfaser-Tarifbenötigen.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Source: rag_data/website/organized_data/Others/https_www_telekom_de_netz_glasfaser_mehr_breitband_fuer_mich.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/netz/glasfaser/mehr-breitband-fuer-mich\n",
      "\n",
      "Question: Warum ist Glasfaser die Zukunft?\n",
      "Answer: Glasfasern leiten Informationen über Lichtwellen weiter und ermöglichen somit sensationelle Übertragungsgeschwindigkeiten. Damit ist die Glasfaser eine echte Zukunftstechnik und konventionellen Anschlüssen, wie DSL oder Kabelnetz, überlegen.\n",
      "Mehr zu Glasfaser\n",
      "\n",
      "Question: Ihre Wohnung oder Ihr Haus durch moderne Glasfasertechnik aufrüsten – was genau bedeutet das?\n",
      "Answer: Hier finden Sie häufig gestellte Fragen zu unseren Produkten.\n",
      "Hilfe zu unseren Produkten\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Step-Back Context:\n",
      " Source: rag_data/website/organized_data/Others/https_www_telekom_de_netz_glasfaser_neubauprojekte.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/netz/glasfaser/neubauprojekte\n",
      "\n",
      "Question: Warum Telekom Glasfaser für Ihr privates Eigentum?\n",
      "Answer: Mit einem Telekom Glasfaser-Anschluss sind Sie nicht an uns gebunden und können auch Produkte von anderen Anbietern nutzen.\n",
      "Ein Glasfaser-Anschluss ist der neue Standard für die digitale Versorgung und steigert schon heute den Wert Ihrer Immobilie.\n",
      "Profitieren Sie von der Erfahrung und Zuverlässigkeit der Telekom als Partner. Wir stehen Ihnen jederzeit zur Seite und sorgen für einen reibungslosen Ablauf.\n",
      "\n",
      "Question: Worüber möchten Sie sich informieren?\n",
      "Answer: Wer baut, muss rechtzeitig planen. In allen Fragen zum modernen Hausanschluss berät und begleitet Sie unser geschultes Personal gerne – von der Planung bis zum Anschluss mit Glasfaser von der Telekom in Ihrem Neubau.\n",
      "Sie erteilen uns den Auftrag für Ihren Hausanschluss und übermitteln uns Ihre Neubaudaten über das Eigentümerportal.\n",
      "Das beauftragte Partner­unternehmen vereinbart einen Termin mit Ihnen zum Ausbau Ihres Glasfaser-Anschlusses. Dazu gehören Tiefbau, Hauszuführung und Montage.\n",
      "Unmittelbar nach der Installation in den Wohnräumen können Sie Ihre gebuchten Tarife oder Produkte aktivieren und nutzen.\n",
      "Egal, ob Sie Ihre Immobilie vermieten oder selbst bewohnen, Glasfaser ist in jedem Fall ein echter Gewinn. In wenigen Schritten verlegen wir Glasfaser bis in Ihre Immobilie. Registrieren Sie Ihre Immobilie schon jetzt, um frühzeitig bei der Ausbauplanung miteinbezogen und informiert zu werden, sobald der Ausbau startet.Tipp:Bei gleichzeitiger Tarifbuchung verlegen wir in unseren Ausbaugebieten die Glasfaser von der Straße bis in den Hausanschlussraum kostenlos.\n",
      "Noch Fragen?Dann informieren Sie sich auch unter unserer Glasfaser-Hotline unter0800 33 04174\n",
      "\n",
      "Question: Wie spart mir die Telekom Zeit und Kosten beim Glasfaser-Ausbau?\n",
      "Answer: Um Zeit und Kosten zu minimieren, koordinieren wir uns mit lokalen Unternehmen. So stellen wir sicher, dass Sie Ihren Glasfaser-Anschluss möglichst schnell nutzen können.\n",
      "\n",
      "Question: Für wen bietet die Telekom Glasfaser beim Bau eines neuen Hauses an?\n",
      "Answer: Wir unterstützen sowohl Privatpersonen als auch gewerbliche Bauvorhaben beim Anschluss ihres Neubaus an das Glasfaser-Netz.\n",
      "\n",
      "Question: Ist ein Glasfaser-Anschluss für jeden Neubau in Deutschland möglich?\n",
      "Answer: Die Verfügbarkeit vonGlasfaser-Anschlüssenhängt von verschiedenen Faktoren ab, darunter die regionale Infrastruktur, die Ausbaupläne der Telekommunikationsanbieter und mögliche baurechtliche Vorgaben. In vielen Regionen ist die Glasfaser-Versorgung bereits weit fortgeschritten, während in anderen Gebieten der Ausbau noch aussteht. Denken Sie außerdem daran, dass Sie für das Surfen mit Highspeed-Geschwindigkeit einenGlasfaser-Tarifbenötigen.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Source: rag_data/website/organized_data/Others/https_www_telekom_de_netz_glasfaseranschluss.txt\n",
      "Content:\n",
      "Source URL: https://www.telekom.de/netz/glasfaseranschluss\n",
      "\n",
      "Question: Ist ein Glasfaser-Anschluss an meiner Adresse verfügbar?\n",
      "Answer: Bei uns können Sie je nach Ihrem individuellen Bedarf zwischen mehreren Tarifen für Ihren Glasfaser-Anschluss wählen. Arbeiten Sie viel im Homeoffice und müssen täglich große Datenmengen verschicken oder sind Sie leidenschaftlicher Gamer? Dann entscheiden Sie sich für Glasfaser 1.000 und genießen Sie ruckelfreies Highspeed-Internet. Sie nutzen das Internet hauptsächlich zum Streamen Ihrer Lieblingsserie und sind viel unterwegs? Dann empfehlen wir Ihnen Glasfaser 600. Die Glasfaser-Anschluss-Tarife der Telekom unterscheiden sich vor allem in der Up- und Downloadgeschwindigkeit – alle Tarife bieten Ihnen eine hervorragende Glasfaser-Internetverbindung. Entscheiden Sie sich zwischen diesen Tarifen für ein schnelles und zuverlässiges Heimnetzwerk:\n",
      "• Glasfaser 150\n",
      "• Glasfaser 300\n",
      "• Glasfaser 600\n",
      "• Glasfaser 1.000\n",
      "• Glasfaser 2.000\n",
      "\n",
      "Question: Welche Tarife bietet die Telekom für einen Glasfaser-Anschluss?\n",
      "Answer: Wenn Sie von einem anderenGlasfaser-Anbieterzu einem Glasfaser-Tarif der Telekom wechseln möchten, unterstützen wir Sie gerne dabei. Wählen Sie einfach den für Sie passenden Tarif bei uns aus und geben Sie beim Buchungsvorgang Ihren bisherigen Anbieter an. Wir veranlassen anschließend die Kündigung bei Ihrem alten Anbieter und sorgen für einen unterbrechungsfreien Wechsel. Natürlich haben Sie jederzeit einen persönlichen Ansprechpartner, falls Sie Fragen zu den Kosten für einen Glasfaser-Anschluss oder weitere Anliegen haben.\n",
      "\n",
      "Question: Wie kann ich von einem anderen Anbieter zu einem Glasfaser-Tarif der Telekom wechseln?\n",
      "Answer: Prüfen Sie zunächst, ob es für Ihren Standort bereits einenFTTH-Anschluss gibt. Ist dies der Fall, benötigen Sie für einen Glasfaser-Anschluss lediglich ein Glasfaser-Modem bzw. einen WLAN-Router mit einem entsprechenden Glasfaser-Anschluss. Diesen verbinden Sie über das Kabel mit Ihrer Glasfaserdose. Das TelekomGlasfaser Modemkommuniziert dann mithilfe optischer Lichtsignale über das Glasfaserkabel mit der Anschlussdose.\n",
      "\n",
      "Question: Was benötige ich für einen Glasfaser-Anschluss bei der Telekom?\n",
      "Answer: Aachen\n",
      "Aalen\n",
      "Ahlen\n",
      "Ahrensburg\n",
      "Alsdorf\n",
      "Altrip\n",
      "Andernach\n",
      "Arnsberg\n",
      "Aschaffenburg\n",
      "Attendorn\n",
      "Augsburg\n",
      "Aurich\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Final Answer:\n",
      " Glasfaser bağlantısı, verileri ışık dalgaları ile ileten bir teknoloji türüdür ve bu da çok yüksek veri aktarım hızlarına olanak sağlar. Bu, özellikle büyük veri hacimlerine sahip olanlar veya kesintisiz internet bağlantısı gerektirenler için idealdir.\n",
      "\n",
      "Glasfaser bağlantısına sahip olduğunuzda bilmeniz gereken önemli noktalar şunlardır:\n",
      "\n",
      "1. İnternet hızınız: Glasfaser, diğer bağlantı türlerine göre genellikle çok daha hızlıdır. Telekom, farklı ihtiyaçlara hitap eden çeşitli Glasfaser tarifeleri sunmaktadır – Glasfaser 150, Glasfaser 300, Glasfaser 600, Glasfaser 1.000 ve Glasfaser 2.000. Bu tarifeler, up ve download hızlarına göre ayrılır.\n",
      "\n",
      "2. Ekipman: Glasfaser bağlantısı için genellikle bir Glasfaser modem veya uygun bir WLAN-Router gereklidir. Bu cihaz, kablo aracılığıyla Glasfaser prizine bağlanır.\n",
      "\n",
      "3. Bağlantının kullanılabilirliği: Glasfaser bağlantısının kullanılabilirliği, bölgenizin altyapısına, telekomünikasyon sağlayıcılarının genişleme planlarına ve olası inşaat düzenlemelerine bağlıdır. Telekom, hem özel kişilere hem de ticari inşaat projelerine Glasfaser ağına bağlantı kurmada destek olmaktadır.\n",
      "\n",
      "4. Yüksek kaliteli hizmet: Telekom, çeşitli hizmetler sunar ve müşterilerine destek olur. Örneğin, bir başka Glasfaser sağlayıcısından Telekom'un Glasfaser tarifesine geçmek istiyorsanız, Telekom sizin için eski sağlayıcınızda iptal işlemlerini gerçekleştirir ve kesintisiz bir geçiş sağlar. \n",
      "\n",
      "5. Değer artışı: Bir Glasfaser bağlantısı, dijital hizmetlere erişimin yeni standardıdır ve zaten emlak değerinizi artırır. Hem kendi evinizde oturuyor olun hem de mülkünüzü kiraya veriyor olun, Glasfaser her durumda gerçek bir kazançtır.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "question = \"Glasfaser baglantisina sahibim, bilmeme gereken en önemli noktalar nelerdir?\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "logging.getLogger('urllib3').setLevel(logging.WARNING)\n",
    "\n",
    "# Load API keys\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize model and embeddings\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def initialize_vectorstore(directory):\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs = loader.load()\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "    return vectorstore\n",
    "\n",
    "# Initialize vector store and retriever\n",
    "vectorstore = initialize_vectorstore(\"rag_data/website/organized_data\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Few Shot Examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Transform examples into example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate step-back queries\n",
    "generate_queries_step_back = prompt | model | StrOutputParser()\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Original Question: {question}\")\n",
    "print(f\"Step-Back Question: {step_back_question}\")\n",
    "\n",
    "# Response prompt template\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "def get_retrieved_content(retrieved_documents):\n",
    "    \"\"\"Format retrieved documents as a string with source information.\"\"\"\n",
    "    seen_sources = set()\n",
    "    content_list = []\n",
    "    for doc in retrieved_documents:\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            content = (\n",
    "                f\"Source: {source}\\n\"\n",
    "                f\"Content:\\n{doc.page_content}\\n\"\n",
    "                \"------------------------------\\n\"\n",
    "            )\n",
    "            content_list.append(content)\n",
    "    return \"\\n\".join(content_list)\n",
    "\n",
    "def format_retrieved_context(query):\n",
    "    \"\"\"Retrieve and format context for the given query.\"\"\"\n",
    "    # Retrieve documents using the 'invoke' method\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    return get_retrieved_content(retrieved_docs)\n",
    "\n",
    "# Construct the chain to retrieve and generate the response\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": lambda x: format_retrieved_context(x[\"question\"]),\n",
    "        \"step_back_context\": lambda x: format_retrieved_context(x[\"step_back_question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "result = chain.invoke({\"question\": question, \"step_back_question\": step_back_question})\n",
    "\n",
    "# Display the final response\n",
    "print(\"\\nNormal Context:\\n\", format_retrieved_context(question))\n",
    "print(\"\\nStep-Back Context:\\n\", format_retrieved_context(step_back_question))\n",
    "print(\"\\nFinal Answer:\\n\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
