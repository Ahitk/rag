{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaynak dosya adi, kaynak url, cos similarity ve kaynak soru cevaplarla veriyor ciktiyi.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "import asyncio\n",
    "\n",
    "# .env dosyasını yükleyerek API anahtarlarını getir\n",
    "load_dotenv()\n",
    "\n",
    "# API Anahtarları\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Model Tanımlaması\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Ana veri dizini: Tüm kategoriler altındaki dosyaları yükleme\n",
    "base_directory = \"rag_data/website/organized_data\"\n",
    "\n",
    "# DirectoryLoader ile belirtilen dizindeki tüm alt klasörlerden txt dosyalarını yükle\n",
    "loader = DirectoryLoader(base_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "# Belgeleri yükle\n",
    "docs = loader.load()\n",
    "\n",
    "# Embedding işlemi\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "\n",
    "# Retriever tanımlaması\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt: ChatPromptTemplate kullanılarak prompt hazırlanır\n",
    "custom_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a Telekom-Hilfe assistant for question-answering tasks, providing answers to Telekom customers or potential customers.\n",
    "        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Cosine Similarity hesaplama fonksiyonu\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Belgeleri formatlama fonksiyonu\n",
    "def format_docs(docs, query_embedding):\n",
    "    unique_sources = set()  # Benzersiz kaynakları saklamak için bir set\n",
    "    formatted_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get(\"source\")  # Kaynağı metadata'dan al\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            document_embedding = embedding.embed_query(doc.page_content)  # Embedding hesapla\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)  # Cosine similarity hesapla\n",
    "            content = doc.page_content.strip() or \"Bu belge içeriği boş.\"  # Belge içeriği\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Chain tanımlaması\n",
    "rag_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | custom_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Sorgu için kullanılan belgeleri ve cevabı döndüren fonksiyon\n",
    "async def retrieve_and_format_docs(question):\n",
    "    # Belgeleri sorgu için al\n",
    "    query_embedding = embedding.embed_query(question)  # Sorgu için embedding al\n",
    "    retrieved_docs = await retriever.ainvoke(question)\n",
    "    \n",
    "    # Belgelerin içeriğini formatla\n",
    "    formatted_docs = format_docs(retrieved_docs, query_embedding)\n",
    "    \n",
    "    # Cevabı al\n",
    "    try:\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Eğer TypeError alınırsa, invoke çağrısının senkron olduğu anlamına gelebilir\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "# Jupyter Notebook gibi bir ortamda, mevcut olay döngüsünü kullanarak asenkron işlevleri çağırma\n",
    "async def main():\n",
    "    question = \"Magenta TV nedir?\"\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "# Mevcut olay döngüsünü kullanarak asenkron işlevleri çalıştırma\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking yapmiyorum esasinda ben, split olayi ancak pdf dosyalarini islerken gerekecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "\n",
    "question = \"Vertrag'imi kündigen yapmak istiyorum, ne yapmaliyim?\"\n",
    "\n",
    "# .env dosyasını yükleyerek API anahtarlarını getir\n",
    "load_dotenv()\n",
    "\n",
    "# API Anahtarları\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Model Tanımlaması\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Ana veri dizini: Tüm kategoriler altındaki dosyaları yükleme\n",
    "base_directory = \"rag_data/website/organized_data\"\n",
    "\n",
    "# DirectoryLoader ile belirtilen dizindeki tüm alt klasörlerden txt dosyalarını yükle\n",
    "loader = DirectoryLoader(base_directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "# Belgeleri yükle\n",
    "docs = loader.load()\n",
    "\n",
    "# Embedding işlemi\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "\n",
    "# Retriever tanımlaması\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "####### Multi Query ########\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "   \n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)\n",
    "print(docs)\n",
    "\n",
    "\n",
    "#################################\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt: ChatPromptTemplate kullanılarak prompt hazırlanır\n",
    "custom_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a Telekom-Hilfe assistant for question-answering tasks, providing answers to Telekom customers or potential customers.\n",
    "        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Cosine Similarity hesaplama fonksiyonu\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Belgeleri formatlama fonksiyonu\n",
    "def format_docs(docs, query_embedding):\n",
    "    unique_sources = set()  # Benzersiz kaynakları saklamak için bir set\n",
    "    formatted_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get(\"source\")  # Kaynağı metadata'dan al\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            document_embedding = embedding.embed_query(doc.page_content)  # Embedding hesapla\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)  # Cosine similarity hesapla\n",
    "            content = doc.page_content.strip() or \"Bu belge içeriği boş.\"  # Belge içeriği\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Chain tanımlaması\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    |\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | custom_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Sorgu için kullanılan belgeleri ve cevabı döndüren fonksiyon\n",
    "async def retrieve_and_format_docs(question):\n",
    "    # Belgeleri sorgu için al\n",
    "    query_embedding = embedding.embed_query(question)  # Sorgu için embedding al\n",
    "    retrieved_docs = await retriever.ainvoke(question)\n",
    "    \n",
    "    # Belgelerin içeriğini formatla\n",
    "    formatted_docs = format_docs(retrieved_docs, query_embedding)\n",
    "    \n",
    "    # Cevabı al\n",
    "    try:\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Eğer TypeError alınırsa, invoke çağrısının senkron olduğu anlamına gelebilir\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "# Jupyter Notebook gibi bir ortamda, mevcut olay döngüsünü kullanarak asenkron işlevleri çağırma\n",
    "async def main():\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "# Mevcut olay döngüsünü kullanarak asenkron işlevleri çalıştırma\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alttaki son kod daha iyi calisiyorsa üsttekini sil!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 14:21:02,817 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:05,384 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:08,884 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:09,183 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:09,230 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:09,240 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:09,248 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:09,272 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:10,792 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "1: 1. Wie kann ich meinen Vertrag kündigen?\n",
      "2: 2. Was sind die Schritte, um meine Vertrag zu kündigen?\n",
      "3: 3. Welche Vorgehensweise ist erforderlich, um meine Vertrag zu beenden?\n",
      "4: 4. Wie beende ich meinen Vertrag ordnungsgemäß?\n",
      "5: 5. Welche Maßnahmen muss ich ergreifen, um meine Vertrag zu kündigen?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 14:21:11,063 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:11,384 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:11,684 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:13,241 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:13,494 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:13,505 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:13,521 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:13,525 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:13,594 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:16,589 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:17,979 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:18,288 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:18,290 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:18,291 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:18,291 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:18,292 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:21:21,459 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: If you want to cancel your contract, you should contact Telekom directly. The documents provided above do not give specific information on how to cancel a contract, but they suggest that changes can be made in the customer center or the MeinMagenta App. For a more accurate answer, I suggest visiting Telekom's official website or contacting their customer service.\n",
      "\n",
      "Sources:\n",
      "Source document: rag_data/website/organized_data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vertrag_aenderung_mobilfunk_vertrag.txt\n",
      "\n",
      "Cosine Similarity: 0.7816\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vertrag/aenderung/mobilfunk-vertrag\n",
      "Telekom > Hilfe & Service > Vertrag & Rechnung > Vertrag > Änderung > Mobilfunk-Vertrag\n",
      "\n",
      "Question: Wie kann ich meinen Mobilfunk-Vertrag ändern?\n",
      "Answer: So können Sie im Kundencenter oder der MeinMagenta App Ihren Tarif verlängern oder ändern:\n",
      "Hinweis: Sie können jederzeit in einen höherwertigen Tarif wechseln. Der Wechsel in einen Tarif, der weniger Leistungen anbietet (weniger kostet), ist erst nach 12 Monaten möglich. Bitte denken Sie daran, dass ein Wechsel in einen kleineren Tarif mit Kosten verbunden ist.\n",
      "• KundencenterGehen Sie im Bereich \"Verträge\" auf Ihren Mobilfunk-Vertrag. Klicken Sie dann in den Vertragsdetails auf den farbigen Button, je nachdem, ob Sie Ihren Tarif ändern oder Ihren Vertrag verlängern möchten. Anschließend werden Ihnen die Optionen für neue Tarife angezeigt.\n",
      "• MeinMagenta AppKlicken Sie auf Ihren Vertrag und gehen dann auf \"Vertrag anpassen\". Klicken Sie dann in den Vertragsdetails auf den farbigen Button, je nachdem, ob Sie Ihren Tarif wechseln oder Ihren Vertrag verlängern möchten. Anschließend werden Ihnen die Optionen für neue Tarife angezeigt.\n",
      "\n",
      "Source document: rag_data/website/organized_data/Vertrag & Rechnung/https_www_telekom_de_hilfe_vertrag_rechnung_vertrag_bestellung_vertragsgenehmigung.txt\n",
      "\n",
      "Cosine Similarity: 0.7728\n",
      "\n",
      "Source URL: https://www.telekom.de/hilfe/vertrag-rechnung/vertrag/bestellung/vertragsgenehmigung\n",
      "Telekom > Hilfe & Service > Vertrag & Rechnung > Vertrag > Bestellung > Vertrag > genehmigen\n",
      "\n",
      "Question: Warum muss ich einen Festnetz- oder Mobilfunk-Vertrag genehmigen?\n",
      "Answer: Mit der Änderung desTelekommunikationsgesetzes (TKG) § 54sind wir als Telekom verpflichtet, Sie vor Vertragsabschluss über die Vertragsinhalte zu informieren.Nach Erhalt der sogenannten vorvertraglichen Informationen (VVI) sowie einer Vertragszusammenfassung (VZF) müssen Sie den Vertrag genehmigen.\n",
      "So genehmigen Sie einen VertragIn einem Verkaufsgespräch (telefonisch oder im Telekom Shop), werden Ihnen die Dokumente vorab ausgehändigt oder per E-Mail zugesendet.Schließen Sie einen Vertrag online ab, wird Ihnen ein Download-Link zur Verfügung gestellt. Hierüber können Sie die Dokumente einsehen und speichern. Die Auftragserteilung erfolgt über den Klick auf den Button \"Zahlungspflichtig bestellen\".\n",
      "Genehmigung nachträglich erteilenIn seltenen Fällen können wir Ihnen die nötigen Unterlagen erst nach Ihrer Auftragserteilung zukommen lassen. Erhalten Sie bspw. bei Klick auf den Download-Link die Fehlermeldung \"Das Dokument konnte nicht geladen werden\", besteht ein technischer Fehler. Wir senden Ihnen daraufhin per E-Mail ein Genehmigungsschreiben zu.\n",
      "Vertrag wird nicht genehmigtErfolgt die Genehmigung auch nach der zweiten Erinnerung nicht, wird der Vertragsabschluss rückgängig gemacht. Im Falle eines Tarifwechsels oder einer Vertragsverlängerung wird der Ursprungstarif wiederhergestellt.Wenn Sie im Rahmen des Vertragsabschlusses ein Gerät (z. B. Smartphone oder Tablet) erhalten haben, sind Sie dazu verpflichtet, dieses an uns zurückzusenden. Bereits gezahlte Preise werden Ihnen im Anschluss gutgeschrieben.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "\n",
    "# Soru Tanımlaması\n",
    "question = \"Vertrag'imi kündigen yapmak istiyorum, ne yapmaliyim?\"\n",
    "\n",
    "# API Anahtarlarını Yükle\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Modeli ve Embedding'i Başlat\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Belgeleri Yükle ve Embedding Oluştur\n",
    "def initialize_vectorstore(directory):\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs = loader.load()\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "    return vectorstore, docs\n",
    "\n",
    "vectorstore, docs = initialize_vectorstore(\"rag_data/website/organized_data\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Alternatif Sorular İçin Şablon\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "multi_query_docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "# Prompt Tanımlaması\n",
    "custom_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a Telekom-Hilfe assistant for question-answering tasks, providing answers to Telekom customers or potential customers.\n",
    "        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Cosine Similarity Hesaplama Fonksiyonu\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Belgeleri Formatlama\n",
    "def format_docs(docs, query_embedding):\n",
    "    unique_sources = set()\n",
    "    formatted_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get(\"source\")\n",
    "        if source and source not in unique_sources:\n",
    "            unique_sources.add(source)\n",
    "            document_embedding = embedding.embed_query(doc.page_content)\n",
    "            similarity = cosine_similarity(query_embedding, document_embedding)\n",
    "            content = doc.page_content.strip() or \"Bu belge içeriği boş.\"\n",
    "            formatted_docs.append(\n",
    "                f\"Source document: {source}\\n\\nCosine Similarity: {similarity:.4f}\\n\\n{content}\"\n",
    "            )\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Chain Tanımlaması\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} \n",
    "    | custom_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async def retrieve_and_format_docs(question):\n",
    "    query_embedding = embedding.embed_query(question)\n",
    "    #retrieved_docs = await retriever.ainvoke(question)\n",
    "    formatted_docs = format_docs(multi_query_docs, query_embedding)\n",
    "    \n",
    "    try:\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "# Asenkron Sorular Fonksiyonu\n",
    "async def print_generated_queries(question):\n",
    "    queries = generate_queries.invoke({\"question\": question})  # Burada await kullanmaya gerek yok\n",
    "    print(\"Generated Questions:\")\n",
    "    for i, q in enumerate(queries):\n",
    "        print(f\"{i+1}: {q}\")\n",
    "\n",
    "# Ana Fonksiyon\n",
    "async def main():\n",
    "    await print_generated_queries(question)\n",
    "    answer, source_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"\\nSources:\")\n",
    "    print(source_docs)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iyi calisiyor gibi ama 4 soru generate ederken bazen dil sorunu olabiliyor türkce girince, ing ve alm olm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 14:42:48,144 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:50,760 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:53,488 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:53,757 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:53,816 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:53,829 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:53,839 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:54,953 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "1: 1. eSIM teknolojisi nedir?\n",
      "2: 2. eSIM avantajları nelerdir?\n",
      "3: 3. eSIM nasıl kullanılır?\n",
      "4: 4. eSIM hangi cihazlarda desteklenmektedir?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 14:42:55,185 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:55,467 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:55,781 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:56,083 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:56,389 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:58,856 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:59,114 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:59,169 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:59,201 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:42:59,221 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:43:03,662 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:43:04,786 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:43:05,132 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:43:05,171 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:43:05,267 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:43:05,414 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-08-31 14:43:08,982 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Bir eSIM, cihazınıza zaten entegre edilmiş bir SIM karttır. Bu, \"e\"nin gömülü (embedded) anlamına geldiği bir çiptir. Mobil ağda tanımlama verileri, \"klasik\" SIM kartta olduğu gibi, eSIM'de bir eSIM profili olarak indirilmelidir.\n",
      "\n",
      "Sources:\n",
      "Source: rag_data/website/organized_data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_esim_wissenswertes_definition.txt\n",
      "Fusion Score: 0.0651\n",
      "Cosine Similarity: 0.7983\n",
      "Content: Source URL: https://www.telekom.de/hilfe/mobilfunk/esim/wissenswertes/definition\n",
      "Telekom > Hilfe & Service > Mobilfunk > eSIM > Wissenswertes > Erklärung\n",
      "\n",
      "Question: Was ist eine eSIM?\n",
      "Answer: Eine eSIM ist eine integrierte SIM-Karte, die als kleiner Chip bereits fest in Ihrem Gerät verbaut ist. Das \"e\" steht für embedded (eingebettet).\n",
      "Die Daten zur Identifikation im Mobilfunk-Netz, die sich bei der \"klassischen\" SIM-Karte auf dem Chip befinden, müssen bei der eSIM als eSIM-Profil runtergeladen werden.\n",
      "\n",
      "\n",
      "\n",
      "Source: rag_data/website/organized_data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_esim_wissenswertes_vorteile.txt\n",
      "Fusion Score: 0.0651\n",
      "Cosine Similarity: 0.7636\n",
      "Content: Source URL: https://www.telekom.de/hilfe/mobilfunk/esim/wissenswertes/vorteile\n",
      "Telekom > Hilfe & Service > Mobilfunk > eSIM > Wissenswertes > eSIM > Vorteile\n",
      "\n",
      "Question: Was ist der Vorteil der eSIM gegenüber der \"klassischen\" SIM-Karte?\n",
      "Answer: Das Einsetzen einer \"klassischen\" SIM-Karte entfällt, da die eSIM fest im Gerät verbaut ist.\n",
      "Dank der eSIM können Sie auch mit Smartwatches Musik streamen, telefonieren, Nachrichten versenden und vieles mehr.\n",
      "Das Beste dabei: Wenn Sie eine Smartwatch mit eSIM nutzen, können Sie Ihr Smartphone zu Hause lassen. Die eSIM übernimmt alle Mobilfunk-Funktionen der klassischen SIM-Karte.\n",
      "\n",
      "\n",
      "\n",
      "Source: rag_data/website/organized_data/Mobilfunk/https_www_telekom_de_hilfe_mobilfunk_esim_bestellen_einrichten_multisim.txt\n",
      "Fusion Score: 0.0651\n",
      "Cosine Similarity: 0.7668\n",
      "Content: Source URL: https://www.telekom.de/hilfe/mobilfunk/esim/bestellen-einrichten/multisim\n",
      "Telekom > Hilfe & Service > Mobilfunk > eSIM > Bestellen & Einrichten > MultiSIM > Bestellung\n",
      "\n",
      "Question: Wie bestelle ich eine MultiSIM?\n",
      "Answer: Sie können eine oder mehrere MultiSIMs über IhrKundencenterbestellen:\n",
      "Wählen Sie den Vertrag über \"Zum Vertrag\" bzw. \"Vertragsdetails\" aus, zu dem die MultiSIM bestellt werden soll. Scrollen Sie zu \"Meine SIMs\" und klicken Sie auf den Button \"MultiSIM bestellen\". Führen Sie zur Authentifikation die \"Sicherheitsüberprüfung\" durch. Mit \"Zahlungspflichtig bestellen\" schließen Sie die Bestellung ab.\n",
      "\n",
      "\n",
      "\n",
      "Source: rag_data/website/organized_data/Geräte & Zubehör/https_www_telekom_de_hilfe_geraete_zubehoer_handy_smartphone_tablet_ipad_mini_4_ipad_mini_4_sim_lock.txt\n",
      "Fusion Score: 0.0651\n",
      "Cosine Similarity: 0.7217\n",
      "Content: Source URL: https://www.telekom.de/hilfe/geraete-zubehoer/handy-smartphone-tablet/ipad-mini-4/ipad-mini-4-sim-lock\n",
      "Telekom > Hilfe & Service > Geräte & Zubehör > Smartphone & Tablet > Apple > iPad > iPad > mini > 4\n",
      "\n",
      "Question: Hat das iPad mini 4 einen SIM-Lock, wenn ich dieses von der Telekom beziehe?\n",
      "Answer: Nein, das iPad mini 4 hat keinen Telekom SIM-Lock.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "\n",
    "# Configure logging to suppress INFO logs from HTTP requests\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "logging.getLogger('urllib3').setLevel(logging.WARNING)\n",
    "\n",
    "# Soru Tanımlaması\n",
    "question = \"esim nedir?\"\n",
    "\n",
    "# API Anahtarlarını Yükle\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Modeli ve Embedding'i Başlat\n",
    "model = ChatOpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Belgeleri Yükle ve Embedding Oluştur\n",
    "def initialize_vectorstore(directory):\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    docs = loader.load()\n",
    "    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "    return vectorstore, docs\n",
    "\n",
    "vectorstore, docs = initialize_vectorstore(\"rag_data/website/organized_data\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "############ RAG-Fusion #############\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Generate four queries\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Function for reciprocal rank fusion\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal rank fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "# Chain for the retrieval process\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "fusion_docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "# Cosine Similarity Calculation Function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2) if (norm_vec1 and norm_vec2) else 0.0\n",
    "\n",
    "# Function to get embeddings for a document's content\n",
    "async def get_document_embeddings(doc):\n",
    "    return embedding.embed_query(doc.page_content)\n",
    "\n",
    "# Function to format fusion_docs as a readable string with similarity scores\n",
    "async def format_fusion_docs_with_similarity(fusion_docs):\n",
    "    formatted_docs = []\n",
    "    question_embedding = embedding.embed_query(question)\n",
    "    \n",
    "    for doc, score in fusion_docs:\n",
    "        doc_embedding = await get_document_embeddings(doc)\n",
    "        similarity = cosine_similarity(question_embedding, doc_embedding)\n",
    "        source = doc.metadata.get(\"source\", \"No source\")\n",
    "        content = doc.page_content\n",
    "        formatted_docs.append(f\"Source: {source}\\nFusion Score: {score:.4f}\\nCosine Similarity: {similarity:.4f}\\nContent: {content}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "####################\n",
    "\n",
    "# Prompt Definition\n",
    "telekom_template = \"\"\"You are a Telekom-Hilfe assistant for question-answering tasks, providing answers to Telekom customers or potential customers. \n",
    "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_telekom = ChatPromptTemplate.from_template(telekom_template)\n",
    "\n",
    "# Chain Definition\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")} \n",
    "    | prompt_telekom\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async def retrieve_and_format_docs(question):\n",
    "    formatted_docs = await format_fusion_docs_with_similarity(fusion_docs)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to get the answer asynchronously\n",
    "        answer = await rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    except TypeError:\n",
    "        # Fallback to synchronous invocation if asynchronous fails\n",
    "        answer = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    \n",
    "    return answer, formatted_docs\n",
    "\n",
    "# Asynchronous function to print generated queries\n",
    "async def print_generated_queries(question):\n",
    "    queries = generate_queries.invoke({\"question\": question})\n",
    "    print(\"Generated Questions:\")\n",
    "    for i, q in enumerate(queries):\n",
    "        print(f\"{i+1}: {q}\")\n",
    "\n",
    "# Main Function\n",
    "async def main():\n",
    "    await print_generated_queries(question)\n",
    "    answer, formatted_docs = await retrieve_and_format_docs(question)\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"\\nSources:\")\n",
    "    print(formatted_docs)  # Print the formatted version of fusion_docs with similarity scores\n",
    "\n",
    "# Run the main function\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
