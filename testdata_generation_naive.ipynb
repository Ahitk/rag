{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating testdata folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### IMPORTANT NOTE: The version v.0.1.21 of RAGAS has been used to create the test data. \n",
    "###### The higher version v.0.2x has a significantly different test data generation structure, which is why v.0.1.21 was preferred.\n",
    "###### The existing test data CSV files are already located in the corresponding data folder.\n",
    "###### If you want to create new test data with this notebook, please use a different environment and install it with `pip install ragas==0.1.21`.\n",
    "###### The latest version has been used for evaluation with RAGAS metrics, \n",
    "###### and this version is specified in the requirements.txt file: `pip install git+https://github.com/explodinggradients/ragas.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test_data folder with 8 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define the source directory containing the original data\n",
    "source_dir = '/Users/taha/Desktop/rag/data'\n",
    "# Define the destination directory for the test data\n",
    "test_dir = '/Users/taha/Desktop/rag/test_data'\n",
    "\n",
    "# Specify the number of files to select from each subfolder\n",
    "file_count = 50\n",
    "\n",
    "# Function to create test data by copying files from the source directory to the destination directory\n",
    "def create_test_data(source, destination, count):\n",
    "    # Create the main test directory if it doesn't already exist\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "    \n",
    "    # Loop through each folder in the main source directory\n",
    "    for folder in os.listdir(source):\n",
    "        source_folder_path = os.path.join(source, folder)  # Path to the source subfolder\n",
    "        dest_folder_path = os.path.join(destination, folder)  # Path to the destination subfolder\n",
    "        \n",
    "        # If the item is not a directory, skip it\n",
    "        if not os.path.isdir(source_folder_path):\n",
    "            continue\n",
    "        \n",
    "        # Create the corresponding destination subfolder\n",
    "        os.makedirs(dest_folder_path, exist_ok=True)\n",
    "        \n",
    "        # List all .txt files in the current subfolder, excluding files that end with '_summary.txt'\n",
    "        files = [\n",
    "            f for f in os.listdir(source_folder_path)\n",
    "            if f.endswith('.txt') and not f.endswith('_summary.txt')\n",
    "        ]\n",
    "        \n",
    "        # Check if the subfolder has fewer files than the specified count\n",
    "        if len(files) < count:\n",
    "            print(f\"Warning: The folder '{folder}' has fewer than {count} files.\")\n",
    "        \n",
    "        # Randomly select up to 'count' files from the available files\n",
    "        selected_files = random.sample(files, min(count, len(files)))\n",
    "        \n",
    "        # Copy each selected file from the source to the destination\n",
    "        for file_name in selected_files:\n",
    "            src_file_path = os.path.join(source_folder_path, file_name)\n",
    "            dest_file_path = os.path.join(dest_folder_path, file_name)\n",
    "            shutil.copy(src_file_path, dest_file_path)\n",
    "            print(f\"Copied file: {file_name} --> {dest_folder_path}\")\n",
    "\n",
    "# Run the function to create the test data set\n",
    "create_test_data(source_dir, test_dir, file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test_data_naive folder without categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define the source directory containing the original data\n",
    "source_dir = '/Users/taha/Desktop/rag/data'\n",
    "# Define the destination directory where all selected files will be saved\n",
    "test_dir = '/Users/taha/Desktop/rag/test_data_naive'\n",
    "\n",
    "# Specify the number of files to select in total\n",
    "file_count = 200\n",
    "\n",
    "# Function to create test data by selecting random files from all folders and copying them to a single directory\n",
    "def create_naive_test_data(source, destination, count):\n",
    "    # Create the main destination directory if it doesn't already exist\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "    # Collect all eligible .txt files (excluding _summary.txt) from all subdirectories\n",
    "    all_files = []\n",
    "    for folder in os.listdir(source):\n",
    "        source_folder_path = os.path.join(source, folder)\n",
    "        \n",
    "        # Skip if the item is not a directory\n",
    "        if not os.path.isdir(source_folder_path):\n",
    "            continue\n",
    "        \n",
    "        # Add all .txt files (excluding _summary.txt) from this subfolder to the list\n",
    "        files = [\n",
    "            os.path.join(source_folder_path, f) for f in os.listdir(source_folder_path)\n",
    "            if f.endswith('.txt') and not f.endswith('_summary.txt')\n",
    "        ]\n",
    "        all_files.extend(files)\n",
    "\n",
    "    # Check if there are fewer files than the desired count\n",
    "    if len(all_files) < count:\n",
    "        print(f\"Warning: Only {len(all_files)} files found, which is less than the specified {count}.\")\n",
    "    \n",
    "    # Randomly select up to 'count' files from the combined list of all .txt files\n",
    "    selected_files = random.sample(all_files, min(count, len(all_files)))\n",
    "    \n",
    "    # Copy each selected file into the single destination folder\n",
    "    for file_path in selected_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        dest_file_path = os.path.join(destination, file_name)\n",
    "        shutil.copy(file_path, dest_file_path)\n",
    "        print(f\"Copied file: {file_name} --> {destination}\")\n",
    "\n",
    "# Run the function to create the naive test data set\n",
    "create_naive_test_data(source_dir, test_dir, file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RAG - Semantic Search - Character Splitting\n",
    "#### Creating testdata with Character Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing test directory\n",
      "[INFO] Found 190 files in test directory\n",
      "[INFO] Splitting complete, 1076 chunks created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.                     \n",
      "Generating: 100%|██████████| 10/10 [00:25<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Testset generated with size 10\n",
      "[INFO] Results saved to /Users/taha/Desktop/rag/test_data_naive/_testset_semantic_gradient.csv\n",
      "[INFO] Completed processing test directory\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import initials\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "\n",
    "# Initialize TestsetGenerator with embedding and model from initials\n",
    "generator = TestsetGenerator.from_langchain(generator_llm=initials.model, critic_llm=initials.model, embeddings=initials.embedding)\n",
    "\n",
    "# Character splitting settings\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='',\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=25,\n",
    ")\n",
    "\n",
    "# CharacterTextSplitter with separator\n",
    "text_splitter_separator = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "#RecursiveCharacterTextSplitter\n",
    "text_splitter_recursive = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Semantic Splitting\n",
    "text_splitter_semantic = SemanticChunker(embeddings=initials.embedding, breakpoint_threshold_type=\"gradient\")\n",
    "\n",
    "# Define test directory path\n",
    "test_directory = \"/Users/taha/Desktop/rag/test_data_naive\"\n",
    "\n",
    "TEST_SIZE = 10\n",
    "\n",
    "# Output CSV file path\n",
    "output_file_path = os.path.join(test_directory, \"_testset_semantic_gradient.csv\")\n",
    "\n",
    "# Skip if CSV already exists\n",
    "if os.path.exists(output_file_path):\n",
    "    print(f\"[INFO] CSV already exists, skipping...\")\n",
    "else:\n",
    "    print(f\"\\n[INFO] Processing test directory\")\n",
    "\n",
    "    # Collect all .txt files in the test directory\n",
    "    txt_files = glob.glob(os.path.join(test_directory, \"*.txt\"))\n",
    "\n",
    "    print(f\"[INFO] Found {len(txt_files)} files in test directory\")\n",
    "\n",
    "    # Load all files\n",
    "    documents = []\n",
    "    for file_path in txt_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            documents.append(f.read())   \n",
    "\n",
    "    # Apply splitting\n",
    "    chunks = text_splitter.create_documents(documents)\n",
    "\n",
    "    print(f\"[INFO] Splitting complete, {len(chunks)} chunks created.\")\n",
    "\n",
    "    # Create a test set of N items\n",
    "    testset = generator.generate_with_langchain_docs(chunks, test_size=TEST_SIZE)\n",
    "    testset_df = testset.to_pandas()\n",
    "    print(f\"[INFO] Testset generated with size\", TEST_SIZE)\n",
    "\n",
    "    # Save DataFrame as CSV\n",
    "    testset_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"[INFO] Results saved to {output_file_path}\")\n",
    "\n",
    "    print(f\"[INFO] Completed processing test directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
